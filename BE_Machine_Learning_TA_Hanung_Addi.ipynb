{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-BMCEJBRH46"
      },
      "source": [
        "# CELL 1: INSTALL DEPEDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zo2di_L042N",
        "outputId": "7aaf68b5-70e0-4015-9043-2f1435aabb12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n",
            "Collecting transformers==4.18\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.18) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0 (from transformers==4.18)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.18) (2.31.0)\n",
            "Collecting sacremoses (from transformers==4.18)\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.18)\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.18) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.18) (2023.7.22)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.18) (1.3.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=bf0317f30f132e0fd58dedf20fcd8ed53968058392b81bf4878194f868f87c9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.18.0 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.41.1\n",
            "Collecting datasets==1.16.1\n",
            "  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.3/298.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==1.16.1) (1.23.5)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.16.1) (9.0.0)\n",
            "Collecting dill (from datasets==1.16.1)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==1.16.1) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.16.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==1.16.1) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==1.16.1) (3.4.1)\n",
            "Collecting multiprocess (from datasets==1.16.1)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.16.1) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==1.16.1) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.16.1) (0.18.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==1.16.1) (23.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.16.1) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.16.1) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.16.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.16.1) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.16.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.16.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.16.1) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.16.1) (3.12.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.16.1) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.16.1) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.16.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.16.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.16.1) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.16.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.16.1) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==1.16.1) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-1.16.1 dill-0.3.7 multiprocess-0.70.15\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.24.0-py3-none-any.whl (260 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.24.0\n"
          ]
        }
      ],
      "source": [
        "#@title Install Depedencies\n",
        "!python --version\n",
        "!pip install transformers==4.18\n",
        "!pip install bitsandbytes\n",
        "!pip install datasets==1.16.1\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzuATzvqRLwx"
      },
      "source": [
        "# CELL 2: MODEL FINETUNING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXIjUDGc5RLq"
      },
      "source": [
        "### Fine-tuning 6-Billion GPT-J (& other models) in colab with LoRA and 8-bit compression\n",
        "\n",
        "This notebook is a simple example for fine-tuning [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6B) with limited memory. A detailed explanation of how it works can be found in [this model card](https://huggingface.co/hivemind/gpt-j-6B-8bit). It is heavily based on [this Colab](https://colab.research.google.com/drive/1ft6wQU0BhqG5PRlwgaZJv2VukKKjU4Es#scrollTo=vfdLQHOuEU7h). Huge thanks to Hivemind!\n",
        "\n",
        "You can also finetune [GPT-Neo-2.7B](https://huggingface.co/gustavecortal/gpt-neo-2.7B-8bit), [French GPT-J (Cedille's Boris)](https://huggingface.co/gustavecortal/fr-boris-8bit) and [T0-3B](https://huggingface.co/gustavecortal/T0_3B-8bit) with limited memory.\n",
        "\n",
        "Twitter: [@gustavecortal](https://twitter.com/gustavecortal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX35XMqY1JfY",
        "outputId": "1ddfc252-e0ac-4e5a-9ab1-e1d7739fccce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8013'), PosixPath('http'), PosixPath('//172.28.0.1')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-3kuxqblst4kaa --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import transformers\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.cuda.amp import custom_fwd, custom_bwd\n",
        "\n",
        "from bitsandbytes.functional import quantize_blockwise, dequantize_blockwise\n",
        "\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47hM3-7-1WOh",
        "outputId": "47dded74-dea2-4156-8b6d-aeb2b918bbad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKti6sXNG4H5"
      },
      "source": [
        "## Converting the model to 8 bits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVLaOT811bVo"
      },
      "outputs": [],
      "source": [
        "class FrozenBNBLinear(nn.Module):\n",
        "    def __init__(self, weight, absmax, code, bias=None):\n",
        "        assert isinstance(bias, nn.Parameter) or bias is None\n",
        "        super().__init__()\n",
        "        self.out_features, self.in_features = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias)\n",
        "        output_clone = output.clone()\n",
        "        if self.adapter:\n",
        "            output_clone += self.adapter(input)\n",
        "        return output_clone\n",
        "\n",
        "    @classmethod\n",
        "    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n",
        "        return cls(weights_int8, *state, linear.bias)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n",
        "\n",
        "\n",
        "class DequantizeAndLinear(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    @custom_fwd\n",
        "    def forward(ctx, input: torch.Tensor, weights_quantized: torch.ByteTensor,\n",
        "                absmax: torch.FloatTensor, code: torch.FloatTensor, bias: torch.FloatTensor):\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        ctx.save_for_backward(input, weights_quantized, absmax, code)\n",
        "        ctx._has_bias = bias is not None\n",
        "        return F.linear(input, weights_deq, bias)\n",
        "\n",
        "    @staticmethod\n",
        "    @custom_bwd\n",
        "    def backward(ctx, grad_output: torch.Tensor):\n",
        "        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n",
        "        input, weights_quantized, absmax, code = ctx.saved_tensors\n",
        "        # grad_output: [*batch, out_features]\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        grad_input = grad_output @ weights_deq\n",
        "        grad_bias = grad_output.flatten(0, -2).sum(dim=0) if ctx._has_bias else None\n",
        "        return grad_input, None, None, None, grad_bias\n",
        "\n",
        "\n",
        "class FrozenBNBEmbedding(nn.Module):\n",
        "    def __init__(self, weight, absmax, code):\n",
        "        super().__init__()\n",
        "        self.num_embeddings, self.embedding_dim = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        "\n",
        "    def forward(self, input, **kwargs):\n",
        "        with torch.no_grad():\n",
        "            # note: both quantuized weights and input indices are *not* differentiable\n",
        "            weight_deq = dequantize_blockwise(self.weight, absmax=self.absmax, code=self.code)\n",
        "            output = F.embedding(input, weight_deq, **kwargs)\n",
        "        if self.adapter:\n",
        "            output += self.adapter(input)\n",
        "        return output\n",
        "\n",
        "    @classmethod\n",
        "    def from_embedding(cls, embedding: nn.Embedding) -> \"FrozenBNBEmbedding\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(embedding.weight)\n",
        "        return cls(weights_int8, *state)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.num_embeddings}, {self.embedding_dim})\"\n",
        "\n",
        "\n",
        "def quantize_blockise_lowmemory(matrix: torch.Tensor, chunk_size: int = 2 ** 20):\n",
        "    assert chunk_size % 4096 == 0\n",
        "    code = None\n",
        "    chunks = []\n",
        "    absmaxes = []\n",
        "    flat_tensor = matrix.view(-1)\n",
        "    for i in range((matrix.numel() - 1) // chunk_size + 1):\n",
        "        input_chunk = flat_tensor[i * chunk_size: (i + 1) * chunk_size].clone()\n",
        "        quantized_chunk, (absmax_chunk, code) = quantize_blockwise(input_chunk, code=code)\n",
        "        chunks.append(quantized_chunk)\n",
        "        absmaxes.append(absmax_chunk)\n",
        "\n",
        "    matrix_i8 = torch.cat(chunks).reshape_as(matrix)\n",
        "    absmax = torch.cat(absmaxes)\n",
        "    return matrix_i8, (absmax, code)\n",
        "\n",
        "\n",
        "def convert_to_int8(model):\n",
        "    \"\"\"Convert linear and embedding modules to 8-bit with optional adapters\"\"\"\n",
        "    for module in list(model.modules()):\n",
        "        for name, child in module.named_children():\n",
        "            if isinstance(child, nn.Linear):\n",
        "                print(name, child)\n",
        "                setattr(\n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBLinear(\n",
        "                        weight=torch.zeros(child.out_features, child.in_features, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                        bias=child.bias,\n",
        "                    ),\n",
        "                )\n",
        "            elif isinstance(child, nn.Embedding):\n",
        "                setattr(\n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBEmbedding(\n",
        "                        weight=torch.zeros(child.num_embeddings, child.embedding_dim, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                    )\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3Y40yNoG_Kn"
      },
      "source": [
        "You have to Monkey-Patch GPT-J before loading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtafVIQn1gcl"
      },
      "outputs": [],
      "source": [
        "class GPTJBlock(transformers.models.gptj.modeling_gptj.GPTJBlock):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        convert_to_int8(self.attn)\n",
        "        convert_to_int8(self.mlp)\n",
        "\n",
        "\n",
        "class GPTJModel(transformers.models.gptj.modeling_gptj.GPTJModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        "\n",
        "\n",
        "class GPTJForCausalLM(transformers.models.gptj.modeling_gptj.GPTJForCausalLM):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        "\n",
        "\n",
        "transformers.models.gptj.modeling_gptj.GPTJBlock = GPTJBlock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpyKyOpsHF27"
      },
      "source": [
        "If you're using another 8-bit quantized model (e.g. T0-3B), remember to Monkey-Patch the model using convert_to_int8()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1q4P6av8e3k"
      },
      "outputs": [],
      "source": [
        "class T5ForConditionalGeneration(transformers.models.t5.modeling_t5.T5ForConditionalGeneration):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        # convert_to_int8(self)\n",
        "\n",
        "transformers.models.t5.modeling_t5.T5ForConditionalGeneration = T5ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwJixCub1uwI"
      },
      "outputs": [],
      "source": [
        "config = transformers.GPTJConfig.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0SXSVLk1zh_"
      },
      "outputs": [],
      "source": [
        "config.pad_token_id = config.eos_token_id\n",
        "tokenizer.pad_token = config.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYYH1l2hHhFZ",
        "outputId": "ac9f42b8-6791-4b4d-a29d-8dadfcd3f775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "lm_head Linear(in_features=4096, out_features=50400, bias=True)\n"
          ]
        }
      ],
      "source": [
        "gpt = GPTJForCausalLM.from_pretrained(\"hivemind/gpt-j-6B-8bit\", low_cpu_mem_usage=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOmJJMf0Ho3H"
      },
      "source": [
        "## LoRA fine-tuning example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxBGvu6SdcDa",
        "outputId": "ed675e66-ad1e-4f12-b4e3-bb39753427ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['prompt:Berikan daftar interface yang ada di Router A,commands:[Router A] show interfaces [eoc]', 'prompt:Saya membutuhkan informasi tentang semua interface di Router A,commands:[Router A] show interfaces [eoc]', 'prompt:Tunjukkan daftar semua interface yang tersedia di Router A,commands:[Router A] show interfaces [eoc]', 'prompt:Beri saya daftar semua antarmuka yang terhubung ke Router A,commands:[Router A] show interfaces [eoc]', 'prompt:Saya ingin melihat daftar interface di Router B,commands:[Router B] show interfaces [eoc]', 'prompt:Tolong berikan informasi tentang semua interface yang tersedia di Router B,commands:[Router B] show interfaces [eoc]', 'prompt:Tunjukkan saya daftar semua antarmuka di Router B,commands:[Router B] show interfaces [eoc]', 'prompt:Berikan daftar semua antarmuka yang terhubung ke Router C,commands:[Router C] show interfaces [eoc]', 'prompt:Saya memerlukan informasi tentang semua interface di Router C,commands:[Router C] show interfaces [eoc]', 'prompt:Tunjukkan semua interface yang tersedia di Router C,commands:[Router C] show interfaces [eoc]', 'prompt:Beri saya daftar semua antarmuka yang terhubung ke Router D,commands:[Router D] show interfaces [eoc]', 'prompt:Mohon informasi tentang semua interface di Router D,commands:[Router D] show interfaces [eoc]', 'prompt:Tolong tunjukkan daftar semua antarmuka di Router D,commands:[Router D] show interfaces [eoc]', 'prompt:Berikan daftar semua antarmuka yang terhubung ke Router internal,commands:[Router internal] show interfaces [eoc]', 'prompt:Saya membutuhkan informasi tentang semua interface di Router external,commands:[Router external] show interfaces [eoc]', 'prompt:Tolong tunjukkan daftar semua antarmuka di Router R1,commands:[Router R1] show interfaces [eoc]', 'prompt:Mohon berikan daftar semua antarmuka yang terhubung ke Router R2,commands:[Router R2] show interfaces [eoc]', 'prompt:Tolong berikan daftar interface untuk Router B Interface G0/0/0,commands:[Router B] show interface g0/0/0 [eoc]', 'prompt:Tunjukkan daftar interface yang tersedia di Router A Interface F0/0,commands:[Router A] show interface f0/0 [eoc]', 'prompt:Berikan daftar semua interface yang terhubung ke Router A Interface F0/1,commands:[Router A] show interface f0/1 [eoc]', 'prompt:Saya membutuhkan informasi tentang antarmuka F1/0 di Router A,commands:[Router A] show interface f1/0 [eoc]', 'prompt:Berikan daftar semua interface untuk Router B G0/1/1,commands:[Router B] show interface g0/1/1 [eoc]', 'prompt:Tolong tunjukkan daftar interface yang tersedia di Router B Interface G1/1/1,commands:[Router B] show interface g1/1/1 [eoc]', 'prompt:Beri saya daftar semua antarmuka yang tersedia di Router C F0/1,commands:[Router C] show interface f0/1 [eoc]', 'prompt:Saya membutuhkan informasi tentang interface G1/0/0 di Router R1,commands:[Router R1] show interface g1/0/0 [eoc]', 'prompt:Tunjukkan daftar semua interface yang terhubung ke Router R2 Interface G0/0/0,commands:[Router R2] show interface g0/0/0 [eoc]', 'prompt:Berikan daftar semua interface untuk Router R2 G0/0/1,commands:[Router R2] show interface g0/0/1 [eoc]', 'prompt:Tolong berikan daftar interface untuk Router R1 Interface F0/1,commands:[Router R1] show interface f0/1 [eoc]', 'prompt:Tolong ubah IP di Router C Interface X menjadi 192.168.1.1,commands:[Router C] configure terminal\\ninterface X\\nip address 192.168.1.1 255.255.255.0\\nend [eoc]', 'prompt:Ubah IP di Router A Interface G0/0/1 menjadi 10.0.0.1,commands:[Router A] configure terminal\\ninterface G0/0/1\\nip address 10.0.0.1 255.255.255.0\\nend [eoc]', 'prompt:Ubah IP di Router B Interface F0/1 menjadi 172.16.0.1,commands:[Router B] configure terminal\\ninterface F0/1\\nip address 172.16.0.1 255.255.255.0\\nend [eoc]', 'prompt:Tolong ubah IP di Internal Router Interface G1/0/0 menjadi 192.168.2.1,commands:[Internal Router] configure terminal\\ninterface G1/0/0\\nip address 192.168.2.1 255.255.255.0\\nend [eoc]', 'prompt:Saya butuh bantuan untuk mengubah IP di External Router Interface F1/0 menjadi 10.1.1.1,commands:[External Router] configure terminal\\ninterface F1/0\\nip address 10.1.1.1 255.255.255.0\\nend [eoc]', 'prompt:Mohon bantu untuk mengganti IP di R1 Interface G0/0/0 menjadi 192.168.3.1,commands:[Router R1] configure terminal\\ninterface G0/0/0\\nip address 192.168.3.1 255.255.255.0\\nend [eoc]', 'prompt:Mohon bantu untuk mengubah IP di Router D Interface F0/1 menjadi 172.16.10.1,commands:[Router D] configure terminal\\ninterface F0/1\\nip address 172.16.10.1 255.255.255.0\\nend [eoc]', 'prompt:Tolong ubah IP di Router E Interface G0/0 menjadi 192.168.5.1,commands:[Router E] configure terminal\\ninterface G0/0\\nip address 192.168.5.1 255.255.255.0\\nend [eoc]', 'prompt:Berikan bantuan untuk mengganti IP di Router F Interface F1/0 menjadi 10.1.2.1,commands:[Router F] configure terminal\\ninterface F1/0\\nip address 10.1.2.1 255.255.255.0\\nend [eoc]', 'prompt:Ubah IP di Router G Interface G0/0/1 menjadi 10.0.1.1,commands:[Router G] configure terminal\\ninterface G0/0/1\\nip address 10.0.1.1 255.255.255.0\\nend [eoc]', 'prompt:Mohon bantu untuk mengubah IP di R2 Interface F0/0 menjadi 192.168.4.1,commands:[Router R2] configure terminal\\ninterface F0/0\\nip address 192.168.4.1 255.255.255.0\\nend [eoc]', 'prompt:Tolong tambahkan kunci enkripsi ke Router D,commands:[Router D] configure terminal\\ncrypto key generate rsa\\nend [eoc]', 'prompt:Mohon tambahkan kunci enkripsi ke Router E,commands:[Router E] configure terminal\\ncrypto key generate rsa\\nend [eoc]', 'prompt:Tolong buat kunci enkripsi di Router F,commands:[Router F] configure terminal\\ncrypto key generate rsa\\nend [eoc]', 'prompt:Mohon tambahkan kunci enkripsi ke Router G,commands:[Router G] configure terminal\\ncrypto key generate rsa\\nend [eoc]', 'prompt:Tolong aktifkan enkripsi di Router H,commands:[Router H] configure terminal\\nservice password-encryption\\nend [eoc]', 'prompt:Mohon bantu untuk menambahkan kunci enkripsi di R1,commands:[Router R1] configure terminal\\ncrypto key generate rsa\\nend [eoc]', 'prompt:Saya ingin melihat daftar semua antarmuka di Router D,commands:[Router D] show interfaces [eoc]', 'prompt:Tunjukkan semua interface yang terhubung ke Router external,commands:[Router external] show interfaces [eoc]', 'prompt:Beri saya daftar semua antarmuka yang terhubung ke Router internal,commands:[Router internal] show interfaces [eoc]', 'prompt:Bagaimana cara mengkonfigurasi OSPF pada Router A?,commands:[Router A] router ospf 1\\nnetwork 10.0.0.0 0.255.255.255 area 0\\nend [eoc]', \"prompt:Berikan contoh penggunaan perintah 'ping' pada Router B,commands:[Router B] ping 10.0.0.1 [eoc]\", 'prompt:Bagaimana cara mengubah hostname pada Router C?,commands:[Router C] configure terminal\\nhostname NewHostName\\nend [eoc]', 'prompt:Saya ingin mengecek routing table pada Router A,commands:[Router A] show ip route [eoc]', 'prompt:Bagaimana cara mengkonfigurasi VLAN pada Switch A?,commands:[Switch A] vlan 10\\nname Sales\\nend [eoc]', 'prompt:Tolong buatkan ACL untuk memblokir akses ke situs web tertentu pada Router D,commands:[Router D] configure terminal\\naccess-list 101 deny tcp any host 192.168.1.2 eq 80\\naccess-list 101 permit ip any any\\ninterface gigabitEthernet 0/0\\nip access-group 101 in\\nend [eoc]', 'prompt:Bagaimana cara menampilkan port yang sedang aktif pada Switch B?,commands:[Switch B] show interface status [eoc]', 'prompt:Saya ingin mengkonfigurasi DHCP pada Router internal,commands:[Internal Router] configure terminal\\ndhcp pool InternalPool\\nnetwork 192.168.0.0 255.255.255.0\\ndefault-router 192.168.0.1\\nend [eoc]', 'prompt:Bagaimana cara menampilkan MAC address table pada Switch C?,commands:[Switch C] show mac-address-table [eoc]', 'prompt:Saya ingin mengkonfigurasi NAT pada Router external,commands:[Router external] configure terminal\\ninterface gigabitEthernet 0/0\\nip nat outside\\ninterface gigabitEthernet 0/1\\nip nat inside\\nip nat inside source list 1 interface gigabitEthernet 0/0 overload\\naccess-list 1 permit 192.168.0.0 0.0.255.255\\nend [eoc]', 'prompt:Bagaimana cara menampilkan log yang tersimpan pada Router R1?,commands:[Router R1] show logging [eoc]', 'prompt:Saya ingin memeriksa status koneksi pada Router R2,commands:[Router R2] show interfaces brief [eoc]', 'prompt:Bagaimana cara mengkonfigurasi trunking pada Switch D?,commands:[Switch D] interface gigabitEthernet 0/0\\nswitchport mode trunk\\nend [eoc]', 'prompt:Tolong buatkan static route pada Router internal untuk mengakses subnet 172.16.0.0/24 melalui Router A,commands:[Internal Router] ip route 172.16.0.0 255.255.255.0 10.0.0.2 [eoc]', 'prompt:Bagaimana cara menghapus VLAN yang tidak dibutuhkan pada Switch E?,commands:[Switch E] configure terminal\\nno vlan 10\\nend [eoc]', 'prompt:Saya ingin menampilkan informasi tentang status port pada Switch F,commands:[Switch F] show interface gigabitEthernet 0/1 [eoc]', 'prompt:Tunjukkan daftar konfigurasi yang ada di Router A,commands:[Router A] show running-config [eoc]', 'prompt:Beri saya daftar semua konfigurasi di Router A,commands:[Router A] show running-config [eoc]', 'prompt:Saya ingin melihat konfigurasi di Router B,commands:[Router B] show running-config [eoc]', 'prompt:Tolong berikan daftar semua konfigurasi di Router B,commands:[Router B] show running-config [eoc]', 'prompt:Berikan daftar semua konfigurasi di Router C,commands:[Router C] show running-config [eoc]', 'prompt:Saya memerlukan informasi tentang konfigurasi di Router C,commands:[Router C] show running-config [eoc]', 'prompt:Tunjukkan semua konfigurasi di Router D,commands:[Router D] show running-config [eoc]', 'prompt:Berikan daftar semua konfigurasi di Router internal,commands:[Router internal] show running-config [eoc]', 'prompt:Saya membutuhkan informasi tentang semua konfigurasi di Router external,commands:[Router external] show running-config [eoc]', 'prompt:Tolong tunjukkan daftar semua konfigurasi di Router R1,commands:[Router R1] show running-config [eoc]', 'prompt:Mohon berikan daftar semua konfigurasi di Router R2,commands:[Router R2] show running-config [eoc]', 'prompt:Tunjukkan daftar konfigurasi untuk Router B Interface G0/0/0,commands:[Router B] show running-config interface g0/0/0 [eoc]', 'prompt:Berikan konfigurasi untuk Router A Interface F0/0,commands:[Router A] show running-config interface f0/0 [eoc]', 'prompt:Saya membutuhkan informasi tentang konfigurasi di Router A Interface F0/1,commands:[Router A] show running-config interface f0/1 [eoc]', 'prompt:Tolong berikan konfigurasi untuk interface F1/0 di Router A,commands:[Router A] show running-config interface f1/0 [eoc]', 'prompt:Berikan konfigurasi untuk Router B G0/1/1,commands:[Router B] show running-config interface g0/1/1 [eoc]', 'prompt:Tunjukkan daftar konfigurasi yang terhubung ke Router B Interface G1/1/1,commands:[Router B] show running-config interface g1/1/1 [eoc]', 'prompt:Beri saya konfigurasi semua antarmuka yang tersedia di Router C F0/1,commands:[Router C] show running-config interface f0/1 [eoc]', 'prompt:Saya membutuhkan informasi tentang konfigurasi di interface G1/0/0 di Router R1,commands:[Router R1] show running-config interface g1/0/0 [eoc]', 'prompt:Tunjukkan daftar konfigurasi semua interface yang terhubung ke Router R2 Interface G0/0/0,commands:[Router R2] show running-config interface g0/0/0 [eoc]', 'prompt:Berikan konfigurasi untuk semua interface pada Router R2 G0/0/1,commands:[Router R2] show running-config interface g0/0/1 [eoc]', 'prompt:Beri saya daftar konfigurasi di Router D yang terkait dengan VLAN 10,commands:[Router D] show running-config | include vlan 10 [eoc]', 'prompt:Tunjukkan konfigurasi yang berhubungan dengan OSPF di Router A,commands:[Router A] show running-config | include ospf [eoc]', 'prompt:Beri saya daftar semua konfigurasi yang terkait dengan NAT di Router B,commands:[Router B] show running-config | include nat [eoc]', 'prompt:Saya ingin melihat daftar konfigurasi yang terkait dengan DHCP di Router C,commands:[Router C] show running-config | include dhcp [eoc]', 'prompt:Tolong tunjukkan daftar semua konfigurasi yang terkait dengan firewall di Router internal,commands:[Router internal] show running-config | include firewall [eoc]', 'prompt:Berikan konfigurasi untuk interface G0/0 di Router external,commands:[Router external] show running-config interface g0/0 [eoc]', 'prompt:Saya memerlukan informasi tentang konfigurasi VRF di Router R1,commands:[Router R1] show running-config | include vrf [eoc]', 'prompt:Tunjukkan daftar konfigurasi terkait dengan access-list di Router R2,commands:[Router R2] show running-config | include access-list [eoc]', 'prompt:Berikan daftar semua konfigurasi OSPF yang terkait dengan area 0 di Router A,commands:[Router A] show running-config | include ospf.*area 0 [eoc]', 'prompt:Saya ingin melihat konfigurasi NAT yang terkait dengan subnet 192.168.1.0/24 di Router B,commands:[Router B] show running-config | include nat.*192.168.1.0/24 [eoc]', 'prompt:Tolong berikan daftar konfigurasi BGP di Router C,commands:[Router C] show running-config | include bgp [eoc]', 'prompt:Saya membutuhkan informasi tentang konfigurasi QoS di Router internal,commands:[Router internal] show running-config | include qos [eoc]', 'prompt:Tunjukkan daftar konfigurasi yang terkait dengan MPLS di Router external,commands:[Router external] show running-config | include mpls [eoc]', 'prompt:Berikan konfigurasi untuk interface F0/1 di Router R1,commands:[Router R1] show running-config interface f0/1 [eoc]', 'prompt:Saya memerlukan informasi tentang konfigurasi VPC di Router R2,commands:[Router R2] show running-config | include vpc [eoc]', 'prompt:Tolong tunjukkan daftar konfigurasi yang terkait dengan IPsec di Router A,commands:[Router A] show running-config | include ipsec [eoc]', 'prompt:Tolong atur VLAN 10 pada Switch A dan hubungkan ke port GigabitEthernet1/0/5,commands:[Switch A] configure terminal\\nvlan 10\\nname VLAN10\\ninterface GigabitEthernet1/0/5\\nswitchport mode access\\nswitchport access vlan 10\\nend [eoc]', 'prompt:Tambahkan static route pada Router D untuk subnet 192.168.10.0/24 melalui Router B dengan metric 10,commands:[Router D] configure terminal\\nip route 192.168.10.0 255.255.255.0 Router B 10\\nend [eoc]', \"prompt:Tolong tambahkan user baru bernama 'john' dengan password 'password123' pada server MySQL,commands:CREATE USER 'john'@'localhost' IDENTIFIED BY 'password123';\\nGRANT ALL PRIVILEGES ON * . * TO 'john'@'localhost';\\nFLUSH PRIVILEGES; [eoc]\", 'prompt:Tolong tambahkan firewall rule pada server web untuk memblokir akses dari IP address 192.168.1.100,commands:sudo ufw deny from 192.168.1.100 [eoc]', 'prompt:Tolong buat konfigurasi NAT pada Router A untuk mengakses jaringan internal (192.168.1.0/24) dari internet,commands:[Router A] configure terminal\\ninterface GigabitEthernet0/0/1\\nip nat outside\\nexit\\ninterface GigabitEthernet0/0/0\\nip nat inside\\nexit\\nip nat inside source list 1 interface GigabitEthernet0/0/1 overload\\naccess-list 1 permit 192.168.1.0 0.0.0.255\\nend [eoc]', 'prompt:Buat route static menuju network 192.168.1.0 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0 di Router A,commands:[Router A] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Ciptakan skrip konfigurasi yang akan menetapkan route statis pada Router A untuk mengarahkan lalu lintas ke jaringan 192.168.1.0 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0,commands:[Router A] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Konfigurasikan routing statis di Router A untuk mengarahkan lalu lintas ke jaringan 192.168.1.0 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0,commands:[Router A] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Ciptakan konfigurasi perutean yang tepat pada Router A untuk memungkinkan lalu lintas menuju jaringan 192.168.1.0 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0,commands:[Router A] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Tulis skrip perutean yang diperlukan untuk mengarahkan semua paket menuju jaringan 192.168.1.0 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0 di Router A,commands:[Router A] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router A dan perkenalkan network 192.168.2.0, 192.168.3.0,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Lakukan konfigurasi pada Router A untuk mengaktifkan protokol RIP versi 2 dan perkenalkan network 192.168.2.0 dan 192.168.3.0 ke dalam perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router A dan pastikan network 192.168.2.0 dan 192.168.3.0 dikenali oleh perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Lakukan langkah-langkah untuk mengaktifkan protokol RIP versi 2 pada Router A dan pastikan network 192.168.2.0 dan 192.168.3.0 dapat diakses melalui perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router A sekarang juga dan segera perkenalkan network 192.168.2.0 dan 192.168.3.0 ke dalam tabel perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 dengan IP address 192.168.0.1 subnet 255.255.255.0 pada interface g0/0 di Router A,commands:[Router A] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router A dan berikan IP address 192.168.0.1 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 11 pada interface g0/0 di Router A dengan IP address 192.168.0.1 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router A dan tetapkan IP address 192.168.0.1 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router A dan pastikan menggunakan IP address 192.168.0.1 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 dengan IP address 192.168.1.1 subnet 255.255.255.0 pada interface g0/0 di Router A,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g0/0 di Router A dan berikan IP address 192.168.1.1 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 22 pada interface g0/0 di Router A dengan IP address 192.168.1.1 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g0/0 di Router A dan tetapkan IP address 192.168.1.1 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g0/0 di Router A dan pastikan menggunakan IP address 192.168.1.1 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 dengan IP address 192.168.2.1 subnet 255.255.255.0 pada interface g0/0 di Router A,commands:[Router A] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 pada interface g0/0 di Router A dan berikan IP address 192.168.2.1 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 33 pada interface g0/0 di Router A dengan IP address 192.168.2.1 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 pada interface g0/0 di Router A dan tetapkan IP address 192.168.2.1 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 pada interface g0/0 di Router A dan pastikan menggunakan IP address 192.168.2.1 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Buat route static menuju network 192.168.0.1 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0 di Router B,commands:[Router B] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Ciptakan skrip konfigurasi yang akan menetapkan route statis pada Router B untuk mengarahkan lalu lintas ke jaringan 192.168.0.1 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0,commands:[Router B] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Konfigurasikan routing statis di Router B untuk mengarahkan lalu lintas ke jaringan 192.168.0.1 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0,commands:[Router B] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Ciptakan konfigurasi perutean yang tepat pada Router B untuk memungkinkan lalu lintas menuju jaringan 192.168.0.1 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0,commands:[Router B] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Tulis skrip perutean yang diperlukan untuk mengarahkan semua paket menuju jaringan 192.168.0.1 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0 di Router B,commands:[Router B] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router B dan perkenalkan network 192.168.1.0, 192.168.2.0,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Lakukan konfigurasi pada Router B untuk mengaktifkan protokol RIP versi 2 dan perkenalkan network 192.168.1.0 dan 192.168.2.0 ke dalam perutean,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router B dan pastikan network 192.168.1.0 dan 192.168.2.0 dikenali oleh perutean,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Lakukan langkah-langkah untuk mengaktifkan protokol RIP versi 2 pada Router B dan pastikan network 192.168.1.0 dan 192.168.2.0 dapat diakses melalui perutean,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router B sekarang juga dan segera perkenalkan network 192.168.1.0 dan 192.168.2.0 ke dalam tabel perutean,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 dengan IP address 192.168.0.2 subnet 255.255.255.0 pada interface g0/0 di Router B,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.2 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 11 pada interface g0/0 di Router B dengan IP address 192.168.0.2 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router B dan berikan IP address 192.168.0.2 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router B dan tetapkan IP address 192.168.0.2 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router B dan pastikan menggunakan IP address 192.168.0.2 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 dengan IP address 192.168.2.1 subnet 255.255.255.0 pada interface g0/0 di Router B,commands:[Router B] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g0/0 di Router B dan berikan IP address 192.168.2.1 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 22 pada interface g0/0 di Router B dengan IP address 192.168.2.1 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g0/0 di Router B dan tetapkan IP address 192.168.2.1 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g0/0 di Router B dan pastikan menggunakan IP address 192.168.2.1 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Buat route static menuju network 192.168.3.0 dengan subnet mask 255.255.255.0 melalui interface g/1/0/0 di Router A,commands:[Router A] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\n end [eoc]', 'prompt:Ciptakan skrip konfigurasi yang akan menetapkan route statis pada Router A untuk mengarahkan lalu lintas ke jaringan 192.168.3.0 dengan subnet mask 255.255.255.0 melalui interface g/1/0/0,commands:[Router A] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\n end [eoc]', 'prompt:Konfigurasikan routing statis di Router A untuk mengarahkan lalu lintas ke jaringan 192.168.3.0 dengan subnet mask 255.255.255.0 melalui interface g/1/0/0,commands:[Router A] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\n end [eoc]', 'prompt:Ciptakan konfigurasi perutean yang tepat pada Router A untuk memungkinkan lalu lintas menuju jaringan 192.168.3.0 dengan subnet mask 255.255.255.0 melalui interface g/1/0/0,commands:[Router A] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\n end [eoc]', 'prompt:Tulis skrip perutean yang diperlukan untuk mengarahkan semua paket menuju jaringan 192.168.3.0 dengan subnet mask 255.255.255.0 melalui interface g/1/0/0 di Router A,commands:[Router A] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router A dan perkenalkan network 192.168.0.1, 192.168.2.0,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Lakukan konfigurasi pada Router A untuk mengaktifkan protokol RIP versi 2 dan perkenalkan network 192.168.0.1 dan 192.168.2.0 ke dalam perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router A dan pastikan network 192.168.0.1 dan 192.168.2.0 dikenali oleh perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Lakukan langkah-langkah untuk mengaktifkan protokol RIP versi 2 pada Router A dan pastikan network 192.168.0.1 dan 192.168.2.0 dapat diakses melalui perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router A sekarang juga dan segera perkenalkan network 192.168.0.1 dan 192.168.2.0 ke dalam tabel perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Buat route static menuju network 192.168.2.0 dengan subnet mask 255.255.255.0 melalui interface g/1/1/0 di Router A,commands:[Router A] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\n end [eoc]', 'prompt:Ciptakan skrip konfigurasi yang akan menetapkan route statis pada Router A untuk mengarahkan lalu lintas ke jaringan 192.168.2.0 dengan subnet mask 255.255.255.0 melalui interface g/1/1/0,commands:[Router A] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\n end [eoc]', 'prompt:Konfigurasikan routing statis di Router A untuk mengarahkan lalu lintas ke jaringan 192.168.2.0 dengan subnet mask 255.255.255.0 melalui interface g/1/1/0,commands:[Router A] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\n end [eoc]', 'prompt:Ciptakan konfigurasi perutean yang tepat pada Router A untuk memungkinkan lalu lintas menuju jaringan 192.168.2.0 dengan subnet mask 255.255.255.0 melalui interface g/1/1/0,commands:[Router A] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\n end [eoc]', 'prompt:Tulis skrip perutean yang diperlukan untuk mengarahkan semua paket menuju jaringan 192.168.2.0 dengan subnet mask 255.255.255.0 melalui interface g/1/1/0 di Router A,commands:[Router A] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router A dan perkenalkan network 192.168.0.1, 192.168.3.0,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Aktifkan versi 2 dari protokol RIP di Router A dan tambahkan jaringan 192.168.0.1 dan 192.168.3.0 ke dalam konfigurasi.,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Menambahkan  jaringan dengan alamat IP 192.168.0.1 dan 192.168.3.0 ke dalam konfigurasi lalu aktifkan protokol RIP versi 2 pada Router A,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Tambahkan jaringan 192.168.0.1 dan 192.168.3.0 ke dalam konfigurasi, kemudian aktifkan protokol RIP versi 2 pada Router A,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Lakukan langkah-langkah untuk mengaktifkan protokol RIP versi 2 pada Router A dan pastikan network 192.168.0.1 dan 192.168.3.0 dapat diakses melalui perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router B dan perkenalkan network 192.168.1.0, 192.168.3.0,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Pada Router A, tambahkan network 192.168.0.1 serta 192.168.3.0 dan aktifkan protokol RIP versi 2,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Tambahkan jaringan 192.168.0.1 dan 192.168.3.0 ke dalam konfigurasi Router A, serta aktifkan protokol RIP versi 2.,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router A, serta pastikan jaringan 192.168.0.1 serta 192.168.3.0 sudah dikenali pada konfigurasi perutean,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Lakukan pengaktifan protokol RIP versi 2 pada Router A dan pastikan bahwa jaringan 192.168.0.1 dan 192.168.3.0 telah terdaftar dalam konfigurasi routing,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 dengan IP address 192.168.0.1 subnet 255.255.255.0 pada interface g0/0 di Router B,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Mengaktifkan VLAN 33 pada antarmuka g0/0 di Router B dengan alamat IP 192.168.0.1 dan subnet 255.255.255.0.,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 33 pada interface g0/0 di Router B dengan IP address 192.168.0.1 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Konfigurasikan VLAN 33 pada antarmuka g0/0 di Router B dengan alamat IP 192.168.0.1 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan konfigurasi VLAN 33 pada interface g0/0 di Router B dengan alamat IP 192.168.0.1 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Buat route static menuju network 192.168.3.0 dengan subnet mask 255.255.255.0 melalui interface g/1/0/1 di Router B,commands:[Router B] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/1\\n end [eoc]', 'prompt:Lakukan konfigurasi yang menetapkan route statis pada Router B untuk mengarahkan lalu lintas ke jaringan 192.168.3.0 dengan subnet mask 255.255.255.0 melalui interface g/1/0/1,commands:[Router B] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/1\\n end [eoc]', 'prompt:Lakukan pengaturan konfigurasi pada Router B untuk mengonfigurasi rute statis yang mengarahkan lalu lintas ke jaringan 192.168.3.0 dengan subnet mask 255.255.255.0 melalui antarmuka g/1/0/1.,commands:[Router B] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/1\\n end [eoc]', 'prompt:Tulis skrip perutean yang diperlukan untuk mengarahkan semua paket menuju jaringan 192.168.3.0 dengan subnet mask 255.255.255.0 melalui interface g/1/0/1 di Router B,commands:[Router B] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/1\\n end [eoc]', 'prompt:Tuliskan skrip rute yang diperlukan untuk mengalihkan semua paket ke jaringan 192.168.3.0 dengan subnet mask 255.255.255.0 melalui antarmuka g/1/0/1 di Router B.,commands:[Router B] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/1\\n end [eoc]', 'prompt:Aktifkan VLAN 33 dengan IP address 192.168.3.0 subnet 255.255.255.0 pada interface g1/0 di Router B,commands:[Router B] configure terminal\\nint g1/0.33\\nencapsulation dot1q 33\\nip address 192.168.3.0 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 33 pada antarmuka g1/0 di Router B dengan IP address 192.168.3.0 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g1/0.33\\nencapsulation dot1q 33\\nip address 192.168.3.0 255.255.255.0\\n end [eoc]', 'prompt:Konfigurasikan VLAN 33 pada interface Router B g1/0 dengan alamat IP 192.168.3.0 dan subnet mask 255.255.255.0.,commands:[Router B] configure terminal\\nint g1/0.33\\nencapsulation dot1q 33\\nip address 192.168.3.0 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan konfigurasi VLAN 33 pada interface g1/0 di Router B dengan alamat IP 192.168.3.0 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g1/0.33\\nencapsulation dot1q 33\\nip address 192.168.3.0 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 pada interface g1/0 di Router B dan pastikan menggunakan IP address 192.168.3.0 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g1/0.33\\nencapsulation dot1q 33\\nip address 192.168.3.0 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 dengan IP address 192.168.2.2 subnet 255.255.255.0 pada interface g0/1 di Router A,commands:[Router A] configure terminal\\nint g0/1.11\\nencapsulation dot1q 11\\nip address 192.168.2.2 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 11 pada interface g0/1 di Router A dengan IP address 192.168.2.2 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/1.11\\nencapsulation dot1q 11\\nip address 192.168.2.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/1 di Router A dan berikan IP address 192.168.2.2 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/1.11\\nencapsulation dot1q 11\\nip address 192.168.2.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/1 di Router A dan tetapkan IP address 192.168.2.2 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/1.11\\nencapsulation dot1q 11\\nip address 192.168.2.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/1 di Router A dan pastikan menggunakan IP address 192.168.2.2 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/1.11\\nencapsulation dot1q 11\\nip address 192.168.2.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 dengan IP address 192.168.1.1 subnet 255.255.255.0 pada interface g0/0 di Router B,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 33 pada antarmuka g0/0 di Router B dengan IP address 192.168.1.1 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Konfigurasikan VLAN 33 pada interface Router B g0/0 dengan alamat IP 192.168.1.1 dan subnet mask 255.255.255.0.,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan konfigurasi VLAN 33 pada interface g0/0 di Router B dengan alamat IP 192.168.1.1 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 pada interface g0/0 di Router B dan pastikan menggunakan IP address 192.168.1.1 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 dengan IP address 192.168.0.3 subnet 255.255.255.0 pada interface g0/1 di Router B,commands:[Router B] configure terminal\\nint g0/1.22\\nencapsulation dot1q 22\\nip address 192.168.0.3 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 22 pada interface g0/1 di Router B dengan IP address 192.168.0.3 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/1.22\\nencapsulation dot1q 22\\nip address 192.168.0.3 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/1 di Router B dan berikan IP address 192.168.0.3 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/1.22\\nencapsulation dot1q 22\\nip address 192.168.0.3 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/1 di Router B dan tetapkan IP address 192.168.0.3 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/1.22\\nencapsulation dot1q 22\\nip address 192.168.0.3 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/1 di Router B dan pastikan menggunakan IP address 192.168.0.3 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/1.22\\nencapsulation dot1q 22\\nip address 192.168.0.3 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 dengan IP address 192.168.2.3 subnet 255.255.255.0 pada interface g0/0 di Router A,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.3 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 2 pada interface g0/0 di Router A dan berikan IP address 192.168.2.3 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.3 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 22 pada interface g0/0 di Router A dengan IP address 192.168.2.3 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.3 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g0/0 di Router A dan tetapkan IP address 192.168.2.3 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.3 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g0/0 di Router A dan pastikan menggunakan IP address 192.168.2.3 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.3 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 dengan IP address 192.168.2.4 subnet 255.255.255.0 pada interface g1/1 di Router A,commands:[Router A] configure terminal\\nint g1/1.22\\nencapsulation dot1q 22\\nip address 192.168.2.4 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 22 pada interface g1/1 di Router A dengan IP address 192.168.2.4 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g1/1.22\\nencapsulation dot1q 22\\nip address 192.168.2.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g1/1 di Router A dan berikan IP address 192.168.2.4 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g1/1.22\\nencapsulation dot1q 22\\nip address 192.168.2.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g1/1 di Router A dan tetapkan IP address 192.168.2.4 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g1/1.22\\nencapsulation dot1q 22\\nip address 192.168.2.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g1/1 di Router A dan pastikan menggunakan IP address 192.168.2.4 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g1/1.22\\nencapsulation dot1q 22\\nip address 192.168.2.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 dengan IP address 192.168.0.4 subnet 255.255.255.0 pada interface g0/0 di Router B,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 pada interface g0/0 di Router B dan berikan IP address 192.168.0.4 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.4 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 33 pada interface g0/0 di Router B dengan IP address 192.168.0.4 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 pada interface g0/0 di Router B dan tetapkan IP address 192.168.0.4 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 pada interface g0/0 di Router B dan pastikan menggunakan IP address 192.168.0.4 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 dengan IP address 192.168.1.2 subnet 255.255.255.0 pada interface g0/0 di Router B,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.1.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router B dan berikan IP address 192.168.1.2 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.1.2 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 11 pada interface g0/0 di Router B dengan IP address 192.168.1.2 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.1.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router B dan tetapkan IP address 192.168.1.2 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.1.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router B dan pastikan menggunakan IP address 192.168.1.2 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.1.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 dengan IP address 192.168.1.4 subnet 255.255.255.0 pada interface g1/1 di Router A,commands:[Router A] configure terminal\\nint g1/1.11\\nencapsulation dot1q 11\\nip address 192.168.1.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g1/1 di Router A dan berikan IP address 192.168.1.4 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g1/1.11\\nencapsulation dot1q 11\\nip address 192.168.1.4 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 11 pada interface g1/1 di Router A dengan IP address 192.168.1.4 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g1/1.11\\nencapsulation dot1q 11\\nip address 192.168.1.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g1/1 di Router A dan tetapkan IP address 192.168.1.4 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g1/1.11\\nencapsulation dot1q 11\\nip address 192.168.1.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g1/1 di Router A dan pastikan menggunakan IP address 192.168.1.4 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g1/1.11\\nencapsulation dot1q 11\\nip address 192.168.1.4 255.255.255.0\\n end [eoc]', 'commands:[Router Z] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\n end [eoc],bot_reply:Untuk membuat rute pada Router Z melalui interface g/0/0/0 menuju jaringan lokal dengan IP 192.168.1.0, subnet mask 255.255.255.0 kita perlu menyatakan pemanggilan konfigurasi Router Z terlebih dahulu dengan menggunakan perintah\\n[Router Z] configure terminal\\nLalu dengan perintah ip route tambahkan alamat IP tujuan, subnet mask, dan interface. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router Z] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\nend [eoc]', 'commands:[Router I] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\n end [eoc],bot_reply:Untuk mengkonfigurasi rute pada Router I untuk jaringan lokal dengan alamat IP 192.168.1.0 dan subnet mask 255.255.255.0, Anda harus mulai dengan mengakses mode konfigurasi menggunakan perintah:\\n[Router I] configure terminal\\nSelanjutnya, tambahkan alamat IP tujuan, subnet mask, dan antarmuka menggunakan perintah `ip route`. Urutan perintah diakhiri dengan yang berikut:\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\nend [eoc]\\nAnda dapat menjalankan serangkaian perintah seperti yang disebutkan di atas.', 'commands:[Router X] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\n end [eoc],bot_reply:Untuk mengatur rute pada Router X untuk jaringan lokal dengan alamat IP 192.168.1.0 dan subnet mask 255.255.255.0, langkah pertama adalah masuk ke mode konfigurasi menggunakan perintah berikut:\\n[Router X] configure terminal\\nSelanjutnya, gunakan perintah `ip route` untuk menambahkan alamat IP tujuan, subnet mask, dan antarmuka. Akhir dari urutan perintah adalah sebagai berikut:\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\nend [eoc]Anda dapat melaksanakan rangkaian perintah sebagaimana dijelaskan di atas.', 'commands:[Router C] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router C dan pendaftaran alamat IP 192.168.2.0 dan 192.168.3.0 pada perutean, perlu dinyatakan ppemanggilan konfigurasi Router C terlebih dahulu dengan perintah\\n[Router C] configure terminal\\nLalu tambahkan perintah router rip dan version 2 untuk menandakan akan digunakan protokol RIP versi 2\\nrouter rip\\nversion 2\\nSetelah itu cantumkan alamat IP asal dan alamat IP tujuan dengan menggunakan perintah network. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router C] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\nend [eoc]', 'commands:[Router K] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router K dan mendaftarkan alamat IP 192.168.2.0 dan 192.168.3.0 pada perutean, langkah awal adalah memasuki mode konfigurasi pada Router K dengan perintah:\\n[Router K] configure terminal\\nSetelah itu, gunakan perintah `router rip` dan `version 2` untuk menandakan bahwa Anda akan menggunakan protokol RIP versi 2:\\nrouter rip\\nversion 2\\nSelanjutnya, tambahkan alamat IP sumber dan tujuan dengan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Rangkaian perintah dapat dijalankan sebagai berikut:\\n[Router K] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\nend [eoc]\\nAnda dapat melaksanakan rangkaian perintah seperti di atas.', 'commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router A dan memasukkan alamat IP 192.168.2.0 dan 192.168.3.0 pada tabel routing, langkah awal adalah mengakses mode konfigurasi pada Router A menggunakan perintah berikut:\\n[Router A] configure terminal\\nAfterward, utilize the `router rip` and `version 2` commands to specify the use of RIP protocol version 2:\\nrouter rip\\nversion 2\\nSelanjutnya, gabungkan alamat IP sumber dan tujuan menggunakan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Perintah-perintah tersebut dapat dijalankan sebagai berikut:\\n[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\nend [eoc]\\nAnda dapat menerapkan urutan perintah seperti yang ditunjukkan di atas.', 'commands:[Router K] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\n end [eoc],bot_reply:Untuk mengaktifkan VLAN 11 pada Router K dengan interface g0/0, alamat IP 192.168.0.1, dan subnet mask 255.255.255.0 kita perlu menyatakan pemanggilan konfigurasi Router K terlebih dahulu dengan menggunakan perintah\\n[Router K] configure terminal\\nLalu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask dapat digunakan perintah int, encapsulation, dan ip address\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router K] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\nend [eoc]', 'commands:[Router G] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\n end [eoc],bot_reply:Untuk mengaktifkan VLAN 11 pada Router G dengan alamat IP 192.168.0.1, interface g0/0, dan subnet mask 255.255.255.0  perlu dilakukan pemanggilan konfigurasi Router G terlebih dahulu dengan menggunakan perintah\\n[Router G] configure terminal\\nSetelah itu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask dapat digunakan perintah int, encapsulation, dan ip address\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah adalah sebagai berikut:\\n[Router G] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\nend [eoc]', 'commands:[Router A] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\n end [eoc],bot_reply:Anda dapat mengaktifkan VLAN 11 pada Router A dengan alamat IP 192.168.0.1, interface g0/0, dan subnet mask 255.255.255.0 dengan langkah pertama melakukan pemanggilan konfigurasi Router A terlebih dahulu dengan menggunakan perintah\\n[Router A] configure terminal\\nLalu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask gunakan perintah int, encapsulation, dan ip address\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Perintah lengkapnya, yaitu:\\n[Router A] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\nend [eoc]', 'commands:[Router G] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\n end [eoc],bot_reply:Untuk mengaktifkan VLAN 22 pada Router G dengan alamat IP 192.168.1.1, interface g0/0, dan subnet mask 255.255.255.0  perlu dilakukan pemanggilan konfigurasi Router G terlebih dahulu dengan menggunakan perintah\\n[Router G] configure terminal\\nSetelah itu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask dapat digunakan perintah int, encapsulation, dan ip address\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah adalah sebagai berikut:\\n[Router G] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\nend [eoc]', 'commands:[Router O] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\n end [eoc],bot_reply:Untuk mengaktifkan VLAN 22 pada Router O dengan interface g0/0, alamat IP 192.168.2.1, dan subnet mask 255.255.255.0 kita perlu menyatakan pemanggilan konfigurasi Router O terlebih dahulu dengan menggunakan perintah\\n[Router O] configure terminal\\nLalu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask dapat digunakan perintah int, encapsulation, dan ip address\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router O] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\nend [eoc]', 'commands:[Router I] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\n end [eoc],bot_reply:Anda dapat mengaktifkan VLAN 22 pada Router I dengan alamat IP 192.168.1.1, interface g0/0, dan subnet mask 255.255.255.0 dengan langkah pertama melakukan pemanggilan konfigurasi Router I terlebih dahulu dengan menggunakan perintah\\n[Router I] configure terminal\\nLalu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask gunakan perintah int, encapsulation, dan ip address\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Perintah lengkapnya, yaitu:\\n[Router I] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\nend [eoc]', 'commands:[Router A] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\n end [eoc],bot_reply:Untuk mengaktifkan VLAN 33 pada Router A dengan alamat IP 192.168.2.1, interface g0/0, dan subnet mask 255.255.255.0  perlu dilakukan pemanggilan konfigurasi Router A terlebih dahulu dengan menggunakan perintah\\n[Router A] configure terminal\\nSetelah itu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask dapat digunakan perintah int, encapsulation, dan ip address\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah adalah sebagai berikut:\\n[Router A] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\nend [eoc]', 'commands:[Router I] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\n end [eoc],bot_reply:Untuk mengaktifkan VLAN 33 pada Router I dengan interface g0/0, alamat IP 192.168.2.1, dan subnet mask 255.255.255.0 kita perlu menyatakan pemanggilan konfigurasi Router I terlebih dahulu dengan menggunakan perintah\\n[Router I] configure terminal\\nLalu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask dapat digunakan perintah int, encapsulation, dan ip address\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router I] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\nend [eoc]', 'commands:[Router Z] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\n end [eoc],bot_reply:Anda dapat mengaktifkan VLAN 33 pada Router Z dengan alamat IP 192.168.2.1, interface g0/0, dan subnet mask 255.255.255.0 dengan langkah pertama melakukan pemanggilan konfigurasi Router Z terlebih dahulu dengan menggunakan perintah\\n[Router Z] configure terminal\\nLalu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask gunakan perintah int, encapsulation, dan ip address\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Perintah lengkapnya, yaitu:\\n[Router Z] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\nend [eoc]', 'commands:[Router K] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\n end [eoc],bot_reply:Untuk membuat rute pada Router K melalui interface g/0/0/0 menuju jaringan lokal dengan IP 192.168.0.1, subnet mask 255.255.255.0 kita perlu menyatakan pemanggilan konfigurasi Router K terlebih dahulu dengan menggunakan perintah\\n[Router K] configure terminal\\nLalu dengan perintah ip route tambahkan alamat IP tujuan, subnet mask, dan interface. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router K] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\nend [eoc]', 'commands:[Router K] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\n end [eoc],bot_reply:Untuk mengkonfigurasi rute pada Router K untuk jaringan lokal dengan alamat IP 192.168.0.1 dan subnet mask 255.255.255.0, Anda harus mulai dengan mengakses mode konfigurasi menggunakan perintah:\\n[Router K] configure terminal\\nSelanjutnya, tambahkan alamat IP tujuan, subnet mask, dan antarmuka menggunakan perintah `ip route`. Urutan perintah diakhiri dengan yang berikut:\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\nend [eoc]\\nAnda dapat menjalankan serangkaian perintah seperti yang disebutkan di atas.', 'commands:[Router C] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\n end [eoc],bot_reply:Untuk mengatur rute pada Router C untuk jaringan lokal dengan alamat IP 192.168.0.1 dan subnet mask 255.255.255.0, langkah pertama adalah masuk ke mode konfigurasi menggunakan perintah berikut:\\n[Router C] configure terminal\\nSelanjutnya, gunakan perintah `ip route` untuk menambahkan alamat IP tujuan, subnet mask, dan antarmuka. Akhir dari urutan perintah adalah sebagai berikut:\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\nend [eoc]Anda dapat melaksanakan rangkaian perintah sebagaimana dijelaskan di atas.', 'commands:[Router X] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router X dan pendaftaran alamat IP 192.168.1.0 dan 192.168.2.0 pada perutean, perlu dinyatakan ppemanggilan konfigurasi Router X terlebih dahulu dengan perintah\\n[Router X] configure terminal\\nLalu tambahkan perintah router rip dan version 2 untuk menandakan akan digunakan protokol RIP versi 2\\nrouter rip\\nversion 2\\nSetelah itu cantumkan alamat IP asal dan alamat IP tujuan dengan menggunakan perintah network. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router X] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\nend [eoc]', 'commands:[Router Y] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router Y dan mendaftarkan alamat IP 192.168.1.0 dan 192.168.2.0 pada perutean, langkah awal adalah memasuki mode konfigurasi pada Router Y dengan perintah:\\n[Router Y] configure terminal\\nSetelah itu, gunakan perintah `router rip` dan `version 2` untuk menandakan bahwa Anda akan menggunakan protokol RIP versi 2:\\nrouter rip\\nversion 2\\nSelanjutnya, tambahkan alamat IP sumber dan tujuan dengan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Rangkaian perintah dapat dijalankan sebagai berikut:\\n[Router Y] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\nend [eoc]\\nAnda dapat melaksanakan rangkaian perintah seperti di atas.', 'commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router B dan memasukkan alamat IP 192.168.1.0 dan 192.168.2.0 pada tabel routing, langkah awal adalah mengakses mode konfigurasi pada Router B menggunakan perintah berikut:\\n[Router B] configure terminal\\nAfterward, utilize the `router rip` and `version 2` commands to specify the use of RIP protocol version 2:\\nrouter rip\\nversion 2\\nSelanjutnya, gabungkan alamat IP sumber dan tujuan menggunakan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Perintah-perintah tersebut dapat dijalankan sebagai berikut:\\n[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\nend [eoc]\\nAnda dapat menerapkan urutan perintah seperti yang ditunjukkan di atas.', 'commands:[Router B] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\n end [eoc],bot_reply:Untuk membuat rute pada Router B melalui interface g/1/0/0 menuju jaringan lokal dengan IP 192.168.3.0, subnet mask 255.255.255.0 kita perlu menyatakan pemanggilan konfigurasi Router B terlebih dahulu dengan menggunakan perintah\\n[Router B] configure terminal\\nLalu dengan perintah ip route tambahkan alamat IP tujuan, subnet mask, dan interface. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router B] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\nend [eoc]', 'commands:[Router Y] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\n end [eoc],bot_reply:Untuk mengkonfigurasi rute pada Router Y untuk jaringan lokal dengan alamat IP 192.168.3.0 dan subnet mask 255.255.255.0, Anda harus mulai dengan mengakses mode konfigurasi menggunakan perintah:\\n[Router Y] configure terminal\\nSelanjutnya, tambahkan alamat IP tujuan, subnet mask, dan antarmuka menggunakan perintah `ip route`. Urutan perintah diakhiri dengan yang berikut:\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\nend [eoc]\\nAnda dapat menjalankan serangkaian perintah seperti yang disebutkan di atas.', 'commands:[Router Y] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\n end [eoc],bot_reply:Untuk mengatur rute pada Router Y untuk jaringan lokal dengan alamat IP 192.168.3.0 dan subnet mask 255.255.255.0, langkah pertama adalah masuk ke mode konfigurasi menggunakan perintah berikut:\\n[Router Y] configure terminal\\nSelanjutnya, gunakan perintah `ip route` untuk menambahkan alamat IP tujuan, subnet mask, dan antarmuka. Akhir dari urutan perintah adalah sebagai berikut:\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\nend [eoc]Anda dapat melaksanakan rangkaian perintah sebagaimana dijelaskan di atas.', 'commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router B dan pendaftaran alamat IP 192.168.0.1 dan 192.168.2.0 pada perutean, perlu dinyatakan ppemanggilan konfigurasi Router B terlebih dahulu dengan perintah\\n[Router B] configure terminal\\nLalu tambahkan perintah router rip dan version 2 untuk menandakan akan digunakan protokol RIP versi 2\\nrouter rip\\nversion 2\\nSetelah itu cantumkan alamat IP asal dan alamat IP tujuan dengan menggunakan perintah network. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\nend [eoc]', 'commands:[Router Z] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router Z dan mendaftarkan alamat IP 192.168.0.1 dan 192.168.2.0 pada perutean, langkah awal adalah memasuki mode konfigurasi pada Router Z dengan perintah:\\n[Router Z] configure terminal\\nSetelah itu, gunakan perintah `router rip` dan `version 2` untuk menandakan bahwa Anda akan menggunakan protokol RIP versi 2:\\nrouter rip\\nversion 2\\nSelanjutnya, tambahkan alamat IP sumber dan tujuan dengan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Rangkaian perintah dapat dijalankan sebagai berikut:\\n[Router Z] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\nend [eoc]\\nAnda dapat melaksanakan rangkaian perintah seperti di atas.', 'commands:[Router I] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router I dan memasukkan alamat IP 192.168.0.1 dan 192.168.2.0 pada tabel routing, langkah awal adalah mengakses mode konfigurasi pada Router I menggunakan perintah berikut:\\n[Router I] configure terminal\\nAfterward, utilize the `router rip` and `version 2` commands to specify the use of RIP protocol version 2:\\nrouter rip\\nversion 2\\nSelanjutnya, gabungkan alamat IP sumber dan tujuan menggunakan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Perintah-perintah tersebut dapat dijalankan sebagai berikut:\\n[Router I] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\nend [eoc]\\nAnda dapat menerapkan urutan perintah seperti yang ditunjukkan di atas.', 'commands:[Router Z] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\n end [eoc],bot_reply:Untuk membuat rute pada Router Z melalui interface g/1/1/0 menuju jaringan lokal dengan IP 192.168.2.0, subnet mask 255.255.255.0 kita perlu menyatakan pemanggilan konfigurasi Router Z terlebih dahulu dengan menggunakan perintah\\n[Router Z] configure terminal\\nLalu dengan perintah ip route tambahkan alamat IP tujuan, subnet mask, dan interface. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router Z] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\nend [eoc]', 'commands:[Router A] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\n end [eoc],bot_reply:Untuk mengkonfigurasi rute pada Router A untuk jaringan lokal dengan alamat IP 192.168.2.0 dan subnet mask 255.255.255.0, Anda harus mulai dengan mengakses mode konfigurasi menggunakan perintah:\\n[Router A] configure terminal\\nSelanjutnya, tambahkan alamat IP tujuan, subnet mask, dan antarmuka menggunakan perintah `ip route`. Urutan perintah diakhiri dengan yang berikut:\\nip route 192.168.1.0 255.255.255.0 g1/1/0\\nend [eoc]\\nAnda dapat menjalankan serangkaian perintah seperti yang disebutkan di atas.', 'commands:[Router B] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\n end [eoc],bot_reply:Untuk mengatur rute pada Router B untuk jaringan lokal dengan alamat IP 192.168.2.0 dan subnet mask 255.255.255.0, langkah pertama adalah masuk ke mode konfigurasi menggunakan perintah berikut:\\n[Router B] configure terminal\\nSelanjutnya, gunakan perintah `ip route` untuk menambahkan alamat IP tujuan, subnet mask, dan antarmuka. Akhir dari urutan perintah adalah sebagai berikut:\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\nend [eoc]Anda dapat melaksanakan rangkaian perintah sebagaimana dijelaskan di atas.', 'commands:[Router K] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router K dan pendaftaran alamat IP 192.168.0.1 dan 192.168.3.0 pada perutean, perlu dinyatakan ppemanggilan konfigurasi Router K terlebih dahulu dengan perintah\\n[Router K] configure terminal\\nLalu tambahkan perintah router rip dan version 2 untuk menandakan akan digunakan protokol RIP versi 2\\nrouter rip\\nversion 2\\nSetelah itu cantumkan alamat IP asal dan alamat IP tujuan dengan menggunakan perintah network. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router K] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\nend [eoc]', 'commands:[Router G] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router G dan mendaftarkan alamat IP 192.168.0.1 dan 192.168.3.0 pada perutean, langkah awal adalah memasuki mode konfigurasi pada Router G dengan perintah:\\n[Router G] configure terminal\\nSetelah itu, gunakan perintah `router rip` dan `version 2` untuk menandakan bahwa Anda akan menggunakan protokol RIP versi 2:\\nrouter rip\\nversion 2\\nSelanjutnya, tambahkan alamat IP sumber dan tujuan dengan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Rangkaian perintah dapat dijalankan sebagai berikut:\\n[Router G] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\nend [eoc]\\nAnda dapat melaksanakan rangkaian perintah seperti di atas.', 'commands:[Router I] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router I dan memasukkan alamat IP 192.168.0.1 dan 192.168.3.0 pada tabel routing, langkah awal adalah mengakses mode konfigurasi pada Router I menggunakan perintah berikut:\\n[Router I] configure terminal\\nAfterward, utilize the `router rip` and `version 2` commands to specify the use of RIP protocol version 2:\\nrouter rip\\nversion 2\\nSelanjutnya, gabungkan alamat IP sumber dan tujuan menggunakan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Perintah-perintah tersebut dapat dijalankan sebagai berikut:\\n[Router I] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\nend [eoc]\\nAnda dapat menerapkan urutan perintah seperti yang ditunjukkan di atas.', 'commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router B dan pendaftaran alamat IP 192.168.1.0 dan 192.168.3.0 pada perutean, perlu dinyatakan ppemanggilan konfigurasi Router B terlebih dahulu dengan perintah\\n[Router B] configure terminal\\nLalu tambahkan perintah router rip dan version 2 untuk menandakan akan digunakan protokol RIP versi 2\\nrouter rip\\nversion 2\\nSetelah itu cantumkan alamat IP asal dan alamat IP tujuan dengan menggunakan perintah network. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\nend [eoc]', 'commands:[Router G] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router G dan mendaftarkan alamat IP 192.168.1.0 dan 192.168.3.0 pada perutean, langkah awal adalah memasuki mode konfigurasi pada Router G dengan perintah:\\n[Router G] configure terminal\\nSetelah itu, gunakan perintah `router rip` dan `version 2` untuk menandakan bahwa Anda akan menggunakan protokol RIP versi 2:\\nrouter rip\\nversion 2\\nSelanjutnya, tambahkan alamat IP sumber dan tujuan dengan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Rangkaian perintah dapat dijalankan sebagai berikut:\\n[Router G] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\nend [eoc]\\nAnda dapat melaksanakan rangkaian perintah seperti di atas.', 'commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router A dan memasukkan alamat IP 192.168.1.0 dan 192.168.3.0 pada tabel routing, langkah awal adalah mengakses mode konfigurasi pada Router A menggunakan perintah berikut:\\n[Router A] configure terminal\\nAfterward, utilize the `router rip` and `version 2` commands to specify the use of RIP protocol version 2:\\nrouter rip\\nversion 2\\nSelanjutnya, gabungkan alamat IP sumber dan tujuan menggunakan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Perintah-perintah tersebut dapat dijalankan sebagai berikut:\\n[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\nend [eoc]\\nAnda dapat menerapkan urutan perintah seperti yang ditunjukkan di atas.']\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "f = open('train.json')\n",
        "\n",
        "data = json.load(f)\n",
        "array = {'Prompt':[]}\n",
        "for i in data:\n",
        "    if 'bot_reply' not in i:\n",
        "        array['Prompt'].append('prompt:' + i['prompt'] + ',commands:' + i['completion'])\n",
        "    else:\n",
        "        array['Prompt'].append('commands:' + i['commands'] + ',bot_reply:' + i['bot_reply'])\n",
        "\n",
        "dataset = pd.DataFrame.from_dict(array)\n",
        "dataset.to_csv('/content/train_data.csv', index=False)\n",
        "\n",
        "print(array['Prompt'])\n",
        "# print(len(array['Prompt']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7hQvV8Qmr2V",
        "outputId": "50c9a888-6051-4ad5-89c9-cd4fa0676cf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['prompt:Berikan daftar interface yang ada di Router A,commands:[Router A] show interfaces [eoc]', 'prompt:Saya membutuhkan informasi tentang semua interface di Router A,commands:[Router A] show interfaces [eoc]', 'prompt:Tunjukkan daftar semua interface yang tersedia di Router A,commands:[Router A] show interfaces [eoc]', 'prompt:Beri saya daftar semua antarmuka yang terhubung ke Router A,commands:[Router A] show interfaces [eoc]', 'prompt:Saya ingin melihat daftar interface di Router B,commands:[Router B] show interfaces [eoc]', 'prompt:Tolong berikan informasi tentang semua interface yang tersedia di Router B,commands:[Router B] show interfaces [eoc]', 'prompt:Tunjukkan saya daftar semua antarmuka di Router B,commands:[Router B] show interfaces [eoc]', 'prompt:Berikan daftar semua antarmuka yang terhubung ke Router C,commands:[Router C] show interfaces [eoc]', 'prompt:Saya memerlukan informasi tentang semua interface di Router C,commands:[Router C] show interfaces [eoc]', 'prompt:Tunjukkan semua interface yang tersedia di Router C,commands:[Router C] show interfaces [eoc]', 'prompt:Beri saya daftar semua antarmuka yang terhubung ke Router D,commands:[Router D] show interfaces [eoc]', 'prompt:Mohon informasi tentang semua interface di Router D,commands:[Router D] show interfaces [eoc]', 'prompt:Tolong tunjukkan daftar semua antarmuka di Router D,commands:[Router D] show interfaces [eoc]', 'prompt:Berikan daftar semua antarmuka yang terhubung ke Router internal,commands:[Router internal] show interfaces [eoc]', 'prompt:Saya membutuhkan informasi tentang semua interface di Router external,commands:[Router external] show interfaces [eoc]', 'prompt:Tolong tunjukkan daftar semua antarmuka di Router R1,commands:[Router R1] show interfaces [eoc]', 'prompt:Mohon berikan daftar semua antarmuka yang terhubung ke Router R2,commands:[Router R2] show interfaces [eoc]', 'prompt:Tolong berikan daftar interface untuk Router B Interface G0/0/0,commands:[Router B] show interface g0/0/0 [eoc]', 'prompt:Tunjukkan daftar interface yang tersedia di Router A Interface F0/0,commands:[Router A] show interface f0/0 [eoc]', 'prompt:Berikan daftar semua interface yang terhubung ke Router A Interface F0/1,commands:[Router A] show interface f0/1 [eoc]', 'prompt:Saya membutuhkan informasi tentang antarmuka F1/0 di Router A,commands:[Router A] show interface f1/0 [eoc]', 'prompt:Berikan daftar semua interface untuk Router B G0/1/1,commands:[Router B] show interface g0/1/1 [eoc]', 'prompt:Tolong tunjukkan daftar interface yang tersedia di Router B Interface G1/1/1,commands:[Router B] show interface g1/1/1 [eoc]', 'prompt:Beri saya daftar semua antarmuka yang tersedia di Router C F0/1,commands:[Router C] show interface f0/1 [eoc]', 'prompt:Saya membutuhkan informasi tentang interface G1/0/0 di Router R1,commands:[Router R1] show interface g1/0/0 [eoc]', 'prompt:Tunjukkan daftar semua interface yang terhubung ke Router R2 Interface G0/0/0,commands:[Router R2] show interface g0/0/0 [eoc]', 'prompt:Berikan daftar semua interface untuk Router R2 G0/0/1,commands:[Router R2] show interface g0/0/1 [eoc]', 'prompt:Tolong berikan daftar interface untuk Router R1 Interface F0/1,commands:[Router R1] show interface f0/1 [eoc]', 'prompt:Tolong ubah IP di Router C Interface X menjadi 192.168.1.1,commands:[Router C] configure terminal\\ninterface X\\nip address 192.168.1.1 255.255.255.0\\nend [eoc]', 'prompt:Ubah IP di Router A Interface G0/0/1 menjadi 10.0.0.1,commands:[Router A] configure terminal\\ninterface G0/0/1\\nip address 10.0.0.1 255.255.255.0\\nend [eoc]', 'prompt:Ubah IP di Router B Interface F0/1 menjadi 172.16.0.1,commands:[Router B] configure terminal\\ninterface F0/1\\nip address 172.16.0.1 255.255.255.0\\nend [eoc]', 'prompt:Tolong ubah IP di Internal Router Interface G1/0/0 menjadi 192.168.2.1,commands:[Internal Router] configure terminal\\ninterface G1/0/0\\nip address 192.168.2.1 255.255.255.0\\nend [eoc]', 'prompt:Saya butuh bantuan untuk mengubah IP di External Router Interface F1/0 menjadi 10.1.1.1,commands:[External Router] configure terminal\\ninterface F1/0\\nip address 10.1.1.1 255.255.255.0\\nend [eoc]', 'prompt:Mohon bantu untuk mengganti IP di R1 Interface G0/0/0 menjadi 192.168.3.1,commands:[Router R1] configure terminal\\ninterface G0/0/0\\nip address 192.168.3.1 255.255.255.0\\nend [eoc]', 'prompt:Mohon bantu untuk mengubah IP di Router D Interface F0/1 menjadi 172.16.10.1,commands:[Router D] configure terminal\\ninterface F0/1\\nip address 172.16.10.1 255.255.255.0\\nend [eoc]', 'prompt:Tolong ubah IP di Router E Interface G0/0 menjadi 192.168.5.1,commands:[Router E] configure terminal\\ninterface G0/0\\nip address 192.168.5.1 255.255.255.0\\nend [eoc]', 'prompt:Berikan bantuan untuk mengganti IP di Router F Interface F1/0 menjadi 10.1.2.1,commands:[Router F] configure terminal\\ninterface F1/0\\nip address 10.1.2.1 255.255.255.0\\nend [eoc]', 'prompt:Ubah IP di Router G Interface G0/0/1 menjadi 10.0.1.1,commands:[Router G] configure terminal\\ninterface G0/0/1\\nip address 10.0.1.1 255.255.255.0\\nend [eoc]', 'prompt:Mohon bantu untuk mengubah IP di R2 Interface F0/0 menjadi 192.168.4.1,commands:[Router R2] configure terminal\\ninterface F0/0\\nip address 192.168.4.1 255.255.255.0\\nend [eoc]', 'prompt:Tolong tambahkan kunci enkripsi ke Router D,commands:[Router D] configure terminal\\ncrypto key generate rsa\\nend [eoc]', 'prompt:Mohon tambahkan kunci enkripsi ke Router E,commands:[Router E] configure terminal\\ncrypto key generate rsa\\nend [eoc]', 'prompt:Tolong buat kunci enkripsi di Router F,commands:[Router F] configure terminal\\ncrypto key generate rsa\\nend [eoc]', 'prompt:Mohon tambahkan kunci enkripsi ke Router G,commands:[Router G] configure terminal\\ncrypto key generate rsa\\nend [eoc]', 'prompt:Tolong aktifkan enkripsi di Router H,commands:[Router H] configure terminal\\nservice password-encryption\\nend [eoc]', 'prompt:Mohon bantu untuk menambahkan kunci enkripsi di R1,commands:[Router R1] configure terminal\\ncrypto key generate rsa\\nend [eoc]', 'prompt:Saya ingin melihat daftar semua antarmuka di Router D,commands:[Router D] show interfaces [eoc]', 'prompt:Tunjukkan semua interface yang terhubung ke Router external,commands:[Router external] show interfaces [eoc]', 'prompt:Beri saya daftar semua antarmuka yang terhubung ke Router internal,commands:[Router internal] show interfaces [eoc]', 'prompt:Bagaimana cara mengkonfigurasi OSPF pada Router A?,commands:[Router A] router ospf 1\\nnetwork 10.0.0.0 0.255.255.255 area 0\\nend [eoc]', \"prompt:Berikan contoh penggunaan perintah 'ping' pada Router B,commands:[Router B] ping 10.0.0.1 [eoc]\", 'prompt:Bagaimana cara mengubah hostname pada Router C?,commands:[Router C] configure terminal\\nhostname NewHostName\\nend [eoc]', 'prompt:Saya ingin mengecek routing table pada Router A,commands:[Router A] show ip route [eoc]', 'prompt:Bagaimana cara mengkonfigurasi VLAN pada Switch A?,commands:[Switch A] vlan 10\\nname Sales\\nend [eoc]', 'prompt:Tolong buatkan ACL untuk memblokir akses ke situs web tertentu pada Router D,commands:[Router D] configure terminal\\naccess-list 101 deny tcp any host 192.168.1.2 eq 80\\naccess-list 101 permit ip any any\\ninterface gigabitEthernet 0/0\\nip access-group 101 in\\nend [eoc]', 'prompt:Bagaimana cara menampilkan port yang sedang aktif pada Switch B?,commands:[Switch B] show interface status [eoc]', 'prompt:Saya ingin mengkonfigurasi DHCP pada Router internal,commands:[Internal Router] configure terminal\\ndhcp pool InternalPool\\nnetwork 192.168.0.0 255.255.255.0\\ndefault-router 192.168.0.1\\nend [eoc]', 'prompt:Bagaimana cara menampilkan MAC address table pada Switch C?,commands:[Switch C] show mac-address-table [eoc]', 'prompt:Saya ingin mengkonfigurasi NAT pada Router external,commands:[Router external] configure terminal\\ninterface gigabitEthernet 0/0\\nip nat outside\\ninterface gigabitEthernet 0/1\\nip nat inside\\nip nat inside source list 1 interface gigabitEthernet 0/0 overload\\naccess-list 1 permit 192.168.0.0 0.0.255.255\\nend [eoc]', 'prompt:Bagaimana cara menampilkan log yang tersimpan pada Router R1?,commands:[Router R1] show logging [eoc]', 'prompt:Saya ingin memeriksa status koneksi pada Router R2,commands:[Router R2] show interfaces brief [eoc]', 'prompt:Bagaimana cara mengkonfigurasi trunking pada Switch D?,commands:[Switch D] interface gigabitEthernet 0/0\\nswitchport mode trunk\\nend [eoc]', 'prompt:Tolong buatkan static route pada Router internal untuk mengakses subnet 172.16.0.0/24 melalui Router A,commands:[Internal Router] ip route 172.16.0.0 255.255.255.0 10.0.0.2 [eoc]', 'prompt:Bagaimana cara menghapus VLAN yang tidak dibutuhkan pada Switch E?,commands:[Switch E] configure terminal\\nno vlan 10\\nend [eoc]', 'prompt:Saya ingin menampilkan informasi tentang status port pada Switch F,commands:[Switch F] show interface gigabitEthernet 0/1 [eoc]', 'prompt:Tunjukkan daftar konfigurasi yang ada di Router A,commands:[Router A] show running-config [eoc]', 'prompt:Beri saya daftar semua konfigurasi di Router A,commands:[Router A] show running-config [eoc]', 'prompt:Saya ingin melihat konfigurasi di Router B,commands:[Router B] show running-config [eoc]', 'prompt:Tolong berikan daftar semua konfigurasi di Router B,commands:[Router B] show running-config [eoc]', 'prompt:Berikan daftar semua konfigurasi di Router C,commands:[Router C] show running-config [eoc]', 'prompt:Saya memerlukan informasi tentang konfigurasi di Router C,commands:[Router C] show running-config [eoc]', 'prompt:Tunjukkan semua konfigurasi di Router D,commands:[Router D] show running-config [eoc]', 'prompt:Berikan daftar semua konfigurasi di Router internal,commands:[Router internal] show running-config [eoc]', 'prompt:Saya membutuhkan informasi tentang semua konfigurasi di Router external,commands:[Router external] show running-config [eoc]', 'prompt:Tolong tunjukkan daftar semua konfigurasi di Router R1,commands:[Router R1] show running-config [eoc]', 'prompt:Mohon berikan daftar semua konfigurasi di Router R2,commands:[Router R2] show running-config [eoc]', 'prompt:Tunjukkan daftar konfigurasi untuk Router B Interface G0/0/0,commands:[Router B] show running-config interface g0/0/0 [eoc]', 'prompt:Berikan konfigurasi untuk Router A Interface F0/0,commands:[Router A] show running-config interface f0/0 [eoc]', 'prompt:Saya membutuhkan informasi tentang konfigurasi di Router A Interface F0/1,commands:[Router A] show running-config interface f0/1 [eoc]', 'prompt:Tolong berikan konfigurasi untuk interface F1/0 di Router A,commands:[Router A] show running-config interface f1/0 [eoc]', 'prompt:Berikan konfigurasi untuk Router B G0/1/1,commands:[Router B] show running-config interface g0/1/1 [eoc]', 'prompt:Tunjukkan daftar konfigurasi yang terhubung ke Router B Interface G1/1/1,commands:[Router B] show running-config interface g1/1/1 [eoc]', 'prompt:Beri saya konfigurasi semua antarmuka yang tersedia di Router C F0/1,commands:[Router C] show running-config interface f0/1 [eoc]', 'prompt:Saya membutuhkan informasi tentang konfigurasi di interface G1/0/0 di Router R1,commands:[Router R1] show running-config interface g1/0/0 [eoc]', 'prompt:Tunjukkan daftar konfigurasi semua interface yang terhubung ke Router R2 Interface G0/0/0,commands:[Router R2] show running-config interface g0/0/0 [eoc]', 'prompt:Berikan konfigurasi untuk semua interface pada Router R2 G0/0/1,commands:[Router R2] show running-config interface g0/0/1 [eoc]', 'prompt:Beri saya daftar konfigurasi di Router D yang terkait dengan VLAN 10,commands:[Router D] show running-config | include vlan 10 [eoc]', 'prompt:Tunjukkan konfigurasi yang berhubungan dengan OSPF di Router A,commands:[Router A] show running-config | include ospf [eoc]', 'prompt:Beri saya daftar semua konfigurasi yang terkait dengan NAT di Router B,commands:[Router B] show running-config | include nat [eoc]', 'prompt:Saya ingin melihat daftar konfigurasi yang terkait dengan DHCP di Router C,commands:[Router C] show running-config | include dhcp [eoc]', 'prompt:Tolong tunjukkan daftar semua konfigurasi yang terkait dengan firewall di Router internal,commands:[Router internal] show running-config | include firewall [eoc]', 'prompt:Berikan konfigurasi untuk interface G0/0 di Router external,commands:[Router external] show running-config interface g0/0 [eoc]', 'prompt:Saya memerlukan informasi tentang konfigurasi VRF di Router R1,commands:[Router R1] show running-config | include vrf [eoc]', 'prompt:Tunjukkan daftar konfigurasi terkait dengan access-list di Router R2,commands:[Router R2] show running-config | include access-list [eoc]', 'prompt:Berikan daftar semua konfigurasi OSPF yang terkait dengan area 0 di Router A,commands:[Router A] show running-config | include ospf.*area 0 [eoc]', 'prompt:Saya ingin melihat konfigurasi NAT yang terkait dengan subnet 192.168.1.0/24 di Router B,commands:[Router B] show running-config | include nat.*192.168.1.0/24 [eoc]', 'prompt:Tolong berikan daftar konfigurasi BGP di Router C,commands:[Router C] show running-config | include bgp [eoc]', 'prompt:Saya membutuhkan informasi tentang konfigurasi QoS di Router internal,commands:[Router internal] show running-config | include qos [eoc]', 'prompt:Tunjukkan daftar konfigurasi yang terkait dengan MPLS di Router external,commands:[Router external] show running-config | include mpls [eoc]', 'prompt:Berikan konfigurasi untuk interface F0/1 di Router R1,commands:[Router R1] show running-config interface f0/1 [eoc]', 'prompt:Saya memerlukan informasi tentang konfigurasi VPC di Router R2,commands:[Router R2] show running-config | include vpc [eoc]', 'prompt:Tolong tunjukkan daftar konfigurasi yang terkait dengan IPsec di Router A,commands:[Router A] show running-config | include ipsec [eoc]', 'prompt:Tolong atur VLAN 10 pada Switch A dan hubungkan ke port GigabitEthernet1/0/5,commands:[Switch A] configure terminal\\nvlan 10\\nname VLAN10\\ninterface GigabitEthernet1/0/5\\nswitchport mode access\\nswitchport access vlan 10\\nend [eoc]', 'prompt:Tambahkan static route pada Router D untuk subnet 192.168.10.0/24 melalui Router B dengan metric 10,commands:[Router D] configure terminal\\nip route 192.168.10.0 255.255.255.0 Router B 10\\nend [eoc]', \"prompt:Tolong tambahkan user baru bernama 'john' dengan password 'password123' pada server MySQL,commands:CREATE USER 'john'@'localhost' IDENTIFIED BY 'password123';\\nGRANT ALL PRIVILEGES ON * . * TO 'john'@'localhost';\\nFLUSH PRIVILEGES; [eoc]\", 'prompt:Tolong tambahkan firewall rule pada server web untuk memblokir akses dari IP address 192.168.1.100,commands:sudo ufw deny from 192.168.1.100 [eoc]', 'prompt:Tolong buat konfigurasi NAT pada Router A untuk mengakses jaringan internal (192.168.1.0/24) dari internet,commands:[Router A] configure terminal\\ninterface GigabitEthernet0/0/1\\nip nat outside\\nexit\\ninterface GigabitEthernet0/0/0\\nip nat inside\\nexit\\nip nat inside source list 1 interface GigabitEthernet0/0/1 overload\\naccess-list 1 permit 192.168.1.0 0.0.0.255\\nend [eoc]', 'prompt:Buat route static menuju network 192.168.1.0 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0 di Router A,commands:[Router A] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Ciptakan skrip konfigurasi yang akan menetapkan route statis pada Router A untuk mengarahkan lalu lintas ke jaringan 192.168.1.0 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0,commands:[Router A] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Konfigurasikan routing statis di Router A untuk mengarahkan lalu lintas ke jaringan 192.168.1.0 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0,commands:[Router A] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Ciptakan konfigurasi perutean yang tepat pada Router A untuk memungkinkan lalu lintas menuju jaringan 192.168.1.0 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0,commands:[Router A] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Tulis skrip perutean yang diperlukan untuk mengarahkan semua paket menuju jaringan 192.168.1.0 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0 di Router A,commands:[Router A] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router A dan perkenalkan network 192.168.2.0, 192.168.3.0,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Lakukan konfigurasi pada Router A untuk mengaktifkan protokol RIP versi 2 dan perkenalkan network 192.168.2.0 dan 192.168.3.0 ke dalam perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router A dan pastikan network 192.168.2.0 dan 192.168.3.0 dikenali oleh perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Lakukan langkah-langkah untuk mengaktifkan protokol RIP versi 2 pada Router A dan pastikan network 192.168.2.0 dan 192.168.3.0 dapat diakses melalui perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router A sekarang juga dan segera perkenalkan network 192.168.2.0 dan 192.168.3.0 ke dalam tabel perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 dengan IP address 192.168.0.1 subnet 255.255.255.0 pada interface g0/0 di Router A,commands:[Router A] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router A dan berikan IP address 192.168.0.1 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 11 pada interface g0/0 di Router A dengan IP address 192.168.0.1 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router A dan tetapkan IP address 192.168.0.1 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router A dan pastikan menggunakan IP address 192.168.0.1 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 dengan IP address 192.168.1.1 subnet 255.255.255.0 pada interface g0/0 di Router A,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g0/0 di Router A dan berikan IP address 192.168.1.1 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 22 pada interface g0/0 di Router A dengan IP address 192.168.1.1 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g0/0 di Router A dan tetapkan IP address 192.168.1.1 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g0/0 di Router A dan pastikan menggunakan IP address 192.168.1.1 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 dengan IP address 192.168.2.1 subnet 255.255.255.0 pada interface g0/0 di Router A,commands:[Router A] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 pada interface g0/0 di Router A dan berikan IP address 192.168.2.1 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 33 pada interface g0/0 di Router A dengan IP address 192.168.2.1 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 pada interface g0/0 di Router A dan tetapkan IP address 192.168.2.1 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 pada interface g0/0 di Router A dan pastikan menggunakan IP address 192.168.2.1 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Buat route static menuju network 192.168.0.1 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0 di Router B,commands:[Router B] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Ciptakan skrip konfigurasi yang akan menetapkan route statis pada Router B untuk mengarahkan lalu lintas ke jaringan 192.168.0.1 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0,commands:[Router B] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Konfigurasikan routing statis di Router B untuk mengarahkan lalu lintas ke jaringan 192.168.0.1 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0,commands:[Router B] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Ciptakan konfigurasi perutean yang tepat pada Router B untuk memungkinkan lalu lintas menuju jaringan 192.168.0.1 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0,commands:[Router B] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Tulis skrip perutean yang diperlukan untuk mengarahkan semua paket menuju jaringan 192.168.0.1 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0 di Router B,commands:[Router B] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router B dan perkenalkan network 192.168.1.0, 192.168.2.0,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Lakukan konfigurasi pada Router B untuk mengaktifkan protokol RIP versi 2 dan perkenalkan network 192.168.1.0 dan 192.168.2.0 ke dalam perutean,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router B dan pastikan network 192.168.1.0 dan 192.168.2.0 dikenali oleh perutean,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Lakukan langkah-langkah untuk mengaktifkan protokol RIP versi 2 pada Router B dan pastikan network 192.168.1.0 dan 192.168.2.0 dapat diakses melalui perutean,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router B sekarang juga dan segera perkenalkan network 192.168.1.0 dan 192.168.2.0 ke dalam tabel perutean,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 dengan IP address 192.168.0.2 subnet 255.255.255.0 pada interface g0/0 di Router B,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.2 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 11 pada interface g0/0 di Router B dengan IP address 192.168.0.2 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router B dan berikan IP address 192.168.0.2 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router B dan tetapkan IP address 192.168.0.2 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router B dan pastikan menggunakan IP address 192.168.0.2 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 dengan IP address 192.168.2.1 subnet 255.255.255.0 pada interface g0/0 di Router B,commands:[Router B] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g0/0 di Router B dan berikan IP address 192.168.2.1 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 22 pada interface g0/0 di Router B dengan IP address 192.168.2.1 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g0/0 di Router B dan tetapkan IP address 192.168.2.1 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g0/0 di Router B dan pastikan menggunakan IP address 192.168.2.1 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.1 255.255.255.0\\n end [eoc]', 'prompt:Buat route static menuju network 192.168.3.0 dengan subnet mask 255.255.255.0 melalui interface g/1/0/0 di Router A,commands:[Router A] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\n end [eoc]', 'prompt:Ciptakan skrip konfigurasi yang akan menetapkan route statis pada Router A untuk mengarahkan lalu lintas ke jaringan 192.168.3.0 dengan subnet mask 255.255.255.0 melalui interface g/1/0/0,commands:[Router A] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\n end [eoc]', 'prompt:Konfigurasikan routing statis di Router A untuk mengarahkan lalu lintas ke jaringan 192.168.3.0 dengan subnet mask 255.255.255.0 melalui interface g/1/0/0,commands:[Router A] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\n end [eoc]', 'prompt:Ciptakan konfigurasi perutean yang tepat pada Router A untuk memungkinkan lalu lintas menuju jaringan 192.168.3.0 dengan subnet mask 255.255.255.0 melalui interface g/1/0/0,commands:[Router A] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\n end [eoc]', 'prompt:Tulis skrip perutean yang diperlukan untuk mengarahkan semua paket menuju jaringan 192.168.3.0 dengan subnet mask 255.255.255.0 melalui interface g/1/0/0 di Router A,commands:[Router A] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router A dan perkenalkan network 192.168.0.1, 192.168.2.0,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Lakukan konfigurasi pada Router A untuk mengaktifkan protokol RIP versi 2 dan perkenalkan network 192.168.0.1 dan 192.168.2.0 ke dalam perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router A dan pastikan network 192.168.0.1 dan 192.168.2.0 dikenali oleh perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Lakukan langkah-langkah untuk mengaktifkan protokol RIP versi 2 pada Router A dan pastikan network 192.168.0.1 dan 192.168.2.0 dapat diakses melalui perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router A sekarang juga dan segera perkenalkan network 192.168.0.1 dan 192.168.2.0 ke dalam tabel perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\n end [eoc]', 'prompt:Buat route static menuju network 192.168.2.0 dengan subnet mask 255.255.255.0 melalui interface g/1/1/0 di Router A,commands:[Router A] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\n end [eoc]', 'prompt:Ciptakan skrip konfigurasi yang akan menetapkan route statis pada Router A untuk mengarahkan lalu lintas ke jaringan 192.168.2.0 dengan subnet mask 255.255.255.0 melalui interface g/1/1/0,commands:[Router A] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\n end [eoc]', 'prompt:Konfigurasikan routing statis di Router A untuk mengarahkan lalu lintas ke jaringan 192.168.2.0 dengan subnet mask 255.255.255.0 melalui interface g/1/1/0,commands:[Router A] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\n end [eoc]', 'prompt:Ciptakan konfigurasi perutean yang tepat pada Router A untuk memungkinkan lalu lintas menuju jaringan 192.168.2.0 dengan subnet mask 255.255.255.0 melalui interface g/1/1/0,commands:[Router A] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\n end [eoc]', 'prompt:Tulis skrip perutean yang diperlukan untuk mengarahkan semua paket menuju jaringan 192.168.2.0 dengan subnet mask 255.255.255.0 melalui interface g/1/1/0 di Router A,commands:[Router A] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router A dan perkenalkan network 192.168.0.1, 192.168.3.0,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Aktifkan versi 2 dari protokol RIP di Router A dan tambahkan jaringan 192.168.0.1 dan 192.168.3.0 ke dalam konfigurasi.,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Menambahkan  jaringan dengan alamat IP 192.168.0.1 dan 192.168.3.0 ke dalam konfigurasi lalu aktifkan protokol RIP versi 2 pada Router A,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Tambahkan jaringan 192.168.0.1 dan 192.168.3.0 ke dalam konfigurasi, kemudian aktifkan protokol RIP versi 2 pada Router A,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Lakukan langkah-langkah untuk mengaktifkan protokol RIP versi 2 pada Router A dan pastikan network 192.168.0.1 dan 192.168.3.0 dapat diakses melalui perutean,commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router B dan perkenalkan network 192.168.1.0, 192.168.3.0,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Pada Router A, tambahkan network 192.168.0.1 serta 192.168.3.0 dan aktifkan protokol RIP versi 2,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Tambahkan jaringan 192.168.0.1 dan 192.168.3.0 ke dalam konfigurasi Router A, serta aktifkan protokol RIP versi 2.,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Aktifkan protokol RIP versi 2 pada Router A, serta pastikan jaringan 192.168.0.1 serta 192.168.3.0 sudah dikenali pada konfigurasi perutean,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Lakukan pengaktifan protokol RIP versi 2 pada Router A dan pastikan bahwa jaringan 192.168.0.1 dan 192.168.3.0 telah terdaftar dalam konfigurasi routing,commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 dengan IP address 192.168.0.1 subnet 255.255.255.0 pada interface g0/0 di Router B,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Mengaktifkan VLAN 33 pada antarmuka g0/0 di Router B dengan alamat IP 192.168.0.1 dan subnet 255.255.255.0.,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 33 pada interface g0/0 di Router B dengan IP address 192.168.0.1 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Konfigurasikan VLAN 33 pada antarmuka g0/0 di Router B dengan alamat IP 192.168.0.1 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan konfigurasi VLAN 33 pada interface g0/0 di Router B dengan alamat IP 192.168.0.1 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.1 255.255.255.0\\n end [eoc]', 'prompt:Buat route static menuju network 192.168.3.0 dengan subnet mask 255.255.255.0 melalui interface g/1/0/1 di Router B,commands:[Router B] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/1\\n end [eoc]', 'prompt:Lakukan konfigurasi yang menetapkan route statis pada Router B untuk mengarahkan lalu lintas ke jaringan 192.168.3.0 dengan subnet mask 255.255.255.0 melalui interface g/1/0/1,commands:[Router B] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/1\\n end [eoc]', 'prompt:Lakukan pengaturan konfigurasi pada Router B untuk mengonfigurasi rute statis yang mengarahkan lalu lintas ke jaringan 192.168.3.0 dengan subnet mask 255.255.255.0 melalui antarmuka g/1/0/1.,commands:[Router B] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/1\\n end [eoc]', 'prompt:Tulis skrip perutean yang diperlukan untuk mengarahkan semua paket menuju jaringan 192.168.3.0 dengan subnet mask 255.255.255.0 melalui interface g/1/0/1 di Router B,commands:[Router B] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/1\\n end [eoc]', 'prompt:Tuliskan skrip rute yang diperlukan untuk mengalihkan semua paket ke jaringan 192.168.3.0 dengan subnet mask 255.255.255.0 melalui antarmuka g/1/0/1 di Router B.,commands:[Router B] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/1\\n end [eoc]', 'prompt:Aktifkan VLAN 33 dengan IP address 192.168.3.0 subnet 255.255.255.0 pada interface g1/0 di Router B,commands:[Router B] configure terminal\\nint g1/0.33\\nencapsulation dot1q 33\\nip address 192.168.3.0 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 33 pada antarmuka g1/0 di Router B dengan IP address 192.168.3.0 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g1/0.33\\nencapsulation dot1q 33\\nip address 192.168.3.0 255.255.255.0\\n end [eoc]', 'prompt:Konfigurasikan VLAN 33 pada interface Router B g1/0 dengan alamat IP 192.168.3.0 dan subnet mask 255.255.255.0.,commands:[Router B] configure terminal\\nint g1/0.33\\nencapsulation dot1q 33\\nip address 192.168.3.0 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan konfigurasi VLAN 33 pada interface g1/0 di Router B dengan alamat IP 192.168.3.0 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g1/0.33\\nencapsulation dot1q 33\\nip address 192.168.3.0 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 pada interface g1/0 di Router B dan pastikan menggunakan IP address 192.168.3.0 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g1/0.33\\nencapsulation dot1q 33\\nip address 192.168.3.0 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 dengan IP address 192.168.2.2 subnet 255.255.255.0 pada interface g0/1 di Router A,commands:[Router A] configure terminal\\nint g0/1.11\\nencapsulation dot1q 11\\nip address 192.168.2.2 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 11 pada interface g0/1 di Router A dengan IP address 192.168.2.2 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/1.11\\nencapsulation dot1q 11\\nip address 192.168.2.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/1 di Router A dan berikan IP address 192.168.2.2 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/1.11\\nencapsulation dot1q 11\\nip address 192.168.2.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/1 di Router A dan tetapkan IP address 192.168.2.2 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/1.11\\nencapsulation dot1q 11\\nip address 192.168.2.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/1 di Router A dan pastikan menggunakan IP address 192.168.2.2 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/1.11\\nencapsulation dot1q 11\\nip address 192.168.2.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 dengan IP address 192.168.1.1 subnet 255.255.255.0 pada interface g0/0 di Router B,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 33 pada antarmuka g0/0 di Router B dengan IP address 192.168.1.1 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Konfigurasikan VLAN 33 pada interface Router B g0/0 dengan alamat IP 192.168.1.1 dan subnet mask 255.255.255.0.,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan konfigurasi VLAN 33 pada interface g0/0 di Router B dengan alamat IP 192.168.1.1 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 pada interface g0/0 di Router B dan pastikan menggunakan IP address 192.168.1.1 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.1.1 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 dengan IP address 192.168.0.3 subnet 255.255.255.0 pada interface g0/1 di Router B,commands:[Router B] configure terminal\\nint g0/1.22\\nencapsulation dot1q 22\\nip address 192.168.0.3 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 22 pada interface g0/1 di Router B dengan IP address 192.168.0.3 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/1.22\\nencapsulation dot1q 22\\nip address 192.168.0.3 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/1 di Router B dan berikan IP address 192.168.0.3 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/1.22\\nencapsulation dot1q 22\\nip address 192.168.0.3 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/1 di Router B dan tetapkan IP address 192.168.0.3 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/1.22\\nencapsulation dot1q 22\\nip address 192.168.0.3 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/1 di Router B dan pastikan menggunakan IP address 192.168.0.3 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/1.22\\nencapsulation dot1q 22\\nip address 192.168.0.3 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 dengan IP address 192.168.2.3 subnet 255.255.255.0 pada interface g0/0 di Router A,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.3 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 2 pada interface g0/0 di Router A dan berikan IP address 192.168.2.3 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.3 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 22 pada interface g0/0 di Router A dengan IP address 192.168.2.3 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.3 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g0/0 di Router A dan tetapkan IP address 192.168.2.3 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.3 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g0/0 di Router A dan pastikan menggunakan IP address 192.168.2.3 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.2.3 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 dengan IP address 192.168.2.4 subnet 255.255.255.0 pada interface g1/1 di Router A,commands:[Router A] configure terminal\\nint g1/1.22\\nencapsulation dot1q 22\\nip address 192.168.2.4 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 22 pada interface g1/1 di Router A dengan IP address 192.168.2.4 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g1/1.22\\nencapsulation dot1q 22\\nip address 192.168.2.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g1/1 di Router A dan berikan IP address 192.168.2.4 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g1/1.22\\nencapsulation dot1q 22\\nip address 192.168.2.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g1/1 di Router A dan tetapkan IP address 192.168.2.4 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g1/1.22\\nencapsulation dot1q 22\\nip address 192.168.2.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 22 pada interface g1/1 di Router A dan pastikan menggunakan IP address 192.168.2.4 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g1/1.22\\nencapsulation dot1q 22\\nip address 192.168.2.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 dengan IP address 192.168.0.4 subnet 255.255.255.0 pada interface g0/0 di Router B,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 pada interface g0/0 di Router B dan berikan IP address 192.168.0.4 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.4 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 33 pada interface g0/0 di Router B dengan IP address 192.168.0.4 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 pada interface g0/0 di Router B dan tetapkan IP address 192.168.0.4 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 33 pada interface g0/0 di Router B dan pastikan menggunakan IP address 192.168.0.4 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.0.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 dengan IP address 192.168.1.2 subnet 255.255.255.0 pada interface g0/0 di Router B,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.1.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router B dan berikan IP address 192.168.1.2 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.1.2 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 11 pada interface g0/0 di Router B dengan IP address 192.168.1.2 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.1.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router B dan tetapkan IP address 192.168.1.2 dengan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.1.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g0/0 di Router B dan pastikan menggunakan IP address 192.168.1.2 dan subnet 255.255.255.0,commands:[Router B] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.1.2 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 dengan IP address 192.168.1.4 subnet 255.255.255.0 pada interface g1/1 di Router A,commands:[Router A] configure terminal\\nint g1/1.11\\nencapsulation dot1q 11\\nip address 192.168.1.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g1/1 di Router A dan berikan IP address 192.168.1.4 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g1/1.11\\nencapsulation dot1q 11\\nip address 192.168.1.4 255.255.255.0\\n end [eoc]', 'prompt:Lakukan konfigurasi VLAN 11 pada interface g1/1 di Router A dengan IP address 192.168.1.4 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g1/1.11\\nencapsulation dot1q 11\\nip address 192.168.1.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g1/1 di Router A dan tetapkan IP address 192.168.1.4 dengan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g1/1.11\\nencapsulation dot1q 11\\nip address 192.168.1.4 255.255.255.0\\n end [eoc]', 'prompt:Aktifkan VLAN 11 pada interface g1/1 di Router A dan pastikan menggunakan IP address 192.168.1.4 dan subnet 255.255.255.0,commands:[Router A] configure terminal\\nint g1/1.11\\nencapsulation dot1q 11\\nip address 192.168.1.4 255.255.255.0\\n end [eoc]', 'commands:[Router Z] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\n end [eoc],bot_reply:Untuk membuat rute pada Router Z melalui interface g/0/0/0 menuju jaringan lokal dengan IP 192.168.1.0, subnet mask 255.255.255.0 kita perlu menyatakan pemanggilan konfigurasi Router Z terlebih dahulu dengan menggunakan perintah\\n[Router Z] configure terminal\\nLalu dengan perintah ip route tambahkan alamat IP tujuan, subnet mask, dan interface. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router Z] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\nend [eoc]', 'commands:[Router I] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\n end [eoc],bot_reply:Untuk mengkonfigurasi rute pada Router I untuk jaringan lokal dengan alamat IP 192.168.1.0 dan subnet mask 255.255.255.0, Anda harus mulai dengan mengakses mode konfigurasi menggunakan perintah:\\n[Router I] configure terminal\\nSelanjutnya, tambahkan alamat IP tujuan, subnet mask, dan antarmuka menggunakan perintah `ip route`. Urutan perintah diakhiri dengan yang berikut:\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\nend [eoc]\\nAnda dapat menjalankan serangkaian perintah seperti yang disebutkan di atas.', 'commands:[Router X] configure terminal\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\n end [eoc],bot_reply:Untuk mengatur rute pada Router X untuk jaringan lokal dengan alamat IP 192.168.1.0 dan subnet mask 255.255.255.0, langkah pertama adalah masuk ke mode konfigurasi menggunakan perintah berikut:\\n[Router X] configure terminal\\nSelanjutnya, gunakan perintah `ip route` untuk menambahkan alamat IP tujuan, subnet mask, dan antarmuka. Akhir dari urutan perintah adalah sebagai berikut:\\nip route 192.168.1.0 255.255.255.0 g0/0/0\\nend [eoc]Anda dapat melaksanakan rangkaian perintah sebagaimana dijelaskan di atas.', 'commands:[Router C] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router C dan pendaftaran alamat IP 192.168.2.0 dan 192.168.3.0 pada perutean, perlu dinyatakan ppemanggilan konfigurasi Router C terlebih dahulu dengan perintah\\n[Router C] configure terminal\\nLalu tambahkan perintah router rip dan version 2 untuk menandakan akan digunakan protokol RIP versi 2\\nrouter rip\\nversion 2\\nSetelah itu cantumkan alamat IP asal dan alamat IP tujuan dengan menggunakan perintah network. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router C] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\nend [eoc]', 'commands:[Router K] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router K dan mendaftarkan alamat IP 192.168.2.0 dan 192.168.3.0 pada perutean, langkah awal adalah memasuki mode konfigurasi pada Router K dengan perintah:\\n[Router K] configure terminal\\nSetelah itu, gunakan perintah `router rip` dan `version 2` untuk menandakan bahwa Anda akan menggunakan protokol RIP versi 2:\\nrouter rip\\nversion 2\\nSelanjutnya, tambahkan alamat IP sumber dan tujuan dengan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Rangkaian perintah dapat dijalankan sebagai berikut:\\n[Router K] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\nend [eoc]\\nAnda dapat melaksanakan rangkaian perintah seperti di atas.', 'commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router A dan memasukkan alamat IP 192.168.2.0 dan 192.168.3.0 pada tabel routing, langkah awal adalah mengakses mode konfigurasi pada Router A menggunakan perintah berikut:\\n[Router A] configure terminal\\nAfterward, utilize the `router rip` and `version 2` commands to specify the use of RIP protocol version 2:\\nrouter rip\\nversion 2\\nSelanjutnya, gabungkan alamat IP sumber dan tujuan menggunakan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Perintah-perintah tersebut dapat dijalankan sebagai berikut:\\n[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.2.0\\nnetwork 192.168.3.0\\nend [eoc]\\nAnda dapat menerapkan urutan perintah seperti yang ditunjukkan di atas.', 'commands:[Router K] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\n end [eoc],bot_reply:Untuk mengaktifkan VLAN 11 pada Router K dengan interface g0/0, alamat IP 192.168.0.1, dan subnet mask 255.255.255.0 kita perlu menyatakan pemanggilan konfigurasi Router K terlebih dahulu dengan menggunakan perintah\\n[Router K] configure terminal\\nLalu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask dapat digunakan perintah int, encapsulation, dan ip address\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router K] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\nend [eoc]', 'commands:[Router G] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\n end [eoc],bot_reply:Untuk mengaktifkan VLAN 11 pada Router G dengan alamat IP 192.168.0.1, interface g0/0, dan subnet mask 255.255.255.0  perlu dilakukan pemanggilan konfigurasi Router G terlebih dahulu dengan menggunakan perintah\\n[Router G] configure terminal\\nSetelah itu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask dapat digunakan perintah int, encapsulation, dan ip address\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah adalah sebagai berikut:\\n[Router G] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\nend [eoc]', 'commands:[Router A] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\n end [eoc],bot_reply:Anda dapat mengaktifkan VLAN 11 pada Router A dengan alamat IP 192.168.0.1, interface g0/0, dan subnet mask 255.255.255.0 dengan langkah pertama melakukan pemanggilan konfigurasi Router A terlebih dahulu dengan menggunakan perintah\\n[Router A] configure terminal\\nLalu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask gunakan perintah int, encapsulation, dan ip address\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Perintah lengkapnya, yaitu:\\n[Router A] configure terminal\\nint g0/0.11\\nencapsulation dot1q 11\\nip address 192.168.0.1 255.255.255.0\\nend [eoc]', 'commands:[Router G] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\n end [eoc],bot_reply:Untuk mengaktifkan VLAN 22 pada Router G dengan alamat IP 192.168.1.1, interface g0/0, dan subnet mask 255.255.255.0  perlu dilakukan pemanggilan konfigurasi Router G terlebih dahulu dengan menggunakan perintah\\n[Router G] configure terminal\\nSetelah itu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask dapat digunakan perintah int, encapsulation, dan ip address\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah adalah sebagai berikut:\\n[Router G] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\nend [eoc]', 'commands:[Router O] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\n end [eoc],bot_reply:Untuk mengaktifkan VLAN 22 pada Router O dengan interface g0/0, alamat IP 192.168.2.1, dan subnet mask 255.255.255.0 kita perlu menyatakan pemanggilan konfigurasi Router O terlebih dahulu dengan menggunakan perintah\\n[Router O] configure terminal\\nLalu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask dapat digunakan perintah int, encapsulation, dan ip address\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router O] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\nend [eoc]', 'commands:[Router I] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\n end [eoc],bot_reply:Anda dapat mengaktifkan VLAN 22 pada Router I dengan alamat IP 192.168.1.1, interface g0/0, dan subnet mask 255.255.255.0 dengan langkah pertama melakukan pemanggilan konfigurasi Router I terlebih dahulu dengan menggunakan perintah\\n[Router I] configure terminal\\nLalu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask gunakan perintah int, encapsulation, dan ip address\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Perintah lengkapnya, yaitu:\\n[Router I] configure terminal\\nint g0/0.22\\nencapsulation dot1q 22\\nip address 192.168.1.1 255.255.255.0\\nend [eoc]', 'commands:[Router A] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\n end [eoc],bot_reply:Untuk mengaktifkan VLAN 33 pada Router A dengan alamat IP 192.168.2.1, interface g0/0, dan subnet mask 255.255.255.0  perlu dilakukan pemanggilan konfigurasi Router A terlebih dahulu dengan menggunakan perintah\\n[Router A] configure terminal\\nSetelah itu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask dapat digunakan perintah int, encapsulation, dan ip address\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah adalah sebagai berikut:\\n[Router A] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\nend [eoc]', 'commands:[Router I] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\n end [eoc],bot_reply:Untuk mengaktifkan VLAN 33 pada Router I dengan interface g0/0, alamat IP 192.168.2.1, dan subnet mask 255.255.255.0 kita perlu menyatakan pemanggilan konfigurasi Router I terlebih dahulu dengan menggunakan perintah\\n[Router I] configure terminal\\nLalu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask dapat digunakan perintah int, encapsulation, dan ip address\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router I] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\nend [eoc]', 'commands:[Router Z] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\n end [eoc],bot_reply:Anda dapat mengaktifkan VLAN 33 pada Router Z dengan alamat IP 192.168.2.1, interface g0/0, dan subnet mask 255.255.255.0 dengan langkah pertama melakukan pemanggilan konfigurasi Router Z terlebih dahulu dengan menggunakan perintah\\n[Router Z] configure terminal\\nLalu berdasarkan interface, tipe vlan, alamat IP, dan subnet mask gunakan perintah int, encapsulation, dan ip address\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\nRangkaian perintah diakhiri dengan end [eoc]. Perintah lengkapnya, yaitu:\\n[Router Z] configure terminal\\nint g0/0.33\\nencapsulation dot1q 33\\nip address 192.168.2.1 255.255.255.0\\nend [eoc]', 'commands:[Router K] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\n end [eoc],bot_reply:Untuk membuat rute pada Router K melalui interface g/0/0/0 menuju jaringan lokal dengan IP 192.168.0.1, subnet mask 255.255.255.0 kita perlu menyatakan pemanggilan konfigurasi Router K terlebih dahulu dengan menggunakan perintah\\n[Router K] configure terminal\\nLalu dengan perintah ip route tambahkan alamat IP tujuan, subnet mask, dan interface. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router K] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\nend [eoc]', 'commands:[Router K] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\n end [eoc],bot_reply:Untuk mengkonfigurasi rute pada Router K untuk jaringan lokal dengan alamat IP 192.168.0.1 dan subnet mask 255.255.255.0, Anda harus mulai dengan mengakses mode konfigurasi menggunakan perintah:\\n[Router K] configure terminal\\nSelanjutnya, tambahkan alamat IP tujuan, subnet mask, dan antarmuka menggunakan perintah `ip route`. Urutan perintah diakhiri dengan yang berikut:\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\nend [eoc]\\nAnda dapat menjalankan serangkaian perintah seperti yang disebutkan di atas.', 'commands:[Router C] configure terminal\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\n end [eoc],bot_reply:Untuk mengatur rute pada Router C untuk jaringan lokal dengan alamat IP 192.168.0.1 dan subnet mask 255.255.255.0, langkah pertama adalah masuk ke mode konfigurasi menggunakan perintah berikut:\\n[Router C] configure terminal\\nSelanjutnya, gunakan perintah `ip route` untuk menambahkan alamat IP tujuan, subnet mask, dan antarmuka. Akhir dari urutan perintah adalah sebagai berikut:\\nip route 192.168.0.1 255.255.255.0 g0/0/0\\nend [eoc]Anda dapat melaksanakan rangkaian perintah sebagaimana dijelaskan di atas.', 'commands:[Router X] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router X dan pendaftaran alamat IP 192.168.1.0 dan 192.168.2.0 pada perutean, perlu dinyatakan ppemanggilan konfigurasi Router X terlebih dahulu dengan perintah\\n[Router X] configure terminal\\nLalu tambahkan perintah router rip dan version 2 untuk menandakan akan digunakan protokol RIP versi 2\\nrouter rip\\nversion 2\\nSetelah itu cantumkan alamat IP asal dan alamat IP tujuan dengan menggunakan perintah network. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router X] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\nend [eoc]', 'commands:[Router Y] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router Y dan mendaftarkan alamat IP 192.168.1.0 dan 192.168.2.0 pada perutean, langkah awal adalah memasuki mode konfigurasi pada Router Y dengan perintah:\\n[Router Y] configure terminal\\nSetelah itu, gunakan perintah `router rip` dan `version 2` untuk menandakan bahwa Anda akan menggunakan protokol RIP versi 2:\\nrouter rip\\nversion 2\\nSelanjutnya, tambahkan alamat IP sumber dan tujuan dengan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Rangkaian perintah dapat dijalankan sebagai berikut:\\n[Router Y] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\nend [eoc]\\nAnda dapat melaksanakan rangkaian perintah seperti di atas.', 'commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router B dan memasukkan alamat IP 192.168.1.0 dan 192.168.2.0 pada tabel routing, langkah awal adalah mengakses mode konfigurasi pada Router B menggunakan perintah berikut:\\n[Router B] configure terminal\\nAfterward, utilize the `router rip` and `version 2` commands to specify the use of RIP protocol version 2:\\nrouter rip\\nversion 2\\nSelanjutnya, gabungkan alamat IP sumber dan tujuan menggunakan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Perintah-perintah tersebut dapat dijalankan sebagai berikut:\\n[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.2.0\\nend [eoc]\\nAnda dapat menerapkan urutan perintah seperti yang ditunjukkan di atas.', 'commands:[Router B] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\n end [eoc],bot_reply:Untuk membuat rute pada Router B melalui interface g/1/0/0 menuju jaringan lokal dengan IP 192.168.3.0, subnet mask 255.255.255.0 kita perlu menyatakan pemanggilan konfigurasi Router B terlebih dahulu dengan menggunakan perintah\\n[Router B] configure terminal\\nLalu dengan perintah ip route tambahkan alamat IP tujuan, subnet mask, dan interface. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router B] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\nend [eoc]', 'commands:[Router Y] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\n end [eoc],bot_reply:Untuk mengkonfigurasi rute pada Router Y untuk jaringan lokal dengan alamat IP 192.168.3.0 dan subnet mask 255.255.255.0, Anda harus mulai dengan mengakses mode konfigurasi menggunakan perintah:\\n[Router Y] configure terminal\\nSelanjutnya, tambahkan alamat IP tujuan, subnet mask, dan antarmuka menggunakan perintah `ip route`. Urutan perintah diakhiri dengan yang berikut:\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\nend [eoc]\\nAnda dapat menjalankan serangkaian perintah seperti yang disebutkan di atas.', 'commands:[Router Y] configure terminal\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\n end [eoc],bot_reply:Untuk mengatur rute pada Router Y untuk jaringan lokal dengan alamat IP 192.168.3.0 dan subnet mask 255.255.255.0, langkah pertama adalah masuk ke mode konfigurasi menggunakan perintah berikut:\\n[Router Y] configure terminal\\nSelanjutnya, gunakan perintah `ip route` untuk menambahkan alamat IP tujuan, subnet mask, dan antarmuka. Akhir dari urutan perintah adalah sebagai berikut:\\nip route 192.168.3.0 255.255.255.0 g1/0/0\\nend [eoc]Anda dapat melaksanakan rangkaian perintah sebagaimana dijelaskan di atas.', 'commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router B dan pendaftaran alamat IP 192.168.0.1 dan 192.168.2.0 pada perutean, perlu dinyatakan ppemanggilan konfigurasi Router B terlebih dahulu dengan perintah\\n[Router B] configure terminal\\nLalu tambahkan perintah router rip dan version 2 untuk menandakan akan digunakan protokol RIP versi 2\\nrouter rip\\nversion 2\\nSetelah itu cantumkan alamat IP asal dan alamat IP tujuan dengan menggunakan perintah network. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\nend [eoc]', 'commands:[Router Z] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router Z dan mendaftarkan alamat IP 192.168.0.1 dan 192.168.2.0 pada perutean, langkah awal adalah memasuki mode konfigurasi pada Router Z dengan perintah:\\n[Router Z] configure terminal\\nSetelah itu, gunakan perintah `router rip` dan `version 2` untuk menandakan bahwa Anda akan menggunakan protokol RIP versi 2:\\nrouter rip\\nversion 2\\nSelanjutnya, tambahkan alamat IP sumber dan tujuan dengan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Rangkaian perintah dapat dijalankan sebagai berikut:\\n[Router Z] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\nend [eoc]\\nAnda dapat melaksanakan rangkaian perintah seperti di atas.', 'commands:[Router I] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router I dan memasukkan alamat IP 192.168.0.1 dan 192.168.2.0 pada tabel routing, langkah awal adalah mengakses mode konfigurasi pada Router I menggunakan perintah berikut:\\n[Router I] configure terminal\\nAfterward, utilize the `router rip` and `version 2` commands to specify the use of RIP protocol version 2:\\nrouter rip\\nversion 2\\nSelanjutnya, gabungkan alamat IP sumber dan tujuan menggunakan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Perintah-perintah tersebut dapat dijalankan sebagai berikut:\\n[Router I] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.2.0\\nend [eoc]\\nAnda dapat menerapkan urutan perintah seperti yang ditunjukkan di atas.', 'commands:[Router Z] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\n end [eoc],bot_reply:Untuk membuat rute pada Router Z melalui interface g/1/1/0 menuju jaringan lokal dengan IP 192.168.2.0, subnet mask 255.255.255.0 kita perlu menyatakan pemanggilan konfigurasi Router Z terlebih dahulu dengan menggunakan perintah\\n[Router Z] configure terminal\\nLalu dengan perintah ip route tambahkan alamat IP tujuan, subnet mask, dan interface. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router Z] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\nend [eoc]', 'commands:[Router A] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\n end [eoc],bot_reply:Untuk mengkonfigurasi rute pada Router A untuk jaringan lokal dengan alamat IP 192.168.2.0 dan subnet mask 255.255.255.0, Anda harus mulai dengan mengakses mode konfigurasi menggunakan perintah:\\n[Router A] configure terminal\\nSelanjutnya, tambahkan alamat IP tujuan, subnet mask, dan antarmuka menggunakan perintah `ip route`. Urutan perintah diakhiri dengan yang berikut:\\nip route 192.168.1.0 255.255.255.0 g1/1/0\\nend [eoc]\\nAnda dapat menjalankan serangkaian perintah seperti yang disebutkan di atas.', 'commands:[Router B] configure terminal\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\n end [eoc],bot_reply:Untuk mengatur rute pada Router B untuk jaringan lokal dengan alamat IP 192.168.2.0 dan subnet mask 255.255.255.0, langkah pertama adalah masuk ke mode konfigurasi menggunakan perintah berikut:\\n[Router B] configure terminal\\nSelanjutnya, gunakan perintah `ip route` untuk menambahkan alamat IP tujuan, subnet mask, dan antarmuka. Akhir dari urutan perintah adalah sebagai berikut:\\nip route 192.168.2.0 255.255.255.0 g1/1/0\\nend [eoc]Anda dapat melaksanakan rangkaian perintah sebagaimana dijelaskan di atas.', 'commands:[Router K] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router K dan pendaftaran alamat IP 192.168.0.1 dan 192.168.3.0 pada perutean, perlu dinyatakan ppemanggilan konfigurasi Router K terlebih dahulu dengan perintah\\n[Router K] configure terminal\\nLalu tambahkan perintah router rip dan version 2 untuk menandakan akan digunakan protokol RIP versi 2\\nrouter rip\\nversion 2\\nSetelah itu cantumkan alamat IP asal dan alamat IP tujuan dengan menggunakan perintah network. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router K] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\nend [eoc]', 'commands:[Router G] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router G dan mendaftarkan alamat IP 192.168.0.1 dan 192.168.3.0 pada perutean, langkah awal adalah memasuki mode konfigurasi pada Router G dengan perintah:\\n[Router G] configure terminal\\nSetelah itu, gunakan perintah `router rip` dan `version 2` untuk menandakan bahwa Anda akan menggunakan protokol RIP versi 2:\\nrouter rip\\nversion 2\\nSelanjutnya, tambahkan alamat IP sumber dan tujuan dengan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Rangkaian perintah dapat dijalankan sebagai berikut:\\n[Router G] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\nend [eoc]\\nAnda dapat melaksanakan rangkaian perintah seperti di atas.', 'commands:[Router I] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router I dan memasukkan alamat IP 192.168.0.1 dan 192.168.3.0 pada tabel routing, langkah awal adalah mengakses mode konfigurasi pada Router I menggunakan perintah berikut:\\n[Router I] configure terminal\\nAfterward, utilize the `router rip` and `version 2` commands to specify the use of RIP protocol version 2:\\nrouter rip\\nversion 2\\nSelanjutnya, gabungkan alamat IP sumber dan tujuan menggunakan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Perintah-perintah tersebut dapat dijalankan sebagai berikut:\\n[Router I] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.0.1\\nnetwork 192.168.3.0\\nend [eoc]\\nAnda dapat menerapkan urutan perintah seperti yang ditunjukkan di atas.', 'commands:[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router B dan pendaftaran alamat IP 192.168.1.0 dan 192.168.3.0 pada perutean, perlu dinyatakan ppemanggilan konfigurasi Router B terlebih dahulu dengan perintah\\n[Router B] configure terminal\\nLalu tambahkan perintah router rip dan version 2 untuk menandakan akan digunakan protokol RIP versi 2\\nrouter rip\\nversion 2\\nSetelah itu cantumkan alamat IP asal dan alamat IP tujuan dengan menggunakan perintah network. Rangkaian perintah diakhiri dengan end [eoc]. Rangkaian perintah dapat dilakukan sebagai berikut:\\n[Router B] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\nend [eoc]', 'commands:[Router G] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router G dan mendaftarkan alamat IP 192.168.1.0 dan 192.168.3.0 pada perutean, langkah awal adalah memasuki mode konfigurasi pada Router G dengan perintah:\\n[Router G] configure terminal\\nSetelah itu, gunakan perintah `router rip` dan `version 2` untuk menandakan bahwa Anda akan menggunakan protokol RIP versi 2:\\nrouter rip\\nversion 2\\nSelanjutnya, tambahkan alamat IP sumber dan tujuan dengan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Rangkaian perintah dapat dijalankan sebagai berikut:\\n[Router G] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\nend [eoc]\\nAnda dapat melaksanakan rangkaian perintah seperti di atas.', 'commands:[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\n end [eoc],bot_reply:Untuk mengaktifkan protokol RIP versi 2 pada Router A dan memasukkan alamat IP 192.168.1.0 dan 192.168.3.0 pada tabel routing, langkah awal adalah mengakses mode konfigurasi pada Router A menggunakan perintah berikut:\\n[Router A] configure terminal\\nAfterward, utilize the `router rip` and `version 2` commands to specify the use of RIP protocol version 2:\\nrouter rip\\nversion 2\\nSelanjutnya, gabungkan alamat IP sumber dan tujuan menggunakan perintah `network`. Urutan perintah diakhiri dengan `end [eoc]`. Perintah-perintah tersebut dapat dijalankan sebagai berikut:\\n[Router A] configure terminal\\nrouter rip\\nversion 2\\nnetwork 192.168.1.0\\nnetwork 192.168.3.0\\nend [eoc]\\nAnda dapat menerapkan urutan perintah seperti yang ditunjukkan di atas.']\n"
          ]
        }
      ],
      "source": [
        "print(array['Prompt'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "1SUuVsoZ18rb",
        "outputId": "141e5b2a-bfbc-4a20-e371-2cc9ec862992"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a1410bdbb989>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Prompt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatas\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prompt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Prompt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prompt:'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prompt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m',commands:'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'completion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# train, test = train_test_split(data, test_size=0.01)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"float\") to str"
          ]
        }
      ],
      "source": [
        "data = pd.read_json('/content/train.json')\n",
        "array = {'Prompt':[]}\n",
        "for idx, datas in enumerate(data['prompt']):\n",
        "  array['Prompt'].append('prompt:' + data['prompt'][idx] + ',commands:' + data['completion'][idx])\n",
        "# train, test = train_test_split(data, test_size=0.01)\n",
        "dataset = pd.DataFrame.from_dict(array)\n",
        "dataset.to_csv('/content/train_data.csv', index=False)\n",
        "# test.to_csv('/content/test_pgbp_example.csv', index=False)\n",
        "print(len(array['Prompt']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "d4839891428b4809ad8b942d8b049ca7",
            "88be0bb297154b0e854467279a5c27b4",
            "f46bd9598f3e40338159172baa2cdd36",
            "2393ae60dce6441da0c9a5d698dfc46e",
            "d8d20ff6f5104b428f2f065c22e6172b",
            "9d58b3178f1643da9dca808fdf2c1d5c",
            "8cf96837e9e34362b244ba727e45fd2c",
            "4eaa71e7a9954780b3cc0b464279ff89",
            "d566535df8444e868222da339ea3b969",
            "91467d50ce8d4c0ba1f0df1387d000dc",
            "1a4c52b1d95e4a538588195d8f8aef81",
            "f8b6eeb07a3d4b85bfbfdf73a999d618",
            "42337807457141b8bd6d68ecb8954ead",
            "950129df07df4a2ebc557c4f52e46374",
            "7bb086ec6d6e47bb8c542e7c1349f344",
            "0c1ad1b39a1645c0a7398d1d38e4be44",
            "651b02eb35304665861d2d553b8187a9",
            "41f5701217164a768206f0810f34bf3c",
            "16b29a5e93804d7487c35c9ffe6f9ec9",
            "b8e3ab428ae4482d9c65d43782a42a09",
            "be590f3cd3684f5b8a406f84d3a2b422",
            "5b0d3a297f5b467192ac5387b9efcb2b",
            "a4e3b34ee6b341e7a607b098ca6614f3",
            "2796ee751d2b495a960c1761fca55c63",
            "2860d52f020e44eb8dac5c5048e37ca0",
            "b701f68b627d48a69afdaa77d77fc1e3",
            "0180be892d8f419e81855a1ecc7033e3",
            "8cdee56d95114e728a1a3c5ef5836c38",
            "c61cd2b63a7f4d0f95282dc449f21db9",
            "b279ecb918d444ab93f71458ca1c8309",
            "b602080fb65c4b8e86c16a28e43c5285",
            "8460893d5e1a469ea29f1bf274142807",
            "dff750f83f9d436aa49c5270b6355c0f"
          ]
        },
        "id": "eo6uZOqU2DxU",
        "outputId": "b4ddc355-fb7c-4188-a619-cb59a6e0082d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default-b316a5dab2223d91\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-b316a5dab2223d91/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4839891428b4809ad8b942d8b049ca7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8b6eeb07a3d4b85bfbfdf73a999d618",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-b316a5dab2223d91/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/csv/csv.py:170: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
            "  csv_file_reader = pd.read_csv(file, iterator=True, dtype=dtype, **self.config.read_csv_kwargs)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4e3b34ee6b341e7a607b098ca6614f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('csv', data_files={'train': '/content/train_data.csv'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "8d615e45ccf841e0b9c0ed09875fcced",
            "6091e99e7d8848a385313a98d5c753e9",
            "01ac8010aabc4ed4843fa8241da464dc",
            "97fec64e4261403b9dc9dd65c6db5dbd",
            "292181dada26415a9850729403268400",
            "f58dcba811744e8893b3c15f34cd81b0",
            "28ae3fd672eb4422b9ae57c520aeb706",
            "58bccdd26b5844edaca8d09215ea64b9",
            "406316501eba4b80b2bd53d1d4c484e3",
            "71fc6011610941b7a26e743ce21881da",
            "5d5f1e3d45d24c5d97195e8e22fd4593"
          ]
        },
        "id": "S0QrNw_r2OMr",
        "outputId": "882ef5c7-5d66-4588-9ef4-4bbb0b196761"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.fingerprint:Parameter 'function'=<function tokenize_function at 0x7f3588dcdd80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d615e45ccf841e0b9c0ed09875fcced",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"Prompt\"], padding=True, truncation=True, max_length= 1024)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"Prompt\"])\n",
        "tokenized_datasets.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vltw5L0i2b3d"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "full_train_dataset = tokenized_datasets[\"train\"]\n",
        "train_dataloader = DataLoader(full_train_dataset, shuffle=False, batch_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MUl4jkSKi6W"
      },
      "source": [
        "Add adapters to Embedding/MLP/Attention/LMHead layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4Jfpe2h2mHI",
        "outputId": "13909443-82fe-4bed-991d-1be61eb1c938"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adding adapter to transformer.wte\n",
            "Initializing transformer.wte\n",
            "Adding adapter to transformer.h.0.attn.k_proj\n",
            "Initializing transformer.h.0.attn.k_proj\n",
            "Adding adapter to transformer.h.0.attn.v_proj\n",
            "Initializing transformer.h.0.attn.v_proj\n",
            "Adding adapter to transformer.h.0.attn.q_proj\n",
            "Initializing transformer.h.0.attn.q_proj\n",
            "Adding adapter to transformer.h.0.attn.out_proj\n",
            "Initializing transformer.h.0.attn.out_proj\n",
            "Adding adapter to transformer.h.0.mlp.fc_in\n",
            "Initializing transformer.h.0.mlp.fc_in\n",
            "Adding adapter to transformer.h.0.mlp.fc_out\n",
            "Initializing transformer.h.0.mlp.fc_out\n",
            "Adding adapter to transformer.h.1.attn.k_proj\n",
            "Initializing transformer.h.1.attn.k_proj\n",
            "Adding adapter to transformer.h.1.attn.v_proj\n",
            "Initializing transformer.h.1.attn.v_proj\n",
            "Adding adapter to transformer.h.1.attn.q_proj\n",
            "Initializing transformer.h.1.attn.q_proj\n",
            "Adding adapter to transformer.h.1.attn.out_proj\n",
            "Initializing transformer.h.1.attn.out_proj\n",
            "Adding adapter to transformer.h.1.mlp.fc_in\n",
            "Initializing transformer.h.1.mlp.fc_in\n",
            "Adding adapter to transformer.h.1.mlp.fc_out\n",
            "Initializing transformer.h.1.mlp.fc_out\n",
            "Adding adapter to transformer.h.2.attn.k_proj\n",
            "Initializing transformer.h.2.attn.k_proj\n",
            "Adding adapter to transformer.h.2.attn.v_proj\n",
            "Initializing transformer.h.2.attn.v_proj\n",
            "Adding adapter to transformer.h.2.attn.q_proj\n",
            "Initializing transformer.h.2.attn.q_proj\n",
            "Adding adapter to transformer.h.2.attn.out_proj\n",
            "Initializing transformer.h.2.attn.out_proj\n",
            "Adding adapter to transformer.h.2.mlp.fc_in\n",
            "Initializing transformer.h.2.mlp.fc_in\n",
            "Adding adapter to transformer.h.2.mlp.fc_out\n",
            "Initializing transformer.h.2.mlp.fc_out\n",
            "Adding adapter to transformer.h.3.attn.k_proj\n",
            "Initializing transformer.h.3.attn.k_proj\n",
            "Adding adapter to transformer.h.3.attn.v_proj\n",
            "Initializing transformer.h.3.attn.v_proj\n",
            "Adding adapter to transformer.h.3.attn.q_proj\n",
            "Initializing transformer.h.3.attn.q_proj\n",
            "Adding adapter to transformer.h.3.attn.out_proj\n",
            "Initializing transformer.h.3.attn.out_proj\n",
            "Adding adapter to transformer.h.3.mlp.fc_in\n",
            "Initializing transformer.h.3.mlp.fc_in\n",
            "Adding adapter to transformer.h.3.mlp.fc_out\n",
            "Initializing transformer.h.3.mlp.fc_out\n",
            "Adding adapter to transformer.h.4.attn.k_proj\n",
            "Initializing transformer.h.4.attn.k_proj\n",
            "Adding adapter to transformer.h.4.attn.v_proj\n",
            "Initializing transformer.h.4.attn.v_proj\n",
            "Adding adapter to transformer.h.4.attn.q_proj\n",
            "Initializing transformer.h.4.attn.q_proj\n",
            "Adding adapter to transformer.h.4.attn.out_proj\n",
            "Initializing transformer.h.4.attn.out_proj\n",
            "Adding adapter to transformer.h.4.mlp.fc_in\n",
            "Initializing transformer.h.4.mlp.fc_in\n",
            "Adding adapter to transformer.h.4.mlp.fc_out\n",
            "Initializing transformer.h.4.mlp.fc_out\n",
            "Adding adapter to transformer.h.5.attn.k_proj\n",
            "Initializing transformer.h.5.attn.k_proj\n",
            "Adding adapter to transformer.h.5.attn.v_proj\n",
            "Initializing transformer.h.5.attn.v_proj\n",
            "Adding adapter to transformer.h.5.attn.q_proj\n",
            "Initializing transformer.h.5.attn.q_proj\n",
            "Adding adapter to transformer.h.5.attn.out_proj\n",
            "Initializing transformer.h.5.attn.out_proj\n",
            "Adding adapter to transformer.h.5.mlp.fc_in\n",
            "Initializing transformer.h.5.mlp.fc_in\n",
            "Adding adapter to transformer.h.5.mlp.fc_out\n",
            "Initializing transformer.h.5.mlp.fc_out\n",
            "Adding adapter to transformer.h.6.attn.k_proj\n",
            "Initializing transformer.h.6.attn.k_proj\n",
            "Adding adapter to transformer.h.6.attn.v_proj\n",
            "Initializing transformer.h.6.attn.v_proj\n",
            "Adding adapter to transformer.h.6.attn.q_proj\n",
            "Initializing transformer.h.6.attn.q_proj\n",
            "Adding adapter to transformer.h.6.attn.out_proj\n",
            "Initializing transformer.h.6.attn.out_proj\n",
            "Adding adapter to transformer.h.6.mlp.fc_in\n",
            "Initializing transformer.h.6.mlp.fc_in\n",
            "Adding adapter to transformer.h.6.mlp.fc_out\n",
            "Initializing transformer.h.6.mlp.fc_out\n",
            "Adding adapter to transformer.h.7.attn.k_proj\n",
            "Initializing transformer.h.7.attn.k_proj\n",
            "Adding adapter to transformer.h.7.attn.v_proj\n",
            "Initializing transformer.h.7.attn.v_proj\n",
            "Adding adapter to transformer.h.7.attn.q_proj\n",
            "Initializing transformer.h.7.attn.q_proj\n",
            "Adding adapter to transformer.h.7.attn.out_proj\n",
            "Initializing transformer.h.7.attn.out_proj\n",
            "Adding adapter to transformer.h.7.mlp.fc_in\n",
            "Initializing transformer.h.7.mlp.fc_in\n",
            "Adding adapter to transformer.h.7.mlp.fc_out\n",
            "Initializing transformer.h.7.mlp.fc_out\n",
            "Adding adapter to transformer.h.8.attn.k_proj\n",
            "Initializing transformer.h.8.attn.k_proj\n",
            "Adding adapter to transformer.h.8.attn.v_proj\n",
            "Initializing transformer.h.8.attn.v_proj\n",
            "Adding adapter to transformer.h.8.attn.q_proj\n",
            "Initializing transformer.h.8.attn.q_proj\n",
            "Adding adapter to transformer.h.8.attn.out_proj\n",
            "Initializing transformer.h.8.attn.out_proj\n",
            "Adding adapter to transformer.h.8.mlp.fc_in\n",
            "Initializing transformer.h.8.mlp.fc_in\n",
            "Adding adapter to transformer.h.8.mlp.fc_out\n",
            "Initializing transformer.h.8.mlp.fc_out\n",
            "Adding adapter to transformer.h.9.attn.k_proj\n",
            "Initializing transformer.h.9.attn.k_proj\n",
            "Adding adapter to transformer.h.9.attn.v_proj\n",
            "Initializing transformer.h.9.attn.v_proj\n",
            "Adding adapter to transformer.h.9.attn.q_proj\n",
            "Initializing transformer.h.9.attn.q_proj\n",
            "Adding adapter to transformer.h.9.attn.out_proj\n",
            "Initializing transformer.h.9.attn.out_proj\n",
            "Adding adapter to transformer.h.9.mlp.fc_in\n",
            "Initializing transformer.h.9.mlp.fc_in\n",
            "Adding adapter to transformer.h.9.mlp.fc_out\n",
            "Initializing transformer.h.9.mlp.fc_out\n",
            "Adding adapter to transformer.h.10.attn.k_proj\n",
            "Initializing transformer.h.10.attn.k_proj\n",
            "Adding adapter to transformer.h.10.attn.v_proj\n",
            "Initializing transformer.h.10.attn.v_proj\n",
            "Adding adapter to transformer.h.10.attn.q_proj\n",
            "Initializing transformer.h.10.attn.q_proj\n",
            "Adding adapter to transformer.h.10.attn.out_proj\n",
            "Initializing transformer.h.10.attn.out_proj\n",
            "Adding adapter to transformer.h.10.mlp.fc_in\n",
            "Initializing transformer.h.10.mlp.fc_in\n",
            "Adding adapter to transformer.h.10.mlp.fc_out\n",
            "Initializing transformer.h.10.mlp.fc_out\n",
            "Adding adapter to transformer.h.11.attn.k_proj\n",
            "Initializing transformer.h.11.attn.k_proj\n",
            "Adding adapter to transformer.h.11.attn.v_proj\n",
            "Initializing transformer.h.11.attn.v_proj\n",
            "Adding adapter to transformer.h.11.attn.q_proj\n",
            "Initializing transformer.h.11.attn.q_proj\n",
            "Adding adapter to transformer.h.11.attn.out_proj\n",
            "Initializing transformer.h.11.attn.out_proj\n",
            "Adding adapter to transformer.h.11.mlp.fc_in\n",
            "Initializing transformer.h.11.mlp.fc_in\n",
            "Adding adapter to transformer.h.11.mlp.fc_out\n",
            "Initializing transformer.h.11.mlp.fc_out\n",
            "Adding adapter to transformer.h.12.attn.k_proj\n",
            "Initializing transformer.h.12.attn.k_proj\n",
            "Adding adapter to transformer.h.12.attn.v_proj\n",
            "Initializing transformer.h.12.attn.v_proj\n",
            "Adding adapter to transformer.h.12.attn.q_proj\n",
            "Initializing transformer.h.12.attn.q_proj\n",
            "Adding adapter to transformer.h.12.attn.out_proj\n",
            "Initializing transformer.h.12.attn.out_proj\n",
            "Adding adapter to transformer.h.12.mlp.fc_in\n",
            "Initializing transformer.h.12.mlp.fc_in\n",
            "Adding adapter to transformer.h.12.mlp.fc_out\n",
            "Initializing transformer.h.12.mlp.fc_out\n",
            "Adding adapter to transformer.h.13.attn.k_proj\n",
            "Initializing transformer.h.13.attn.k_proj\n",
            "Adding adapter to transformer.h.13.attn.v_proj\n",
            "Initializing transformer.h.13.attn.v_proj\n",
            "Adding adapter to transformer.h.13.attn.q_proj\n",
            "Initializing transformer.h.13.attn.q_proj\n",
            "Adding adapter to transformer.h.13.attn.out_proj\n",
            "Initializing transformer.h.13.attn.out_proj\n",
            "Adding adapter to transformer.h.13.mlp.fc_in\n",
            "Initializing transformer.h.13.mlp.fc_in\n",
            "Adding adapter to transformer.h.13.mlp.fc_out\n",
            "Initializing transformer.h.13.mlp.fc_out\n",
            "Adding adapter to transformer.h.14.attn.k_proj\n",
            "Initializing transformer.h.14.attn.k_proj\n",
            "Adding adapter to transformer.h.14.attn.v_proj\n",
            "Initializing transformer.h.14.attn.v_proj\n",
            "Adding adapter to transformer.h.14.attn.q_proj\n",
            "Initializing transformer.h.14.attn.q_proj\n",
            "Adding adapter to transformer.h.14.attn.out_proj\n",
            "Initializing transformer.h.14.attn.out_proj\n",
            "Adding adapter to transformer.h.14.mlp.fc_in\n",
            "Initializing transformer.h.14.mlp.fc_in\n",
            "Adding adapter to transformer.h.14.mlp.fc_out\n",
            "Initializing transformer.h.14.mlp.fc_out\n",
            "Adding adapter to transformer.h.15.attn.k_proj\n",
            "Initializing transformer.h.15.attn.k_proj\n",
            "Adding adapter to transformer.h.15.attn.v_proj\n",
            "Initializing transformer.h.15.attn.v_proj\n",
            "Adding adapter to transformer.h.15.attn.q_proj\n",
            "Initializing transformer.h.15.attn.q_proj\n",
            "Adding adapter to transformer.h.15.attn.out_proj\n",
            "Initializing transformer.h.15.attn.out_proj\n",
            "Adding adapter to transformer.h.15.mlp.fc_in\n",
            "Initializing transformer.h.15.mlp.fc_in\n",
            "Adding adapter to transformer.h.15.mlp.fc_out\n",
            "Initializing transformer.h.15.mlp.fc_out\n",
            "Adding adapter to transformer.h.16.attn.k_proj\n",
            "Initializing transformer.h.16.attn.k_proj\n",
            "Adding adapter to transformer.h.16.attn.v_proj\n",
            "Initializing transformer.h.16.attn.v_proj\n",
            "Adding adapter to transformer.h.16.attn.q_proj\n",
            "Initializing transformer.h.16.attn.q_proj\n",
            "Adding adapter to transformer.h.16.attn.out_proj\n",
            "Initializing transformer.h.16.attn.out_proj\n",
            "Adding adapter to transformer.h.16.mlp.fc_in\n",
            "Initializing transformer.h.16.mlp.fc_in\n",
            "Adding adapter to transformer.h.16.mlp.fc_out\n",
            "Initializing transformer.h.16.mlp.fc_out\n",
            "Adding adapter to transformer.h.17.attn.k_proj\n",
            "Initializing transformer.h.17.attn.k_proj\n",
            "Adding adapter to transformer.h.17.attn.v_proj\n",
            "Initializing transformer.h.17.attn.v_proj\n",
            "Adding adapter to transformer.h.17.attn.q_proj\n",
            "Initializing transformer.h.17.attn.q_proj\n",
            "Adding adapter to transformer.h.17.attn.out_proj\n",
            "Initializing transformer.h.17.attn.out_proj\n",
            "Adding adapter to transformer.h.17.mlp.fc_in\n",
            "Initializing transformer.h.17.mlp.fc_in\n",
            "Adding adapter to transformer.h.17.mlp.fc_out\n",
            "Initializing transformer.h.17.mlp.fc_out\n",
            "Adding adapter to transformer.h.18.attn.k_proj\n",
            "Initializing transformer.h.18.attn.k_proj\n",
            "Adding adapter to transformer.h.18.attn.v_proj\n",
            "Initializing transformer.h.18.attn.v_proj\n",
            "Adding adapter to transformer.h.18.attn.q_proj\n",
            "Initializing transformer.h.18.attn.q_proj\n",
            "Adding adapter to transformer.h.18.attn.out_proj\n",
            "Initializing transformer.h.18.attn.out_proj\n",
            "Adding adapter to transformer.h.18.mlp.fc_in\n",
            "Initializing transformer.h.18.mlp.fc_in\n",
            "Adding adapter to transformer.h.18.mlp.fc_out\n",
            "Initializing transformer.h.18.mlp.fc_out\n",
            "Adding adapter to transformer.h.19.attn.k_proj\n",
            "Initializing transformer.h.19.attn.k_proj\n",
            "Adding adapter to transformer.h.19.attn.v_proj\n",
            "Initializing transformer.h.19.attn.v_proj\n",
            "Adding adapter to transformer.h.19.attn.q_proj\n",
            "Initializing transformer.h.19.attn.q_proj\n",
            "Adding adapter to transformer.h.19.attn.out_proj\n",
            "Initializing transformer.h.19.attn.out_proj\n",
            "Adding adapter to transformer.h.19.mlp.fc_in\n",
            "Initializing transformer.h.19.mlp.fc_in\n",
            "Adding adapter to transformer.h.19.mlp.fc_out\n",
            "Initializing transformer.h.19.mlp.fc_out\n",
            "Adding adapter to transformer.h.20.attn.k_proj\n",
            "Initializing transformer.h.20.attn.k_proj\n",
            "Adding adapter to transformer.h.20.attn.v_proj\n",
            "Initializing transformer.h.20.attn.v_proj\n",
            "Adding adapter to transformer.h.20.attn.q_proj\n",
            "Initializing transformer.h.20.attn.q_proj\n",
            "Adding adapter to transformer.h.20.attn.out_proj\n",
            "Initializing transformer.h.20.attn.out_proj\n",
            "Adding adapter to transformer.h.20.mlp.fc_in\n",
            "Initializing transformer.h.20.mlp.fc_in\n",
            "Adding adapter to transformer.h.20.mlp.fc_out\n",
            "Initializing transformer.h.20.mlp.fc_out\n",
            "Adding adapter to transformer.h.21.attn.k_proj\n",
            "Initializing transformer.h.21.attn.k_proj\n",
            "Adding adapter to transformer.h.21.attn.v_proj\n",
            "Initializing transformer.h.21.attn.v_proj\n",
            "Adding adapter to transformer.h.21.attn.q_proj\n",
            "Initializing transformer.h.21.attn.q_proj\n",
            "Adding adapter to transformer.h.21.attn.out_proj\n",
            "Initializing transformer.h.21.attn.out_proj\n",
            "Adding adapter to transformer.h.21.mlp.fc_in\n",
            "Initializing transformer.h.21.mlp.fc_in\n",
            "Adding adapter to transformer.h.21.mlp.fc_out\n",
            "Initializing transformer.h.21.mlp.fc_out\n",
            "Adding adapter to transformer.h.22.attn.k_proj\n",
            "Initializing transformer.h.22.attn.k_proj\n",
            "Adding adapter to transformer.h.22.attn.v_proj\n",
            "Initializing transformer.h.22.attn.v_proj\n",
            "Adding adapter to transformer.h.22.attn.q_proj\n",
            "Initializing transformer.h.22.attn.q_proj\n",
            "Adding adapter to transformer.h.22.attn.out_proj\n",
            "Initializing transformer.h.22.attn.out_proj\n",
            "Adding adapter to transformer.h.22.mlp.fc_in\n",
            "Initializing transformer.h.22.mlp.fc_in\n",
            "Adding adapter to transformer.h.22.mlp.fc_out\n",
            "Initializing transformer.h.22.mlp.fc_out\n",
            "Adding adapter to transformer.h.23.attn.k_proj\n",
            "Initializing transformer.h.23.attn.k_proj\n",
            "Adding adapter to transformer.h.23.attn.v_proj\n",
            "Initializing transformer.h.23.attn.v_proj\n",
            "Adding adapter to transformer.h.23.attn.q_proj\n",
            "Initializing transformer.h.23.attn.q_proj\n",
            "Adding adapter to transformer.h.23.attn.out_proj\n",
            "Initializing transformer.h.23.attn.out_proj\n",
            "Adding adapter to transformer.h.23.mlp.fc_in\n",
            "Initializing transformer.h.23.mlp.fc_in\n",
            "Adding adapter to transformer.h.23.mlp.fc_out\n",
            "Initializing transformer.h.23.mlp.fc_out\n",
            "Adding adapter to transformer.h.24.attn.k_proj\n",
            "Initializing transformer.h.24.attn.k_proj\n",
            "Adding adapter to transformer.h.24.attn.v_proj\n",
            "Initializing transformer.h.24.attn.v_proj\n",
            "Adding adapter to transformer.h.24.attn.q_proj\n",
            "Initializing transformer.h.24.attn.q_proj\n",
            "Adding adapter to transformer.h.24.attn.out_proj\n",
            "Initializing transformer.h.24.attn.out_proj\n",
            "Adding adapter to transformer.h.24.mlp.fc_in\n",
            "Initializing transformer.h.24.mlp.fc_in\n",
            "Adding adapter to transformer.h.24.mlp.fc_out\n",
            "Initializing transformer.h.24.mlp.fc_out\n",
            "Adding adapter to transformer.h.25.attn.k_proj\n",
            "Initializing transformer.h.25.attn.k_proj\n",
            "Adding adapter to transformer.h.25.attn.v_proj\n",
            "Initializing transformer.h.25.attn.v_proj\n",
            "Adding adapter to transformer.h.25.attn.q_proj\n",
            "Initializing transformer.h.25.attn.q_proj\n",
            "Adding adapter to transformer.h.25.attn.out_proj\n",
            "Initializing transformer.h.25.attn.out_proj\n",
            "Adding adapter to transformer.h.25.mlp.fc_in\n",
            "Initializing transformer.h.25.mlp.fc_in\n",
            "Adding adapter to transformer.h.25.mlp.fc_out\n",
            "Initializing transformer.h.25.mlp.fc_out\n",
            "Adding adapter to transformer.h.26.attn.k_proj\n",
            "Initializing transformer.h.26.attn.k_proj\n",
            "Adding adapter to transformer.h.26.attn.v_proj\n",
            "Initializing transformer.h.26.attn.v_proj\n",
            "Adding adapter to transformer.h.26.attn.q_proj\n",
            "Initializing transformer.h.26.attn.q_proj\n",
            "Adding adapter to transformer.h.26.attn.out_proj\n",
            "Initializing transformer.h.26.attn.out_proj\n",
            "Adding adapter to transformer.h.26.mlp.fc_in\n",
            "Initializing transformer.h.26.mlp.fc_in\n",
            "Adding adapter to transformer.h.26.mlp.fc_out\n",
            "Initializing transformer.h.26.mlp.fc_out\n",
            "Adding adapter to transformer.h.27.attn.k_proj\n",
            "Initializing transformer.h.27.attn.k_proj\n",
            "Adding adapter to transformer.h.27.attn.v_proj\n",
            "Initializing transformer.h.27.attn.v_proj\n",
            "Adding adapter to transformer.h.27.attn.q_proj\n",
            "Initializing transformer.h.27.attn.q_proj\n",
            "Adding adapter to transformer.h.27.attn.out_proj\n",
            "Initializing transformer.h.27.attn.out_proj\n",
            "Adding adapter to transformer.h.27.mlp.fc_in\n",
            "Initializing transformer.h.27.mlp.fc_in\n",
            "Adding adapter to transformer.h.27.mlp.fc_out\n",
            "Initializing transformer.h.27.mlp.fc_out\n",
            "Adding adapter to lm_head\n",
            "Initializing lm_head\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPTJForCausalLM(\n",
              "  (transformer): GPTJModel(\n",
              "    (wte): FrozenBNBEmbedding(50400, 4096)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-27): 28 x GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
              "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
              "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
              "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
              "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): FrozenBNBLinear(4096, 50400)\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def add_adapters(model, adapter_dim=4, p = 0.1):\n",
        "    assert adapter_dim > 0\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "      if isinstance(module, FrozenBNBLinear):\n",
        "          if \"attn\" in name or \"mlp\" in name or \"head\" in name:\n",
        "              print(\"Adding adapter to\", name)\n",
        "              module.adapter = nn.Sequential(\n",
        "                nn.Linear(module.in_features, adapter_dim, bias=False),\n",
        "                nn.Dropout(p=p),\n",
        "                nn.Linear(adapter_dim, module.out_features, bias=False),\n",
        "            )\n",
        "              print(\"Initializing\", name)\n",
        "              nn.init.zeros_(module.adapter[2].weight)\n",
        "\n",
        "          else:\n",
        "              print(\"Not adding adapter to\", name)\n",
        "      elif isinstance(module, FrozenBNBEmbedding):\n",
        "          print(\"Adding adapter to\", name)\n",
        "          module.adapter = nn.Sequential(\n",
        "                nn.Embedding(module.num_embeddings, adapter_dim),\n",
        "                nn.Dropout(p=p),\n",
        "                nn.Linear(adapter_dim, module.embedding_dim, bias=False),\n",
        "            )\n",
        "          print(\"Initializing\", name)\n",
        "          nn.init.zeros_(module.adapter[2].weight)\n",
        "\n",
        "add_adapters(gpt)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "gpt.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOnrE2C330UL"
      },
      "outputs": [],
      "source": [
        "from bitsandbytes.optim import Adam8bit\n",
        "\n",
        "gpt.gradient_checkpointing_enable()\n",
        "optimizer = Adam8bit(gpt.parameters(), lr=1e-5, weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvD2RYYK32qH",
        "outputId": "685292ed-d2ee-4c3a-873c-5d6d1727b63a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3400\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 100\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "print(num_training_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM_29EAi37rr"
      },
      "outputs": [],
      "source": [
        "lr_scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "    optimizer, int(num_training_steps*0.1), num_training_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "87b1a5b20d1e4031bcc7d6eef518763e",
            "3cd5de7a1aa54a2193b9e26a6943f73d",
            "9038fa22d6da4cedba5d72f6f32a7e55",
            "a05f242e3ffd40379666b80991e7071f",
            "d9f28d0315b0488aa6759ab6670b3d19",
            "d30850ca66b04f60978dc8df00627918",
            "fa1ef3a7df0d4307936ea8a8a71f0e80",
            "fe174f78cca24f28aa0b8e2cea4e1917",
            "096f5e97c600483d82e74e08167dd29f",
            "b46755c5795542868b85416edea370a7",
            "da7ef222d7e9470d8102b55efe5b980c"
          ]
        },
        "id": "JMJltmES4FdM",
        "outputId": "d0ccdb3f-1fe8-4700-f52f-f46f2513a1b4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87b1a5b20d1e4031bcc7d6eef518763e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3400 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.3327, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.3264, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.2384, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.0861, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.0278, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.2995, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.2664, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.2292, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.3323, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.2224, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.1485, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.2143, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.1511, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8735, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.9767, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8635, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8359, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.0058, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8569, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.9599, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.9463, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.0928, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8376, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8681, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8495, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8756, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8676, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8446, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.7946, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.0105, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.0155, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.1381, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.0580, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.9357, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: train_loss = 3.894, train_acc = 0.896\n",
            "tensor(4.3100, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.3024, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.2131, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.0547, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.9948, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.2683, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.2318, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.1952, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.2981, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.1865, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.1114, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.1774, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.1125, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8348, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.9360, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8193, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.7913, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.9604, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8065, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.9133, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8965, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.0433, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.7811, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8094, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.7891, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8127, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8040, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.7767, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.7328, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.9998, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.0041, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.1221, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.0445, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.9266, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: train_loss = 3.855, train_acc = 0.896\n",
            "tensor(4.2321, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.2231, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.1308, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.9593, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8984, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.1760, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.1326, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.0992, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.1931, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.0736, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.0034, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.0680, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.0002, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.7279, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8246, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.6971, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.6708, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8391, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.6743, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.7905, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.7673, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.9142, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.6331, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.6571, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.6318, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.6509, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.6403, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.6051, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.5736, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.9725, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.9763, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.0830, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.0115, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.9051, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: train_loss = 3.748, train_acc = 0.898\n",
            "tensor(4.0336, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.0158, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.9188, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.7239, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.6584, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.9435, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8909, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8623, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.9410, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.7997, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.7408, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.8083, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.7324, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.4751, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.5581, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.4093, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.3868, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.5520, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.3668, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.5086, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.4658, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.6183, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.2991, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.3131, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.2786, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.2862, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.2745, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.2192, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.2231, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.9156, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.9178, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.9998, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.9435, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.8612, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: train_loss = 3.498, train_acc = 0.901\n",
            "tensor(3.5857, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.5638, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.4411, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.2012, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.1355, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.4267, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.3592, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.3518, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.3620, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.2323, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.1773, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.2389, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.1631, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.9350, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.0070, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.8164, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.8052, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.9600, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.7253, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.9144, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.8457, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.9853, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.6275, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.6239, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.5674, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.5572, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.5349, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.4677, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.5180, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.8054, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.8060, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.8408, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.8157, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.7844, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: train_loss = 2.976, train_acc = 0.908\n",
            "tensor(2.6685, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.6605, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.5306, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.2739, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.2279, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.4915, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.3998, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.4148, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.3320, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.2076, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.1803, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.2238, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.1680, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.1130, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.1175, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.9383, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.9990, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.0762, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8874, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.0774, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.0377, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.0760, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.9222, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.9252, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8455, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8420, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8343, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8170, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.9126, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.6453, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.6456, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.6199, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.6410, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.6662, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: train_loss = 2.218, train_acc = 0.915\n",
            "tensor(1.7642, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7544, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7493, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7734, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8379, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7733, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8249, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8225, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7059, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7432, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7752, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7594, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7969, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.9230, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8653, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7681, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8868, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8676, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7701, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.9215, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.9159, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.9135, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8377, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8304, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7555, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7443, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7355, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7217, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8231, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.5408, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.5467, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.5251, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.5384, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.5504, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: train_loss = 1.914, train_acc = 0.926\n",
            "tensor(1.7779, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7714, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7481, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7083, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7455, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7515, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7680, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7795, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6603, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6736, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7052, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7085, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7401, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8487, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7733, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6753, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8152, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7852, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6912, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8503, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8561, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8419, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7837, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7661, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6876, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6698, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6618, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6495, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7421, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.4167, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.4235, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.3969, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.4116, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.4089, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: train_loss = 1.844, train_acc = 0.937\n",
            "tensor(1.6451, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6489, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6497, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6515, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6986, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6722, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7114, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7194, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5960, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6215, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6514, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6540, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6962, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7883, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7044, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6091, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7397, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7117, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6191, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7760, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7820, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7773, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7118, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7002, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6276, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6135, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6086, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5984, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6785, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.2757, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.2799, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.2574, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.2638, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.2521, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: train_loss = 1.764, train_acc = 0.947\n",
            "tensor(1.5756, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5806, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5979, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6188, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6660, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6200, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6705, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6756, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5520, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5786, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6103, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6100, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6521, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7259, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6355, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5632, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6727, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6415, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5688, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6980, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7081, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7174, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6536, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6395, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5764, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5652, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5609, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5514, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6209, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.1281, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.1283, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.1160, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.1197, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.1018, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: train_loss = 1.697, train_acc = 0.956\n",
            "tensor(1.5406, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5426, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5608, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5835, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6307, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5817, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6403, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6445, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5201, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5435, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5732, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5758, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6171, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6685, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5881, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5302, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6173, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5887, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5337, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6338, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6462, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6711, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6086, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5943, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5433, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5332, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5300, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5223, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5806, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.9938, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.9925, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.9881, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.9894, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.9735, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: train_loss = 1.644, train_acc = 0.963\n",
            "tensor(1.5169, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5156, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5326, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5575, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6016, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5532, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6182, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6203, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5004, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5194, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5459, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5514, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5955, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6253, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5544, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5110, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5766, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5557, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5130, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5873, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6006, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6388, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5775, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5651, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5228, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5136, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5127, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5051, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5528, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8878, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8883, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8815, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8813, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8654, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: train_loss = 1.604, train_acc = 0.971\n",
            "tensor(1.5033, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5008, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5168, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5405, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5819, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5330, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6028, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6019, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4880, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5037, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5289, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5351, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5741, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5911, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5275, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5008, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5476, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5284, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4997, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5521, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5657, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6175, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5548, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5460, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5087, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5021, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5019, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4944, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5332, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8028, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8112, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7948, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7957, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7784, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: train_loss = 1.575, train_acc = 0.977\n",
            "tensor(1.4910, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4879, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5024, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5250, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5637, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5146, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5874, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5867, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4786, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4918, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5151, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5254, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5603, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5656, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5096, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4907, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5240, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5103, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4900, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5260, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5389, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5988, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5358, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5294, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4979, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4935, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4928, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4869, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5190, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7421, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7565, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7310, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7314, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7130, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: train_loss = 1.553, train_acc = 0.981\n",
            "tensor(1.4819, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4789, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4919, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5117, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5456, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5008, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5735, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5718, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4718, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4818, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5046, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5157, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5476, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5461, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4979, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4841, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5076, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4982, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4822, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5095, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5211, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5846, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5246, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5186, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4904, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4869, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4873, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4808, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5082, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6966, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.7136, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6852, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6851, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6718, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: train_loss = 1.537, train_acc = 0.983\n",
            "tensor(1.4762, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4729, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4839, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5009, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5319, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4915, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5615, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5584, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4669, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4764, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4961, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5081, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5384, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5326, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4911, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4781, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4971, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4915, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4770, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4996, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5089, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5722, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5131, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5104, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4842, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4827, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4821, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4763, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4993, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6654, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6815, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6540, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6557, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6436, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: train_loss = 1.525, train_acc = 0.984\n",
            "tensor(1.4729, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4684, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4790, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4912, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5209, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4845, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5470, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5458, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4636, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4711, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4888, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5016, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5290, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5211, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4851, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4746, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4899, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4853, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4732, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4931, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5016, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5608, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5049, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5030, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4805, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4788, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4782, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4729, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4931, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6445, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6568, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6328, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6344, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6242, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: train_loss = 1.516, train_acc = 0.985\n",
            "tensor(1.4696, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4657, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4738, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4847, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5124, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4783, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5382, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5350, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4606, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4676, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4840, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4964, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5223, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5150, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4811, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4724, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4863, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4807, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4704, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4881, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4947, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5497, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4990, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4983, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4766, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4762, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4751, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4690, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4889, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6312, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6405, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6205, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6216, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6122, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: train_loss = 1.510, train_acc = 0.985\n",
            "tensor(1.4679, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4623, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4719, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4789, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5049, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4748, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5275, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5252, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4585, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4640, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4787, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4901, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5144, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5076, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4785, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4705, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4817, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4768, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4677, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4842, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4904, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5416, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4946, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4935, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4730, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4722, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4720, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4672, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4848, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6192, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6258, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6071, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6108, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5996, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: train_loss = 1.504, train_acc = 0.986\n",
            "tensor(1.4644, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4592, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4672, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4748, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4995, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4708, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5215, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5183, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4564, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4624, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4751, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4871, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5088, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5041, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4737, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4673, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4793, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4732, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4654, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4808, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4858, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5318, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4902, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4898, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4708, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4714, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4699, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4646, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4813, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6058, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6125, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5968, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5987, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5934, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: train_loss = 1.499, train_acc = 0.987\n",
            "tensor(1.4623, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4579, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4656, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4713, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4938, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4685, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5132, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5106, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4547, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4601, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4714, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4825, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5047, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4983, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4728, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4661, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4751, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4706, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4626, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4778, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4836, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5245, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4851, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4860, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4680, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4686, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4680, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4626, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4788, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5911, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.6000, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5821, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5861, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5775, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: train_loss = 1.494, train_acc = 0.987\n",
            "tensor(1.4592, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4564, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4629, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4694, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4896, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4655, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5089, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5056, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4538, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4584, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4692, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4793, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5025, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4944, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4697, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4631, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4731, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4693, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4612, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4759, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4801, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5186, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4824, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4827, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4663, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4669, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4658, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4607, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4754, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5795, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5842, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5725, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5770, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5718, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: train_loss = 1.490, train_acc = 0.987\n",
            "tensor(1.4603, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4559, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4620, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4662, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4842, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4641, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5013, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4991, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4520, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4569, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4671, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4759, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4971, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4921, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4682, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4627, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4716, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4663, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4599, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4735, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4778, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5119, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4797, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4799, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4647, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4651, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4643, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4594, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4745, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5711, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5808, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5645, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5678, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5598, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: train_loss = 1.487, train_acc = 0.988\n",
            "tensor(1.4570, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4543, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4605, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4644, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4815, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4617, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4977, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4955, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4515, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4564, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4651, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4728, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4935, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4885, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4676, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4601, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4698, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4659, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4582, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4727, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4762, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5090, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4783, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4778, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4628, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4634, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4628, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4583, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4709, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5641, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5674, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5595, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5625, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5573, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: train_loss = 1.484, train_acc = 0.988\n",
            "tensor(1.4558, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4525, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4586, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4627, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4781, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4607, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4921, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4902, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4500, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4538, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4631, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4700, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4899, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4884, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4669, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4613, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4692, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4644, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4577, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4704, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4745, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5043, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4761, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4762, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4627, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4631, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4608, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4569, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4705, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5598, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5671, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5526, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5560, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5482, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: train_loss = 1.482, train_acc = 0.989\n",
            "tensor(1.4551, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4520, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4562, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4603, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4762, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4585, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4894, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4882, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4510, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4538, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4612, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4689, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4865, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4858, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4669, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4601, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4692, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4657, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4572, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4696, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4726, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5033, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4761, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4738, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4614, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4626, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4600, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4559, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4672, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5538, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5561, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5478, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5517, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5462, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: train_loss = 1.480, train_acc = 0.989\n",
            "tensor(1.4542, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4514, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4550, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4594, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4727, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4563, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4867, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4845, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4493, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4513, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4593, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4662, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4825, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4830, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4646, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4580, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4671, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4625, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4558, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4671, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4708, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4956, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4724, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4725, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4604, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4615, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4599, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4555, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4678, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5481, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5564, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5426, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5473, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5389, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: train_loss = 1.478, train_acc = 0.989\n",
            "tensor(1.4523, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4503, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4542, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4582, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4709, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4560, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4842, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4812, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4489, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4513, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4585, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4643, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4799, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4804, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4625, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4572, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4655, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4615, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4550, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4670, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4693, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4941, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4707, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4706, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4578, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4597, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4589, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4542, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4655, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5429, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5458, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5394, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5430, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5387, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: train_loss = 1.476, train_acc = 0.989\n",
            "tensor(1.4509, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4492, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4523, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4569, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4689, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4541, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4812, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4782, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4472, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4509, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4569, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4624, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4779, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4788, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4616, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4562, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4637, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4599, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4539, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4655, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4682, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4884, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4689, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4692, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4582, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4588, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4567, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4537, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4647, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5411, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5477, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5361, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5393, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5321, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: train_loss = 1.474, train_acc = 0.990\n",
            "tensor(1.4505, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4485, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4528, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4555, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4667, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4535, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4783, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4764, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4484, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4499, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4568, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4611, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4760, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4610, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4553, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4623, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4584, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4527, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4652, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4665, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4867, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4677, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4676, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4563, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4586, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4565, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4527, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4631, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5368, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5399, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5331, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5363, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.5314, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: train_loss = 1.472, train_acc = 0.990\n",
            "tensor(1.4499, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4481, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4517, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4551, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4648, device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-bd6dad0a01ba>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m           loss = F.cross_entropy(out.logits[:, :-1, :].flatten(0, -2), batch['input_ids'][:, 1:].flatten(),\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/modeling_gptj.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m    800\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/modeling_gptj.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m                 outputs = torch.utils.checkpoint.checkpoint(\n\u001b[0m\u001b[1;32m    647\u001b[0m                     \u001b[0mcreate_custom_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_reentrant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         return _checkpoint_without_reentrant(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_context\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_SingleLevelFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/modeling_gptj.py\u001b[0m in \u001b[0;36mcustom_forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    640\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m                         \u001b[0;31m# None for past_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/modeling_gptj.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         attn_outputs = self.attn(\n\u001b[0m\u001b[1;32m    291\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/modeling_gptj.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mq_pass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotary_dim\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0msincos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfixed_pos_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_rot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0mk_rot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rotary_pos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_rot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msincos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mq_rot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rotary_pos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_rot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msincos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/modeling_gptj.py\u001b[0m in \u001b[0;36mfixed_pos_embedding\u001b[0;34m(x, seq_dim, seq_len)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0minv_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0msinusoid_inp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i , j -> i j\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msinusoid_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msinusoid_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "train_loss_values = []\n",
        "train_acc_values = []\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "gpt.train()\n",
        "gpt.gradient_checkpointing_enable()\n",
        "k = 0\n",
        "\n",
        "metrics_path = '/content/metrics.json'\n",
        "batch_size = 8\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    num_samples = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "\n",
        "        k = k + 1\n",
        "        if k % 500 == 0:\n",
        "          print(k)\n",
        "          state = {'k' : k, 'epoch': num_epochs, 'lr_scheduler': lr_scheduler.state_dict(), 'state_dict': gpt.state_dict(), 'optimizer': optimizer.state_dict()}\n",
        "          gpt.save_pretrained(f\"/content/model-step-{k}/\")\n",
        "\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "          out = gpt.forward(**batch,)\n",
        "\n",
        "          loss = F.cross_entropy(out.logits[:, :-1, :].flatten(0, -2), batch['input_ids'][:, 1:].flatten(),\n",
        "                                reduction='mean', label_smoothing=0.1)\n",
        "\n",
        "        print(loss)\n",
        "\n",
        "        # calculate accuracy\n",
        "        preds = torch.argmax(out.logits[:, :-1, :], axis=-1)\n",
        "        acc = (preds == batch['input_ids'][:, 1:]).float().mean()\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(gpt.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * batch_size\n",
        "        running_acc += acc.item() * batch_size\n",
        "        num_samples += batch_size\n",
        "\n",
        "        lr_scheduler.step()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    epoch_loss = running_loss / num_samples\n",
        "    epoch_acc = running_acc / num_samples\n",
        "\n",
        "    # Append the loss and accuracy values to the respective lists\n",
        "    train_loss_values.append(epoch_loss)\n",
        "    train_acc_values.append(epoch_acc)\n",
        "\n",
        "    # Print the metrics for this epoch\n",
        "    print(f\"Epoch {epoch+1}: train_loss = {epoch_loss:.3f}, train_acc = {epoch_acc:.3f}\")\n",
        "\n",
        "gpt.save_pretrained(\"/content/model/\", state_dict=gpt.state_dict(), save_training_args=True)\n",
        "# torch.save(gpt.state_dict(), \"/content/weights.pth\")\n",
        "metrics = {'train_loss': train_loss_values, 'train_acc': train_acc_values}\n",
        "torch.save(metrics, metrics_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "blIcjxBV5xKA",
        "outputId": "9aa70286-8b27-45cc-b371-5fb1af8e512b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Create an index array for the epochs\n",
        "epochs = np.arange(len(train_loss_values))\n",
        "\n",
        "# Create two subplots, one for train loss and one for train accuracy\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax2 = fig.add_subplot(122)\n",
        "\n",
        "# Plot the train loss values on the first subplot\n",
        "ax1.plot(epochs, metrics['train_loss'])\n",
        "ax1.set_title('Train Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "\n",
        "# Plot the train accuracy values on the second subplot\n",
        "ax2.plot(epochs, metrics['train_acc'])\n",
        "ax2.set_title('Train Accuracy')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDxDlRR2zPTj"
      },
      "outputs": [],
      "source": [
        "!cp /content/model/* /content/drive/MyDrive/Tugas_Akhir/model/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXoNC_7Q_sgU"
      },
      "source": [
        "## Text generation example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h7y1_RS_sgV",
        "outputId": "f32e8eb4-41ab-462d-b349-9b4a523f6080"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------+----------------+\n",
            "|                                                         Prompt                                                         |                                          Generated Commands                                          |                                                      Correct Command                                                      | Similarity (%) |\n",
            "+------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------+----------------+\n",
            "| Buat route static menuju network 192.168.100.10 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0 di Router A | [Router A] configure terminal, ip route 192.168.100.10 255.255.255.0 g0/0/0,  end [eoc]<|endoftext|> | [Router X] configure terminal, router rip, version 2, network 192.168.7.0, network 192.168.100.0,  end [eoc]<|endoftext|> |     66.97%     |\n",
            "| Buat route static menuju network 192.168.100.10 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0 di Router A |  [Router A] configure terminal, ip route 192.168.50.0 255.255.255.0 g0/0/0,  end [eoc]<|endoftext|>  | [Router X] configure terminal, router rip, version 2, network 192.168.7.0, network 192.168.100.0,  end [eoc]<|endoftext|> |     65.75%     |\n",
            "+------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------+----------------+\n"
          ]
        }
      ],
      "source": [
        "#@title Load the finetuned model State_Dict to the Pretrained GPT-J Model\n",
        "\n",
        "import random\n",
        "from prettytable import PrettyTable\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Prompt\", \"Generated Commands\", \"Correct Command\", \"Similarity (%)\"]\n",
        "\n",
        "network1 = [\"192.168.7.0\", \"192.168.50.0\", \"192.168.32.0\", \"192.168.40.0\"]\n",
        "\n",
        "network2 = [\"192.168.100.0\", \"192.168.10.0\", \"192.168.2.0\", \"192.168.5.0\"]\n",
        "\n",
        "router = [\"Router A\", \"Router G\", \"Router X\"]\n",
        "\n",
        "interface = [\"G0/2\",\"G1\",\"F0/0/1\",\"F0/3\"]\n",
        "\n",
        "for i in range(1):\n",
        "  current = [random.choice(network1), random.choice(network2), random.choice(router)]\n",
        "  prompt = f\"Buat route static menuju network 192.168.100.10 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0 di Router A\" #@param {type:\"string\"}\n",
        "  correct_command = f\"[{current[2]}] configure terminal, router rip, version 2, network {current[0]}, network {current[1]},  end [eoc]<|endoftext|>\" #@param {type:\"string\"}\n",
        "  with torch.no_grad():\n",
        "    tokenized_prompt = tokenizer(f\"prompt:{prompt},commands:[\", truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
        "    for i in range(2):\n",
        "      tokenized_prompt = {key: value.to(device) for key, value in tokenized_prompt.items()}\n",
        "      out = gpt.generate(**tokenized_prompt, max_length=128, top_k=50, top_p=0.9, temperature=1.0, do_sample=True, repetition_penalty = 1.2, num_beams=1)\n",
        "      outputs = tokenizer.decode(out[0]).split(\",\")\n",
        "      outputs[1] = outputs[1].replace('\\n',', ')\n",
        "      similarity = SequenceMatcher(None, outputs[1].replace('commands:',''), correct_command).ratio() * 100\n",
        "      row = [prompt, outputs[1].replace('commands:',''), correct_command, f\"{similarity:.2f}%\"]\n",
        "      table.add_row(row)\n",
        "\n",
        "print(table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTx0UGzytaEF",
        "outputId": "79e53dca-73d7-4456-8a9d-419c8d8eb7a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "commands:[Router B] configure terminal,ip route 192.168. 100. 10 255.255.255.0 g0/0/0, end [eoc]<|endoftext|>\n",
            "bot_reply:Untuk membuat rute pada Router B melalui interface gigabit0/0/0 menuju jaringan lokal dengan IP 192.168.1.1, subnet mask 255.255.255.0 kita perlu menyatakan pemanggilan konfigurasi Router B terlebih dahulu dengan menggunakan perintah,[Router B] configure terminal,Lalu berdasarkan Interface, tipe vlan, alamat IP, dan subnet mask dapat digelarnya routing table pada Router B. Rangkaian perintah diakhiri dengan end [eoc]. Perintah lengkaps dapat dilakukan sebagai berikut:,[Router B] configure terminal,router rip,version 2,network 192.168.1.1,network 192.168.2.0,network 192.168.3.0,end [eoc]<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "input = f\"Buat route static menuju network 192.168.100.10 dengan subnet mask 255.255.255.0 melalui interface g/0/0/0 di Router A\" #@param {type:\"string\"}\n",
        "gpt.eval()\n",
        "prompt = f\"prompt:{input},commands:[\"\n",
        "with torch.no_grad():\n",
        "  preprocessed_prompt = tokenizer(prompt, truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
        "  preprocessed_prompt = {key: value.to(device) for key, value in preprocessed_prompt.items()}\n",
        "  out = gpt.generate(**preprocessed_prompt, max_length=128, top_k=50, top_p=0.9, temperature=1.0, do_sample=True, repetition_penalty = 1.2, num_beams=1)\n",
        "  outputs = tokenizer.decode(out[0]).split(\",\")\n",
        "  full_commands = outputs[1].replace('\\n',',')\n",
        "\n",
        "  print(full_commands)\n",
        "\n",
        "  prompt = f\"{outputs[1].replace('<|endoftext|>','')},bot_reply:\"\n",
        "  preprocessed_prompt = tokenizer(prompt, truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
        "  preprocessed_prompt = {key: value.to(device) for key, value in preprocessed_prompt.items()}\n",
        "  out = gpt.generate(**preprocessed_prompt, max_length=512, top_k=50, top_p=0.9, temperature=1.0, do_sample=True, repetition_penalty = 1.2, num_beams=1)\n",
        "  outputs = tokenizer.decode(out[0])\n",
        "\n",
        "  pattern = r\"bot_reply:(.*)\"\n",
        "  matches = re.findall(pattern, outputs, re.DOTALL)\n",
        "  bot_reply_text = matches[0].strip()\n",
        "  bot_reply = bot_reply_text.replace(\"\\n\",\",\")\n",
        "\n",
        "  print(f'bot_reply:{bot_reply}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxxp6X0_LRE8",
        "outputId": "364b8db9-9158-4fa7-9c3e-6d15b67cd700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "commands:[Router B] configure terminal\n",
            "ip route 192.168.100.0 255.255.255.0 g 0:00/0\n",
            " end [eoc],bot_reply:Untuk membuat rute pada Router B melalui interface g tujuan, subnet mask 255.255.255.0 dan alamat IP 192.168.1.0 skrip sederhana untuk mengaktifkan protokol RIP versi 2 pada Router B dan perkenalkan network 192.168.2.0 dan 192.168.3.0 ke dalam tabel routing, langkah awal adalah masuk ke mode konfigurasi menggunakan perintah berikut:\n",
            "[Router B] configure terminal\n",
            "Selanjutnya, gunakan perintah `router rip ripversion 2` untuk menandakan bahwa Anda akan menggunakan protocol RIP versi 2, dan tipe ethernets RELATED-cosortium NONSTRICTthan 90EtherHalfUnused adalah alternate IP address\n",
            "end [eoc]<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "486h6kYoSJaM",
        "outputId": "d9aa86b5-40db-4525-b818-c159e288487f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Table successfully converted to table.csv\n"
          ]
        }
      ],
      "source": [
        "data = [table.field_names] + [[str(cell).strip() for cell in row] for row in table._rows]\n",
        "\n",
        "# Write the data to a CSV file\n",
        "filename = \"table.csv\"\n",
        "with open(filename, \"w\", newline=\"\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerows(data)\n",
        "\n",
        "print(f\"Table successfully converted to {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwWGmpxmMkMT"
      },
      "source": [
        "# CELL 3: LOADING FINETUNED MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhClfN3tOiyU",
        "outputId": "8d43c555-c154-48c9-9cb8-d28b92f332b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoK-0J44NGm6",
        "outputId": "237b7752-5b62-4078-a5a5-43a8935c8370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libaria2-0 libc-ares2 lz4\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2 liblz4-tool lz4\n",
            "0 upgraded, 5 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 1,605 kB of archives.\n",
            "After this operation, 5,689 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-ares2 amd64 1.18.1-1ubuntu0.22.04.2 [45.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libaria2-0 amd64 1.36.0-1 [1,086 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 aria2 amd64 1.36.0-1 [381 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 lz4 amd64 1.9.3-2build2 [90.0 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 liblz4-tool all 1.9.3-2build2 [2,342 B]\n",
            "Fetched 1,605 kB in 1s (2,952 kB/s)\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 120874 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.18.1-1ubuntu0.22.04.2_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Selecting previously unselected package lz4.\n",
            "Preparing to unpack .../lz4_1.9.3-2build2_amd64.deb ...\n",
            "Unpacking lz4 (1.9.3-2build2) ...\n",
            "Selecting previously unselected package liblz4-tool.\n",
            "Preparing to unpack .../liblz4-tool_1.9.3-2build2_all.deb ...\n",
            "Unpacking liblz4-tool (1.9.3-2build2) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.2) ...\n",
            "Setting up lz4 (1.9.3-2build2) ...\n",
            "Setting up liblz4-tool (1.9.3-2build2) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            " *** Download Progress Summary as of Wed Oct 25 02:48:50 2023 *** \n",
            "=\n",
            "[#06e73f 1.4GiB/5.7GiB(25%) CN:16 DL:145MiB ETA:30s]\n",
            "FILE: /content/downloaded//model310723.bin\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Wed Oct 25 02:49:01 2023 *** \n",
            "=\n",
            "[#06e73f 3.8GiB/5.7GiB(66%) CN:16 DL:218MiB ETA:9s]\n",
            "FILE: /content/downloaded//model310723.bin\n",
            "-\n",
            "\n",
            " *** Download Progress Summary as of Wed Oct 25 02:49:12 2023 *** \n",
            "=\n",
            "[#06e73f 5.1GiB/5.7GiB(89%) CN:16 DL:133MiB ETA:4s]\n",
            "FILE: /content/downloaded//model310723.bin\n",
            "-\n",
            "\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "06e73f|\u001b[1;32mOK\u001b[0m  |   172MiB/s|/content/downloaded//model310723.bin\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ],
      "source": [
        "#@title ### Dowload Pre-Trained Model\n",
        "#@markdown *Support drive link, Huggingface link*\n",
        "\n",
        "!apt install liblz4-tool aria2\n",
        "import os\n",
        "from urllib.parse import unquote\n",
        "destination_dir = \"/content/downloaded/\"\n",
        "\n",
        "#### select your model here!####\n",
        "\n",
        "### model lama mas hanung\n",
        "#custom_urls = \"https://huggingface.co/hanungaddi/model_router/resolve/main/model310723.bin\" #@param {type:\"string\"}\n",
        "#optional_huggingface_token =  \"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\" #@param {type:\"string\"}\n",
        "\n",
        "## model fine tuning Lidia\n",
        "custom_urls = \"https://huggingface.co/Lilidiasaja/model6.1/blob/main/pytorch_model.bin\" \n",
        "optional_huggingface_token =  \"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\"\n",
        "\n",
        "model_url = custom_urls+\", \" if custom_urls else \"\"\n",
        "\n",
        "token = optional_huggingface_token if optional_huggingface_token else \"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\"\n",
        "user_header = f\"\\\"Authorization: Bearer {token}\\\"\"\n",
        "\n",
        "def download(url):\n",
        "  try:\n",
        "    have_drive_link\n",
        "  except:\n",
        "    if \"drive.google.com\" in url:\n",
        "      # I'm sorry drive ID enjoyer, this will make ID useless :(\n",
        "      !pip install -U gdown\n",
        "      have_drive_link = True\n",
        "  links_and_paths = url.split(',')\n",
        "  !mkdir -p {destination_dir}\n",
        "  http_links = []\n",
        "  huggingface_links = []\n",
        "  for link_or_path in links_and_paths:\n",
        "    link_or_path = link_or_path.strip()\n",
        "    if not link_or_path:\n",
        "      continue\n",
        "\n",
        "    # if any(link_or_path.startswith(prefix.lower()) for prefix in prefixes):\n",
        "    #   handle_manual(link_or_path)\n",
        "    #   continue\n",
        "\n",
        "    # if 'github.com' in link_or_path and ( '.git' in link_or_path or not '.' in link_or_path.split('/')[-1] ):\n",
        "    #   extension_repo.append(link_or_path)\n",
        "    #   continue\n",
        "\n",
        "    if '.yaml' in link_or_path or '.yml' in link_or_path or 'discord' in link_or_path:\n",
        "      !wget {link_or_path} -P {destination_dir} -c\n",
        "    elif 'drive.google' in link_or_path:\n",
        "      if 'folders' in link_or_path:\n",
        "        !gdown --folder {link_or_path} -O {destination_dir} --fuzzy -c\n",
        "      else:\n",
        "        !gdown {link_or_path} -O {destination_dir} --fuzzy -c\n",
        "    elif 'huggingface' in link_or_path:\n",
        "      if '/blob/' in link_or_path:\n",
        "        link_or_path = link_or_path.replace('/blob/', '/resolve/')\n",
        "      huggingface_links.append(link_or_path)\n",
        "    elif 'http' in link_or_path or 'magnet' in link_or_path:\n",
        "      http_links.append(link_or_path)\n",
        "    elif '/' in link_or_path:\n",
        "      if not os.path.exists('/content/gdrive/MyDrive'):\n",
        "        print('Looks like there\\'s a path in your url. You need to mount your drive first.')\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/gdrive')\n",
        "      !rsync -avr --progress /content/gdrive/MyDrive/{link_or_path} {destination_dir}\n",
        "    else:\n",
        "      !gdown {link_or_path} -O {destination_dir} --fuzzy -c\n",
        "  if http_links:\n",
        "    links_string = ' '.join(['\"{}\"'.format(x) for x in http_links])\n",
        "    !aria2c --optimize-concurrent-downloads --console-log-level=error --summary-interval=10 -j5 -x16 -s16 -k1M -c -d {destination_dir}  -Z {links_string}\n",
        "    del links_string\n",
        "  if huggingface_links:\n",
        "    # links_string = ' '.join(['\"{}\"'.format(x) for x in huggingface_links])\n",
        "    # !aria2c --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {destination_dir} -Z {links_string}\n",
        "    # for link in huggingface_links:\n",
        "    #   !aria2c --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {destination_dir} -o {link.split('/')[-1]} {link}\n",
        "    links_string = '\\n'.join(['{}\\n\\tout={}'.format(x,unquote(x.split('/')[-1])) for x in huggingface_links])\n",
        "    !echo -e '{links_string}' | aria2c --header={user_header} --optimize-concurrent-downloads --console-log-level=error --summary-interval=10 -i- -j5 -x16 -s16 -k1M -c -d {destination_dir}\n",
        "\n",
        "\n",
        "download(model_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8yfklpC--mcS"
      },
      "outputs": [],
      "source": [
        "#@title Model to 8 bits Converter\n",
        "#@markdown RUN THIS BEFORE LOADING THE MODEL\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import transformers\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.cuda.amp import custom_fwd, custom_bwd\n",
        "\n",
        "from bitsandbytes.functional import quantize_blockwise, dequantize_blockwise\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "class FrozenBNBLinear(nn.Module):\n",
        "    def __init__(self, weight, absmax, code, bias=None):\n",
        "        assert isinstance(bias, nn.Parameter) or bias is None\n",
        "        super().__init__()\n",
        "        self.out_features, self.in_features = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias)\n",
        "        output_clone = output.clone()\n",
        "        if self.adapter:\n",
        "            output_clone += self.adapter(input)\n",
        "        return output_clone\n",
        "\n",
        "    @classmethod\n",
        "    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n",
        "        return cls(weights_int8, *state, linear.bias)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n",
        "\n",
        "\n",
        "class DequantizeAndLinear(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    @custom_fwd\n",
        "    def forward(ctx, input: torch.Tensor, weights_quantized: torch.ByteTensor,\n",
        "                absmax: torch.FloatTensor, code: torch.FloatTensor, bias: torch.FloatTensor):\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        ctx.save_for_backward(input, weights_quantized, absmax, code)\n",
        "        ctx._has_bias = bias is not None\n",
        "        return F.linear(input, weights_deq, bias)\n",
        "\n",
        "    @staticmethod\n",
        "    @custom_bwd\n",
        "    def backward(ctx, grad_output: torch.Tensor):\n",
        "        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n",
        "        input, weights_quantized, absmax, code = ctx.saved_tensors\n",
        "        # grad_output: [*batch, out_features]\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        grad_input = grad_output @ weights_deq\n",
        "        grad_bias = grad_output.flatten(0, -2).sum(dim=0) if ctx._has_bias else None\n",
        "        return grad_input, None, None, None, grad_bias\n",
        "\n",
        "\n",
        "class FrozenBNBEmbedding(nn.Module):\n",
        "    def __init__(self, weight, absmax, code):\n",
        "        super().__init__()\n",
        "        self.num_embeddings, self.embedding_dim = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        "\n",
        "    def forward(self, input, **kwargs):\n",
        "        with torch.no_grad():\n",
        "            # note: both quantuized weights and input indices are *not* differentiable\n",
        "            weight_deq = dequantize_blockwise(self.weight, absmax=self.absmax, code=self.code)\n",
        "            output = F.embedding(input, weight_deq, **kwargs)\n",
        "        if self.adapter:\n",
        "            output += self.adapter(input)\n",
        "        return output\n",
        "\n",
        "    @classmethod\n",
        "    def from_embedding(cls, embedding: nn.Embedding) -> \"FrozenBNBEmbedding\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(embedding.weight)\n",
        "        return cls(weights_int8, *state)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.num_embeddings}, {self.embedding_dim})\"\n",
        "\n",
        "\n",
        "def quantize_blockise_lowmemory(matrix: torch.Tensor, chunk_size: int = 2 ** 20):\n",
        "    assert chunk_size % 4096 == 0\n",
        "    code = None\n",
        "    chunks = []\n",
        "    absmaxes = []\n",
        "    flat_tensor = matrix.view(-1)\n",
        "    for i in range((matrix.numel() - 1) // chunk_size + 1):\n",
        "        input_chunk = flat_tensor[i * chunk_size: (i + 1) * chunk_size].clone()\n",
        "        quantized_chunk, (absmax_chunk, code) = quantize_blockwise(input_chunk, code=code)\n",
        "        chunks.append(quantized_chunk)\n",
        "        absmaxes.append(absmax_chunk)\n",
        "\n",
        "    matrix_i8 = torch.cat(chunks).reshape_as(matrix)\n",
        "    absmax = torch.cat(absmaxes)\n",
        "    return matrix_i8, (absmax, code)\n",
        "\n",
        "\n",
        "def convert_to_int8(model):\n",
        "    \"\"\"Convert linear and embedding modules to 8-bit with optional adapters\"\"\"\n",
        "    for module in list(model.modules()):\n",
        "        for name, child in module.named_children():\n",
        "            if isinstance(child, nn.Linear):\n",
        "                print(name, child)\n",
        "                setattr(\n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBLinear(\n",
        "                        weight=torch.zeros(child.out_features, child.in_features, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                        bias=child.bias,\n",
        "                    ),\n",
        "                )\n",
        "            elif isinstance(child, nn.Embedding):\n",
        "                setattr(\n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBEmbedding(\n",
        "                        weight=torch.zeros(child.num_embeddings, child.embedding_dim, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "class GPTJBlock(transformers.models.gptj.modeling_gptj.GPTJBlock):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        convert_to_int8(self.attn)\n",
        "        convert_to_int8(self.mlp)\n",
        "\n",
        "\n",
        "class GPTJModel(transformers.models.gptj.modeling_gptj.GPTJModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        "\n",
        "\n",
        "class GPTJForCausalLM(transformers.models.gptj.modeling_gptj.GPTJForCausalLM):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        "\n",
        "\n",
        "transformers.models.gptj.modeling_gptj.GPTJBlock = GPTJBlock\n",
        "\n",
        "class T5ForConditionalGeneration(transformers.models.t5.modeling_t5.T5ForConditionalGeneration):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        # convert_to_int8(self)\n",
        "\n",
        "transformers.models.t5.modeling_t5.T5ForConditionalGeneration = T5ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "d7d186e3da3b4831a8e7d8bdc773631a",
            "d3c8079de8e04277926412f7e343bd0b",
            "24ab1d48242d4c668eab496810753a7a",
            "366091a81d4842ab89a9643f8cfd34b5",
            "486754f3ebc3440abcbbaad87f8c7e30",
            "a4568c0ed6d34f8cba428da42d6ff478",
            "ee661ca21d3c4155ac1c3a163a1fffc6",
            "a7e5906fe0d14325bf63aa5e84a5524d",
            "aa8d2b3523a048c89d6e034fd01e198e",
            "1d328f4dbfb54e4797e99751952ac4ff",
            "8d8150bb405a40028d75f3bfe805ed9a",
            "e31c90c6424f47e09924a9c2eb21d2ef",
            "f399eea3124e429fa744e10ee356d4d4",
            "8313974a676645228503e24f63f9891b",
            "4a426f407ea745e49627017d8f79ea4f",
            "7a3ef1d8b2c7455d977af860e2f109a3",
            "1d3c5612c59d4535bf3811266d39769d",
            "344e08676ea84f98a57fc283e0c49ae1",
            "03da351ee8d842c89a2041c41c95b46e",
            "001143a4ecd846ad84d9e565dc056737",
            "40dc912536714c699562e042ffe1b4a1",
            "a6e45f80f1c34459a13e1867dacbf343",
            "32420ecc341a421e98cf8c7797273be6",
            "b122d4bbac594d26ad38940fa8ee2f26",
            "7112919fc52644428c73b2478f64993b",
            "92662d8e06324f589833a7c1bda4235b",
            "0381111224ec422cae8d37bf3e06706f",
            "81bb3df4536b431382182fff2fce8d54",
            "41f07774e30c4472818585d4124be7fb",
            "f667c17b12aa4545b16c1586d283ad91",
            "5e5048040ed2432dbe8f7c0526bed45c",
            "11b8fdedfd0044ce8417f189dd15b4a3",
            "b24be892edb54d2a8ddfcf26a47eab02",
            "a796b2e81de246e189dd8e3d03acf254",
            "da0ae5acc64240498feb837a37247450",
            "0d52b83c460a4ef08cca248348e9aa66",
            "fc3319bfc16d49fbaefbb1d16a4dbd02",
            "2763eb9aaead4b8d9603c2e221f35175",
            "a7122ceea3604476b531d5e7d9123906",
            "2d8d32832aed423ab71c10831b68f227",
            "5379d9d3752f45939e826617c2a77ec9",
            "5c38c8f010684c328a2ae84aea419ce5",
            "b1b31860c9e8494fbdaaa9cb55dc24a6",
            "d66ea430381f496d9125ccb85cc9617c",
            "9c12090aa8364f89aa34c37f3a073de7",
            "eee1e490eb8a416abe42b16e566d9c89",
            "da0b8feef55e45fba010a72db202b382",
            "741f568c62174a728fa3d82eb3ef7655",
            "4b64e15e47ef4282b3ff7a5d171731b4",
            "757ed197f5b641008c6cacf82f1722ad",
            "4c6c53c7df6f43a2a9587911e5585e63",
            "13e567efe19c4ad79002695870a949d5",
            "427b1a89c1c54de6b34f320a8401d9e0",
            "ed6f10da7e70445ea675975a63879da3",
            "4b9339d842fa40f2be1e3dcc8c7d2eb4",
            "6a9888a9617f436e9c4bdfaade5c0a44",
            "3d46182cae4d4e9c9a6129ec4c4d045f",
            "386621f03649421c91ffc028d87a45c8",
            "e45d850417ce4643b8453c4bda55bcc1",
            "106c60f876b645b6b9803a57f69afbed",
            "c060c0c065bd4c3b9b09a6e8cdb7369d",
            "1e2cb681aade4267ad7ffb5c6b24ec7b",
            "4b66d27d5c594690a40e62a7f23b23c8",
            "67086759051a4e72abe753e2ba322f10",
            "06374d2c569142998d2311ba5d952b99",
            "994e63412f8749c9a181bd06f8f5b9da",
            "71e2993cb61c451da872695a35b2e612",
            "cbd418b0334c488e9e3b6a10d9328a7e",
            "0cfd21107c0f47d097892245cdce09c2",
            "648dedb4d5b24b27bc6965bd2be3ee6f",
            "fe6e613d946a4ff0a3d6a5cd095e624e",
            "2e575b165bff4d5eb43d04401a0813f0",
            "f44a86dd9f4b4d53a308b32f1f8a6ec8",
            "779b436f16074b6b9c08e82ff95397aa",
            "d38b5ac2f43944cda8be7d725c33bd04",
            "8898abb3e1ab40f6af2a0f5b98df1466",
            "e069c7bb58084d5d9bf4e577831b3704"
          ]
        },
        "id": "ynMtCgD8ZNV5",
        "outputId": "47418edb-02d7-4ad6-8e3a-8986c5749757"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7d186e3da3b4831a8e7d8bdc773631a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/930 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e31c90c6424f47e09924a9c2eb21d2ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/619 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32420ecc341a421e98cf8c7797273be6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/779k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a796b2e81de246e189dd8e3d03acf254",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c12090aa8364f89aa34c37f3a073de7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a9888a9617f436e9c4bdfaade5c0a44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/3.94k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71e2993cb61c451da872695a35b2e612",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/357 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Load GPT-J Tokenizer\n",
        "config = transformers.GPTJConfig.from_pretrained(\"EleutherAI/gpt-j-6b\")\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6b\")\n",
        "\n",
        "config.pad_token_id = config.eos_token_id\n",
        "tokenizer.pad_token = config.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8ac5adbdd66f4a1288af1cf68d42e979",
            "a14fb130a27742da9f155b737bdb0e65",
            "f7d7094786524c219116083bc7d944de",
            "6d6a833dac7a4916b56bc3b6eda98d02",
            "d08bba4dfd5c42959e2169e15816cac7",
            "2a153a8ccde7420ba66828a422ef74d2",
            "5b790297f7204c3fb4bd828bba6e624e",
            "e8600d66cfc1431f8b2ce106d8c4c3ef",
            "43d9cbf70a684436b4e585a620bd5d49",
            "1afcb6956bf44c508e8557320f54ee95",
            "0c6a70cc6fd7465799283e6108650ec2",
            "9dce245b6c73463b91c50bb0398c3dc1",
            "f791373a068c41e2bcb09912b3b8ae47",
            "57ad044d24294fa58e9c7a0f98a51e3e",
            "b0bcc221d81644d79432c3cab1760a0d",
            "25e50b25772545d784852d47376d7a3e",
            "893a015bcde7437796e30cdb63fdb178",
            "ecf8b83d036041cf8b6cd2346f618471",
            "5d2c9d0c717d420083af9a045b6015b7",
            "9f618c0fa81e4dafab88e9824edaf4e2",
            "c823e95d20dc4d5ea8ef7c093e887982",
            "bb59191bf62f49d5a4a9ab83fbf536ad"
          ]
        },
        "id": "TY12q9PE5fnn",
        "outputId": "03e079c5-3e2d-43fb-f13f-75dad08e6ebf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ac5adbdd66f4a1288af1cf68d42e979",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9dce245b6c73463b91c50bb0398c3dc1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/5.75G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "k_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "v_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "out_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
            "fc_in Linear(in_features=4096, out_features=16384, bias=True)\n",
            "fc_out Linear(in_features=16384, out_features=4096, bias=True)\n",
            "lm_head Linear(in_features=4096, out_features=50400, bias=True)\n",
            "Adding adapter to transformer.wte\n",
            "Initializing transformer.wte\n",
            "Adding adapter to transformer.h.0.attn.k_proj\n",
            "Initializing transformer.h.0.attn.k_proj\n",
            "Adding adapter to transformer.h.0.attn.v_proj\n",
            "Initializing transformer.h.0.attn.v_proj\n",
            "Adding adapter to transformer.h.0.attn.q_proj\n",
            "Initializing transformer.h.0.attn.q_proj\n",
            "Adding adapter to transformer.h.0.attn.out_proj\n",
            "Initializing transformer.h.0.attn.out_proj\n",
            "Adding adapter to transformer.h.0.mlp.fc_in\n",
            "Initializing transformer.h.0.mlp.fc_in\n",
            "Adding adapter to transformer.h.0.mlp.fc_out\n",
            "Initializing transformer.h.0.mlp.fc_out\n",
            "Adding adapter to transformer.h.1.attn.k_proj\n",
            "Initializing transformer.h.1.attn.k_proj\n",
            "Adding adapter to transformer.h.1.attn.v_proj\n",
            "Initializing transformer.h.1.attn.v_proj\n",
            "Adding adapter to transformer.h.1.attn.q_proj\n",
            "Initializing transformer.h.1.attn.q_proj\n",
            "Adding adapter to transformer.h.1.attn.out_proj\n",
            "Initializing transformer.h.1.attn.out_proj\n",
            "Adding adapter to transformer.h.1.mlp.fc_in\n",
            "Initializing transformer.h.1.mlp.fc_in\n",
            "Adding adapter to transformer.h.1.mlp.fc_out\n",
            "Initializing transformer.h.1.mlp.fc_out\n",
            "Adding adapter to transformer.h.2.attn.k_proj\n",
            "Initializing transformer.h.2.attn.k_proj\n",
            "Adding adapter to transformer.h.2.attn.v_proj\n",
            "Initializing transformer.h.2.attn.v_proj\n",
            "Adding adapter to transformer.h.2.attn.q_proj\n",
            "Initializing transformer.h.2.attn.q_proj\n",
            "Adding adapter to transformer.h.2.attn.out_proj\n",
            "Initializing transformer.h.2.attn.out_proj\n",
            "Adding adapter to transformer.h.2.mlp.fc_in\n",
            "Initializing transformer.h.2.mlp.fc_in\n",
            "Adding adapter to transformer.h.2.mlp.fc_out\n",
            "Initializing transformer.h.2.mlp.fc_out\n",
            "Adding adapter to transformer.h.3.attn.k_proj\n",
            "Initializing transformer.h.3.attn.k_proj\n",
            "Adding adapter to transformer.h.3.attn.v_proj\n",
            "Initializing transformer.h.3.attn.v_proj\n",
            "Adding adapter to transformer.h.3.attn.q_proj\n",
            "Initializing transformer.h.3.attn.q_proj\n",
            "Adding adapter to transformer.h.3.attn.out_proj\n",
            "Initializing transformer.h.3.attn.out_proj\n",
            "Adding adapter to transformer.h.3.mlp.fc_in\n",
            "Initializing transformer.h.3.mlp.fc_in\n",
            "Adding adapter to transformer.h.3.mlp.fc_out\n",
            "Initializing transformer.h.3.mlp.fc_out\n",
            "Adding adapter to transformer.h.4.attn.k_proj\n",
            "Initializing transformer.h.4.attn.k_proj\n",
            "Adding adapter to transformer.h.4.attn.v_proj\n",
            "Initializing transformer.h.4.attn.v_proj\n",
            "Adding adapter to transformer.h.4.attn.q_proj\n",
            "Initializing transformer.h.4.attn.q_proj\n",
            "Adding adapter to transformer.h.4.attn.out_proj\n",
            "Initializing transformer.h.4.attn.out_proj\n",
            "Adding adapter to transformer.h.4.mlp.fc_in\n",
            "Initializing transformer.h.4.mlp.fc_in\n",
            "Adding adapter to transformer.h.4.mlp.fc_out\n",
            "Initializing transformer.h.4.mlp.fc_out\n",
            "Adding adapter to transformer.h.5.attn.k_proj\n",
            "Initializing transformer.h.5.attn.k_proj\n",
            "Adding adapter to transformer.h.5.attn.v_proj\n",
            "Initializing transformer.h.5.attn.v_proj\n",
            "Adding adapter to transformer.h.5.attn.q_proj\n",
            "Initializing transformer.h.5.attn.q_proj\n",
            "Adding adapter to transformer.h.5.attn.out_proj\n",
            "Initializing transformer.h.5.attn.out_proj\n",
            "Adding adapter to transformer.h.5.mlp.fc_in\n",
            "Initializing transformer.h.5.mlp.fc_in\n",
            "Adding adapter to transformer.h.5.mlp.fc_out\n",
            "Initializing transformer.h.5.mlp.fc_out\n",
            "Adding adapter to transformer.h.6.attn.k_proj\n",
            "Initializing transformer.h.6.attn.k_proj\n",
            "Adding adapter to transformer.h.6.attn.v_proj\n",
            "Initializing transformer.h.6.attn.v_proj\n",
            "Adding adapter to transformer.h.6.attn.q_proj\n",
            "Initializing transformer.h.6.attn.q_proj\n",
            "Adding adapter to transformer.h.6.attn.out_proj\n",
            "Initializing transformer.h.6.attn.out_proj\n",
            "Adding adapter to transformer.h.6.mlp.fc_in\n",
            "Initializing transformer.h.6.mlp.fc_in\n",
            "Adding adapter to transformer.h.6.mlp.fc_out\n",
            "Initializing transformer.h.6.mlp.fc_out\n",
            "Adding adapter to transformer.h.7.attn.k_proj\n",
            "Initializing transformer.h.7.attn.k_proj\n",
            "Adding adapter to transformer.h.7.attn.v_proj\n",
            "Initializing transformer.h.7.attn.v_proj\n",
            "Adding adapter to transformer.h.7.attn.q_proj\n",
            "Initializing transformer.h.7.attn.q_proj\n",
            "Adding adapter to transformer.h.7.attn.out_proj\n",
            "Initializing transformer.h.7.attn.out_proj\n",
            "Adding adapter to transformer.h.7.mlp.fc_in\n",
            "Initializing transformer.h.7.mlp.fc_in\n",
            "Adding adapter to transformer.h.7.mlp.fc_out\n",
            "Initializing transformer.h.7.mlp.fc_out\n",
            "Adding adapter to transformer.h.8.attn.k_proj\n",
            "Initializing transformer.h.8.attn.k_proj\n",
            "Adding adapter to transformer.h.8.attn.v_proj\n",
            "Initializing transformer.h.8.attn.v_proj\n",
            "Adding adapter to transformer.h.8.attn.q_proj\n",
            "Initializing transformer.h.8.attn.q_proj\n",
            "Adding adapter to transformer.h.8.attn.out_proj\n",
            "Initializing transformer.h.8.attn.out_proj\n",
            "Adding adapter to transformer.h.8.mlp.fc_in\n",
            "Initializing transformer.h.8.mlp.fc_in\n",
            "Adding adapter to transformer.h.8.mlp.fc_out\n",
            "Initializing transformer.h.8.mlp.fc_out\n",
            "Adding adapter to transformer.h.9.attn.k_proj\n",
            "Initializing transformer.h.9.attn.k_proj\n",
            "Adding adapter to transformer.h.9.attn.v_proj\n",
            "Initializing transformer.h.9.attn.v_proj\n",
            "Adding adapter to transformer.h.9.attn.q_proj\n",
            "Initializing transformer.h.9.attn.q_proj\n",
            "Adding adapter to transformer.h.9.attn.out_proj\n",
            "Initializing transformer.h.9.attn.out_proj\n",
            "Adding adapter to transformer.h.9.mlp.fc_in\n",
            "Initializing transformer.h.9.mlp.fc_in\n",
            "Adding adapter to transformer.h.9.mlp.fc_out\n",
            "Initializing transformer.h.9.mlp.fc_out\n",
            "Adding adapter to transformer.h.10.attn.k_proj\n",
            "Initializing transformer.h.10.attn.k_proj\n",
            "Adding adapter to transformer.h.10.attn.v_proj\n",
            "Initializing transformer.h.10.attn.v_proj\n",
            "Adding adapter to transformer.h.10.attn.q_proj\n",
            "Initializing transformer.h.10.attn.q_proj\n",
            "Adding adapter to transformer.h.10.attn.out_proj\n",
            "Initializing transformer.h.10.attn.out_proj\n",
            "Adding adapter to transformer.h.10.mlp.fc_in\n",
            "Initializing transformer.h.10.mlp.fc_in\n",
            "Adding adapter to transformer.h.10.mlp.fc_out\n",
            "Initializing transformer.h.10.mlp.fc_out\n",
            "Adding adapter to transformer.h.11.attn.k_proj\n",
            "Initializing transformer.h.11.attn.k_proj\n",
            "Adding adapter to transformer.h.11.attn.v_proj\n",
            "Initializing transformer.h.11.attn.v_proj\n",
            "Adding adapter to transformer.h.11.attn.q_proj\n",
            "Initializing transformer.h.11.attn.q_proj\n",
            "Adding adapter to transformer.h.11.attn.out_proj\n",
            "Initializing transformer.h.11.attn.out_proj\n",
            "Adding adapter to transformer.h.11.mlp.fc_in\n",
            "Initializing transformer.h.11.mlp.fc_in\n",
            "Adding adapter to transformer.h.11.mlp.fc_out\n",
            "Initializing transformer.h.11.mlp.fc_out\n",
            "Adding adapter to transformer.h.12.attn.k_proj\n",
            "Initializing transformer.h.12.attn.k_proj\n",
            "Adding adapter to transformer.h.12.attn.v_proj\n",
            "Initializing transformer.h.12.attn.v_proj\n",
            "Adding adapter to transformer.h.12.attn.q_proj\n",
            "Initializing transformer.h.12.attn.q_proj\n",
            "Adding adapter to transformer.h.12.attn.out_proj\n",
            "Initializing transformer.h.12.attn.out_proj\n",
            "Adding adapter to transformer.h.12.mlp.fc_in\n",
            "Initializing transformer.h.12.mlp.fc_in\n",
            "Adding adapter to transformer.h.12.mlp.fc_out\n",
            "Initializing transformer.h.12.mlp.fc_out\n",
            "Adding adapter to transformer.h.13.attn.k_proj\n",
            "Initializing transformer.h.13.attn.k_proj\n",
            "Adding adapter to transformer.h.13.attn.v_proj\n",
            "Initializing transformer.h.13.attn.v_proj\n",
            "Adding adapter to transformer.h.13.attn.q_proj\n",
            "Initializing transformer.h.13.attn.q_proj\n",
            "Adding adapter to transformer.h.13.attn.out_proj\n",
            "Initializing transformer.h.13.attn.out_proj\n",
            "Adding adapter to transformer.h.13.mlp.fc_in\n",
            "Initializing transformer.h.13.mlp.fc_in\n",
            "Adding adapter to transformer.h.13.mlp.fc_out\n",
            "Initializing transformer.h.13.mlp.fc_out\n",
            "Adding adapter to transformer.h.14.attn.k_proj\n",
            "Initializing transformer.h.14.attn.k_proj\n",
            "Adding adapter to transformer.h.14.attn.v_proj\n",
            "Initializing transformer.h.14.attn.v_proj\n",
            "Adding adapter to transformer.h.14.attn.q_proj\n",
            "Initializing transformer.h.14.attn.q_proj\n",
            "Adding adapter to transformer.h.14.attn.out_proj\n",
            "Initializing transformer.h.14.attn.out_proj\n",
            "Adding adapter to transformer.h.14.mlp.fc_in\n",
            "Initializing transformer.h.14.mlp.fc_in\n",
            "Adding adapter to transformer.h.14.mlp.fc_out\n",
            "Initializing transformer.h.14.mlp.fc_out\n",
            "Adding adapter to transformer.h.15.attn.k_proj\n",
            "Initializing transformer.h.15.attn.k_proj\n",
            "Adding adapter to transformer.h.15.attn.v_proj\n",
            "Initializing transformer.h.15.attn.v_proj\n",
            "Adding adapter to transformer.h.15.attn.q_proj\n",
            "Initializing transformer.h.15.attn.q_proj\n",
            "Adding adapter to transformer.h.15.attn.out_proj\n",
            "Initializing transformer.h.15.attn.out_proj\n",
            "Adding adapter to transformer.h.15.mlp.fc_in\n",
            "Initializing transformer.h.15.mlp.fc_in\n",
            "Adding adapter to transformer.h.15.mlp.fc_out\n",
            "Initializing transformer.h.15.mlp.fc_out\n",
            "Adding adapter to transformer.h.16.attn.k_proj\n",
            "Initializing transformer.h.16.attn.k_proj\n",
            "Adding adapter to transformer.h.16.attn.v_proj\n",
            "Initializing transformer.h.16.attn.v_proj\n",
            "Adding adapter to transformer.h.16.attn.q_proj\n",
            "Initializing transformer.h.16.attn.q_proj\n",
            "Adding adapter to transformer.h.16.attn.out_proj\n",
            "Initializing transformer.h.16.attn.out_proj\n",
            "Adding adapter to transformer.h.16.mlp.fc_in\n",
            "Initializing transformer.h.16.mlp.fc_in\n",
            "Adding adapter to transformer.h.16.mlp.fc_out\n",
            "Initializing transformer.h.16.mlp.fc_out\n",
            "Adding adapter to transformer.h.17.attn.k_proj\n",
            "Initializing transformer.h.17.attn.k_proj\n",
            "Adding adapter to transformer.h.17.attn.v_proj\n",
            "Initializing transformer.h.17.attn.v_proj\n",
            "Adding adapter to transformer.h.17.attn.q_proj\n",
            "Initializing transformer.h.17.attn.q_proj\n",
            "Adding adapter to transformer.h.17.attn.out_proj\n",
            "Initializing transformer.h.17.attn.out_proj\n",
            "Adding adapter to transformer.h.17.mlp.fc_in\n",
            "Initializing transformer.h.17.mlp.fc_in\n",
            "Adding adapter to transformer.h.17.mlp.fc_out\n",
            "Initializing transformer.h.17.mlp.fc_out\n",
            "Adding adapter to transformer.h.18.attn.k_proj\n",
            "Initializing transformer.h.18.attn.k_proj\n",
            "Adding adapter to transformer.h.18.attn.v_proj\n",
            "Initializing transformer.h.18.attn.v_proj\n",
            "Adding adapter to transformer.h.18.attn.q_proj\n",
            "Initializing transformer.h.18.attn.q_proj\n",
            "Adding adapter to transformer.h.18.attn.out_proj\n",
            "Initializing transformer.h.18.attn.out_proj\n",
            "Adding adapter to transformer.h.18.mlp.fc_in\n",
            "Initializing transformer.h.18.mlp.fc_in\n",
            "Adding adapter to transformer.h.18.mlp.fc_out\n",
            "Initializing transformer.h.18.mlp.fc_out\n",
            "Adding adapter to transformer.h.19.attn.k_proj\n",
            "Initializing transformer.h.19.attn.k_proj\n",
            "Adding adapter to transformer.h.19.attn.v_proj\n",
            "Initializing transformer.h.19.attn.v_proj\n",
            "Adding adapter to transformer.h.19.attn.q_proj\n",
            "Initializing transformer.h.19.attn.q_proj\n",
            "Adding adapter to transformer.h.19.attn.out_proj\n",
            "Initializing transformer.h.19.attn.out_proj\n",
            "Adding adapter to transformer.h.19.mlp.fc_in\n",
            "Initializing transformer.h.19.mlp.fc_in\n",
            "Adding adapter to transformer.h.19.mlp.fc_out\n",
            "Initializing transformer.h.19.mlp.fc_out\n",
            "Adding adapter to transformer.h.20.attn.k_proj\n",
            "Initializing transformer.h.20.attn.k_proj\n",
            "Adding adapter to transformer.h.20.attn.v_proj\n",
            "Initializing transformer.h.20.attn.v_proj\n",
            "Adding adapter to transformer.h.20.attn.q_proj\n",
            "Initializing transformer.h.20.attn.q_proj\n",
            "Adding adapter to transformer.h.20.attn.out_proj\n",
            "Initializing transformer.h.20.attn.out_proj\n",
            "Adding adapter to transformer.h.20.mlp.fc_in\n",
            "Initializing transformer.h.20.mlp.fc_in\n",
            "Adding adapter to transformer.h.20.mlp.fc_out\n",
            "Initializing transformer.h.20.mlp.fc_out\n",
            "Adding adapter to transformer.h.21.attn.k_proj\n",
            "Initializing transformer.h.21.attn.k_proj\n",
            "Adding adapter to transformer.h.21.attn.v_proj\n",
            "Initializing transformer.h.21.attn.v_proj\n",
            "Adding adapter to transformer.h.21.attn.q_proj\n",
            "Initializing transformer.h.21.attn.q_proj\n",
            "Adding adapter to transformer.h.21.attn.out_proj\n",
            "Initializing transformer.h.21.attn.out_proj\n",
            "Adding adapter to transformer.h.21.mlp.fc_in\n",
            "Initializing transformer.h.21.mlp.fc_in\n",
            "Adding adapter to transformer.h.21.mlp.fc_out\n",
            "Initializing transformer.h.21.mlp.fc_out\n",
            "Adding adapter to transformer.h.22.attn.k_proj\n",
            "Initializing transformer.h.22.attn.k_proj\n",
            "Adding adapter to transformer.h.22.attn.v_proj\n",
            "Initializing transformer.h.22.attn.v_proj\n",
            "Adding adapter to transformer.h.22.attn.q_proj\n",
            "Initializing transformer.h.22.attn.q_proj\n",
            "Adding adapter to transformer.h.22.attn.out_proj\n",
            "Initializing transformer.h.22.attn.out_proj\n",
            "Adding adapter to transformer.h.22.mlp.fc_in\n",
            "Initializing transformer.h.22.mlp.fc_in\n",
            "Adding adapter to transformer.h.22.mlp.fc_out\n",
            "Initializing transformer.h.22.mlp.fc_out\n",
            "Adding adapter to transformer.h.23.attn.k_proj\n",
            "Initializing transformer.h.23.attn.k_proj\n",
            "Adding adapter to transformer.h.23.attn.v_proj\n",
            "Initializing transformer.h.23.attn.v_proj\n",
            "Adding adapter to transformer.h.23.attn.q_proj\n",
            "Initializing transformer.h.23.attn.q_proj\n",
            "Adding adapter to transformer.h.23.attn.out_proj\n",
            "Initializing transformer.h.23.attn.out_proj\n",
            "Adding adapter to transformer.h.23.mlp.fc_in\n",
            "Initializing transformer.h.23.mlp.fc_in\n",
            "Adding adapter to transformer.h.23.mlp.fc_out\n",
            "Initializing transformer.h.23.mlp.fc_out\n",
            "Adding adapter to transformer.h.24.attn.k_proj\n",
            "Initializing transformer.h.24.attn.k_proj\n",
            "Adding adapter to transformer.h.24.attn.v_proj\n",
            "Initializing transformer.h.24.attn.v_proj\n",
            "Adding adapter to transformer.h.24.attn.q_proj\n",
            "Initializing transformer.h.24.attn.q_proj\n",
            "Adding adapter to transformer.h.24.attn.out_proj\n",
            "Initializing transformer.h.24.attn.out_proj\n",
            "Adding adapter to transformer.h.24.mlp.fc_in\n",
            "Initializing transformer.h.24.mlp.fc_in\n",
            "Adding adapter to transformer.h.24.mlp.fc_out\n",
            "Initializing transformer.h.24.mlp.fc_out\n",
            "Adding adapter to transformer.h.25.attn.k_proj\n",
            "Initializing transformer.h.25.attn.k_proj\n",
            "Adding adapter to transformer.h.25.attn.v_proj\n",
            "Initializing transformer.h.25.attn.v_proj\n",
            "Adding adapter to transformer.h.25.attn.q_proj\n",
            "Initializing transformer.h.25.attn.q_proj\n",
            "Adding adapter to transformer.h.25.attn.out_proj\n",
            "Initializing transformer.h.25.attn.out_proj\n",
            "Adding adapter to transformer.h.25.mlp.fc_in\n",
            "Initializing transformer.h.25.mlp.fc_in\n",
            "Adding adapter to transformer.h.25.mlp.fc_out\n",
            "Initializing transformer.h.25.mlp.fc_out\n",
            "Adding adapter to transformer.h.26.attn.k_proj\n",
            "Initializing transformer.h.26.attn.k_proj\n",
            "Adding adapter to transformer.h.26.attn.v_proj\n",
            "Initializing transformer.h.26.attn.v_proj\n",
            "Adding adapter to transformer.h.26.attn.q_proj\n",
            "Initializing transformer.h.26.attn.q_proj\n",
            "Adding adapter to transformer.h.26.attn.out_proj\n",
            "Initializing transformer.h.26.attn.out_proj\n",
            "Adding adapter to transformer.h.26.mlp.fc_in\n",
            "Initializing transformer.h.26.mlp.fc_in\n",
            "Adding adapter to transformer.h.26.mlp.fc_out\n",
            "Initializing transformer.h.26.mlp.fc_out\n",
            "Adding adapter to transformer.h.27.attn.k_proj\n",
            "Initializing transformer.h.27.attn.k_proj\n",
            "Adding adapter to transformer.h.27.attn.v_proj\n",
            "Initializing transformer.h.27.attn.v_proj\n",
            "Adding adapter to transformer.h.27.attn.q_proj\n",
            "Initializing transformer.h.27.attn.q_proj\n",
            "Adding adapter to transformer.h.27.attn.out_proj\n",
            "Initializing transformer.h.27.attn.out_proj\n",
            "Adding adapter to transformer.h.27.mlp.fc_in\n",
            "Initializing transformer.h.27.mlp.fc_in\n",
            "Adding adapter to transformer.h.27.mlp.fc_out\n",
            "Initializing transformer.h.27.mlp.fc_out\n",
            "Adding adapter to lm_head\n",
            "Initializing lm_head\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPTJForCausalLM(\n",
              "  (transformer): GPTJModel(\n",
              "    (wte): FrozenBNBEmbedding(50400, 4096)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-27): 28 x GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): FrozenBNBLinear(4096, 4096)\n",
              "          (v_proj): FrozenBNBLinear(4096, 4096)\n",
              "          (q_proj): FrozenBNBLinear(4096, 4096)\n",
              "          (out_proj): FrozenBNBLinear(4096, 4096)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): FrozenBNBLinear(4096, 16384)\n",
              "          (fc_out): FrozenBNBLinear(16384, 4096)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): FrozenBNBLinear(4096, 50400)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Load Pre-Trained GPT-J Model with 8bit conversion\n",
        "gpt = GPTJForCausalLM.from_pretrained(\"hivemind/gpt-j-6B-8bit\", low_cpu_mem_usage=True)\n",
        "\n",
        "def add_adapters(model, adapter_dim=4, p = 0.1):\n",
        "    assert adapter_dim > 0\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "      if isinstance(module, FrozenBNBLinear):\n",
        "          if \"attn\" in name or \"mlp\" in name or \"head\" in name:\n",
        "              print(\"Adding adapter to\", name)\n",
        "              module.adapter = nn.Sequential(\n",
        "                nn.Linear(module.in_features, adapter_dim, bias=False),\n",
        "                nn.Dropout(p=p),\n",
        "                nn.Linear(adapter_dim, module.out_features, bias=False),\n",
        "            )\n",
        "              print(\"Initializing\", name)\n",
        "              nn.init.zeros_(module.adapter[2].weight)\n",
        "\n",
        "          else:\n",
        "              print(\"Not adding adapter to\", name)\n",
        "      elif isinstance(module, FrozenBNBEmbedding):\n",
        "          print(\"Adding adapter to\", name)\n",
        "          module.adapter = nn.Sequential(\n",
        "                nn.Embedding(module.num_embeddings, adapter_dim),\n",
        "                nn.Dropout(p=p),\n",
        "                nn.Linear(adapter_dim, module.embedding_dim, bias=False),\n",
        "            )\n",
        "          print(\"Initializing\", name)\n",
        "          nn.init.zeros_(module.adapter[2].weight)\n",
        "\n",
        "add_adapters(gpt)\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = 'cuda'\n",
        "gpt.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IsRP966L4oa_"
      },
      "outputs": [],
      "source": [
        "#@title Load the finetuned model State_Dict to the Pretrained GPT-J Model\n",
        "\n",
        "model_path = \"/content/downloaded/pytorch_model.bin\" #@param {type:\"string\"}\n",
        "\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "gpt.load_state_dict(state_dict)\n",
        "del state_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDdPrw8LM2M0"
      },
      "source": [
        "# CELL 4: RUNNING GRADIO INTERFACE & INFERENCE BACKEND"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTDRP8eEQWth"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from datetime import datetime as dt\n",
        "\n",
        "def generate_text(input):\n",
        "  gpt.eval()\n",
        "  prompt = f\"prompt:{input},commands:[\"\n",
        "  with torch.no_grad():\n",
        "    preprocessed_prompt = tokenizer(prompt, truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
        "    preprocessed_prompt = {key: value.to(device) for key, value in preprocessed_prompt.items()}\n",
        "    out = gpt.generate(**preprocessed_prompt, max_length=128, top_k=50, top_p=0.9, temperature=1.0, do_sample=True, repetition_penalty = 1.2, num_beams=1)\n",
        "    outputs = tokenizer.decode(out[0]).split(\",\")\n",
        "    full_commands = outputs[1].replace('\\n',',')\n",
        "\n",
        "    print(full_commands)\n",
        "\n",
        "    prompt = f\"{outputs[1].replace('<|endoftext|>','')},bot_reply:\"\n",
        "    preprocessed_prompt = tokenizer(prompt, truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
        "    preprocessed_prompt = {key: value.to(device) for key, value in preprocessed_prompt.items()}\n",
        "    out = gpt.generate(**preprocessed_prompt, max_length=512, top_k=50, top_p=0.9, temperature=1.0, do_sample=True, repetition_penalty = 1.2, num_beams=1)\n",
        "    outputs = tokenizer.decode(out[0])\n",
        "\n",
        "    pattern = r\"bot_reply:(.*)\"\n",
        "    matches = re.findall(pattern, outputs, re.DOTALL)\n",
        "    bot_reply_text = matches[0].strip()\n",
        "    bot_reply = bot_reply_text.replace(\"\\n\",\",\")\n",
        "\n",
        "    print(f'bot_reply:{bot_reply}')\n",
        "\n",
        "\n",
        "  return (f\"{full_commands}.\\n\\nPenjelasan: {bot_reply}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7yaxGFeRHMY",
        "outputId": "9cf66fee-dfe4-4590-f29b-c163ed0a2c84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: typing-extensions==3.7.4.3 in /usr/local/lib/python3.10/dist-packages (3.7.4.3)\n",
            "Requirement already satisfied: gradio==3.47.1 in /usr/local/lib/python3.10/dist-packages (3.47.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (0.104.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (0.3.1)\n",
            "Requirement already satisfied: gradio-client==0.6.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (0.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (0.25.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (0.18.0)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (6.1.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (1.23.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (3.9.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (9.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (1.10.13)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (0.0.6)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (2.31.0)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (2.10.0)\n",
            "Collecting typing-extensions~=4.0 (from gradio==3.47.1)\n",
            "  Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (0.23.2)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.47.1) (11.0.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.6.0->gradio==3.47.1) (2023.6.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.47.1) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.47.1) (4.19.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.47.1) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio==3.47.1) (3.12.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio==3.47.1) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.47.1) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.47.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.47.1) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.47.1) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.47.1) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio==3.47.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio==3.47.1) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.47.1) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.47.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.47.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio==3.47.1) (2023.7.22)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pyngrok==5.0.0 in /usr/local/lib/python3.10/dist-packages (5.0.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok==5.0.0) (6.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio==3.47.1\n",
        "!pip install pyngrok==5.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "gHYbaQ0BRNop",
        "outputId": "10a36c2a-f28d-472e-bcea-bffc7bdd33f9"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-bd5638c9d6e5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create Gradio interface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Masukkan prompt:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Hasil:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio/components/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgradio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotated_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnnotatedImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgradio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgradio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar_plot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBarPlot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m from gradio.components.base import (\n\u001b[1;32m      5\u001b[0m     \u001b[0mColumn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio/components/annotated_image.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgradio_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocumentation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_documentation_group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgradio_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJSONSerializable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_Image\u001b[0m  \u001b[0;31m# using _ to minimize namespace pollution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio_client/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgradio_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgradio_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m __all__ = [\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"Client\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio_client/client.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgradio_client\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserializing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgradio_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocumentation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_documentation_group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgradio_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSerializable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio_client/serializing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgradio_client\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmedia_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgradio_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_classes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"types.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gradio_client/data_classes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTypedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNotRequired\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'NotRequired' from 'typing_extensions' (/usr/local/lib/python3.10/dist-packages/typing_extensions.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create Gradio interface\n",
        "input_text = gr.components.Textbox(label=\"Masukkan prompt:\")\n",
        "output_text = gr.components.Textbox(label=\"Hasil:\")\n",
        "\n",
        "# input_text = f\"prompt:{input_text},commands:[\"\n",
        "\n",
        "gr.Interface(fn=generate_text, inputs=input_text, outputs=output_text, title=\"Inference Model\", description=\"Contoh proof of concept Tugas Akhir NLP Router Configuration.\").launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "620QXgweR6vh",
        "outputId": "002faaf1-5c52-4dd2-893a-a0e6791da335"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "URL publik untuk antarmuka web: NgrokTunnel: \"http://5511-34-124-208-105.ngrok.io\" -> \"http://localhost:80\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Tunnel Gradio interface\n",
        "public_url = ngrok.connect(port='7860')\n",
        "print(\"URL publik untuk antarmuka web:\", public_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgss_yOBXJ8O"
      },
      "source": [
        "# RUN API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXFoMc2gYnfx",
        "outputId": "6c1c7bfa-9601-4c63-abc9-48aa5332ea2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.27.1)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.3.6)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask-ngrok) (2.1.3)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "--2023-07-20 05:29:49--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.202.168.65, 18.205.222.128, 54.161.241.46, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.202.168.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13856790 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.tgz’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.21M  5.49MB/s    in 2.4s    \n",
            "\n",
            "2023-07-20 05:29:52 (5.49 MB/s) - ‘ngrok-stable-linux-amd64.tgz’ saved [13856790/13856790]\n",
            "\n",
            "ngrok\n",
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "#@markdown Need ngrok token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "ngrok_token = \"2cdT3Tjp5v9dIiRHnXSOk17JDrq_7HkFbfCtzpUPiZ4fZrmAZ\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "!pip install flask-ngrok\n",
        "!wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar -xvf /content/ngrok-v3-stable-linux-amd64.tgz\n",
        "!./ngrok authtoken {ngrok_token}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT3u4HPwV24T",
        "outputId": "894b2344-addc-43f3-8bca-16aa53d0d216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Running on http://1913-34-143-194-51.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:46:19.905122] Processing Input: `Tolong tambahkan kunci enkripsi ke Router D`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 05:46:39] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:46:39.712838] Processing Output: `commands:[Router D] configure terminal,crypto key generate rsa,end [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:48:38.916935] Processing Input: `Berikan konfigurasi untuk semua interface pada Router A G1`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 05:48:43] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:48:43.446371] Processing Output: `commands:[Router A] show running-config interface g1 [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:49:16.107004] Processing Input: `berikan list ip interface pada router A`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 05:49:20] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:49:20.366397] Processing Output: `commands:[Router A] show interface g0/0 [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:49:28.262193] Processing Input: `berikan list ip interface pada router A`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 05:49:31] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:49:31.349682] Processing Output: `commands:[Router A] show interfaces [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:50:34.222738] Processing Input: `tolong ganti ip interface G2 menjadi 10.33.127.131 255.255.240.0`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 05:50:43] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:50:43.868639] Processing Output: `commands:[Router G2] configure terminal,interface G2,ip address 192.168.2.1 255.255.255.0,end [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:50:48.378427] Processing Input: `tolong ganti ip interface G2 menjadi 10.33.127.131 255.255.240.0`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 05:50:57] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:50:57.998226] Processing Output: `commands:[Router G2] configure terminal,interface G2,ip address 255.255.255.0 255.255.255.0,end [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:51:03.400133] Processing Input: `tolong ganti ip interface G2 menjadi 10.33.127.131 255.255.240.0`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 05:51:11] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:51:11.187586] Processing Output: `commands:[Router G2] configure terminal,interface G2,ip address 255.255.255.0,end [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:51:23.414338] Processing Input: `tolong ganti ip interface G2 menjadi 10.33.127.131`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 05:51:33] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:51:33.014753] Processing Output: `commands:[Router R2] configure terminal,interface G2,ip address 10.33.187.0 255.255.255.0,end [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:51:45.068740] Processing Input: `tolong ganti ip interface G2 menjadi 10.33.101.10`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 05:51:54] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:51:54.722778] Processing Output: `commands:[Router G2] configure terminal,interface G2,ip address 10.33.0.1 255.255.255.0,end [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:52:09.920733] Processing Input: `tolong ganti ip interface G2 menjadi 192.168.100.10`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 05:52:19] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:52:19.765467] Processing Output: `commands:[Router G2] configure terminal,interface G2,ip address 192.168.50.1 255.255.255.0,end [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:52:36.580724] Processing Input: `tolong ganti ip interface G2 menjadi 192.168.10.1 255.255.255.0`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 05:52:46] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 05:52:46.324190] Processing Output: `commands:[Router G2] configure terminal,interface G2,ip address 192.168.10.1 255.255.255.0,end [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:41:00.377881] Processing Input: `Tolong ubah ip dari interface G2 menjadi 192.168.10.10 255.255.255.0`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 06:41:10] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:41:10.273211] Processing Output: `commands:[Router A] configure terminal,interface G2,ip address 192.168. 10.0.1 255.255.255.0,end [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:41:28.296301] Processing Input: `Tolong ubah ip dari interface G2 menjadi 192.168.10.10 255.255.255.0`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 06:41:37] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:41:37.849765] Processing Output: `commands:[Router G2] configure terminal,interface G2,ip address 192.168.10.10 255.255.255.0,end [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:44:30.060587] Processing Input: `tolong berikan saya list ip interface para Router A`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 06:44:33] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:44:33.227591] Processing Output: `commands:[Router A] show interfaces [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:46:32.726431] Processing Input: `buatkan saya routing menuju network 192.168.10.0 dengan subnetmask 255.255.255.0 melalui interface G1 di router A`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 06:46:41] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:46:41.643564] Processing Output: `commands:[Router A] configure terminal,ip route 192.168.10.0 255.255.255.0 g1,end [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:47:59.550733] Processing Input: `buatkan saya routing menuju network 192.168.10.0 melalui interface G1`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 06:48:08] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:48:08.648556] Processing Output: `commands:[Router R1] configure terminal,ip route 192.168.10.0 255.255.255.0 G1,end [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:48:53.784608] Processing Input: `tolong kasih ip interface G2`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 06:49:03] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:49:03.035725] Processing Output: `commands:[Router A] configure terminal,interface G2,ip address 192.168.2.1 255.255.255.0,end [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:52:59.549572] Processing Input: `halo sapa saya`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 06:53:08] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:53:08.592467] Processing Output: `commands:[Router A] configure terminal,access-list 1 permit 192.168.0.0 0.0.255.255,end [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:53:59.521463] Processing Input: `tolong hapus ip interface G2`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 06:54:03] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:54:03.743585] Processing Output: `commands:[Router] show running-config interface g2 [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:55:03.985687] Processing Input: `tolong tambahkan enkripsi router A`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 06:55:09] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:55:09.314679] Processing Output: `commands:[Router A] configure terminal,service password-encryption,end [eoc]<|endoftext|>`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:55:18.413195] Processing Input: `tolong tambahkan enkripsi router A`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [20/Jul/2023 06:55:23] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2023-07-20 06:55:23.693504] Processing Output: `commands:[Router A] configure terminal,service password-encryption,end [eoc]<|endoftext|>`\n"
          ]
        }
      ],
      "source": [
        "#@title Run the API\n",
        "#@markdown Wait until ngrok link appear\n",
        "from flask import Flask, request, jsonify\n",
        "from datetime import datetime as dt\n",
        "import re\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import os\n",
        "import threading\n",
        "\n",
        "app = Flask(__name__)\n",
        "port = \"5001\"\n",
        "\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f' * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"')\n",
        "\n",
        "# Update any base URLs to use the public ngrok URL\n",
        "app.config[\"BASE_URL\"] = public_url\n",
        "\n",
        "def generate_text(input):\n",
        "  print(f\"[{dt.now()}] Processing Input: `{input}`\")\n",
        "  gpt.eval()\n",
        "  prompt = f\"prompt:{input},commands:[\"\n",
        "  with torch.no_grad():\n",
        "    preprocessed_prompt = tokenizer(prompt, truncation=True, padding=True, max_length=256, return_tensors='pt')\n",
        "    preprocessed_prompt = {key: value.to(device) for key, value in preprocessed_prompt.items()}\n",
        "    out = gpt.generate(**preprocessed_prompt, max_length=128, top_k=50, top_p=0.9, temperature=1.0, do_sample=True, repetition_penalty = 1.2, num_beams=1)\n",
        "    outputs = tokenizer.decode(out[0]).split(\",\")\n",
        "    full_commands = outputs[1].replace('\\n',',')\n",
        "\n",
        "  return full_commands\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "  input = request.json['input']\n",
        "  full_commands = generate_text(input)\n",
        "\n",
        "  print(f\"[{dt.now()}] Processing Output: `{full_commands}`\")\n",
        "  regex = re.search(r\"\\[(.+)\\] (.+) \\[eoc]\",full_commands)\n",
        "  router = regex[1]\n",
        "  commands = regex[2]\n",
        "  logs = []\n",
        "\n",
        "  commands = {\"commands\": commands.split(\",\")}\n",
        "\n",
        "  return jsonify(commands)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(port=port)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQlwmZTDQijc",
        "outputId": "822a7fdf-14fb-4fbc-ad32-0b88a339137a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "#@title 1. Login to Huggingface hub\n",
        "try:\n",
        "  hub_ok # My packaged dep didn't contains this (but stil have the folder)\n",
        "except:\n",
        "  print(\"Setting up huggingface_hub...\")\n",
        "  !pip install --force-reinstall -qqqq huggingface_hub\n",
        "  hub_ok = True\n",
        "from IPython.display import clear_output\n",
        "from huggingface_hub import login\n",
        "clear_output()\n",
        "\n",
        "#@markdown 1. Of course, you need a Huggingface account first.\n",
        "#@markdown 2. To create a huggingface token, go to [this link](https://huggingface.co/settings/tokens), then `create new token` or copy available token with the `Write` role.\n",
        "\n",
        "write_token = \"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\" #@param {type:\"string\"}\n",
        "login(write_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWxLyUCoQl9U",
        "outputId": "da8ffce6-5f9b-4c06-f747-33ece2cf460a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Repo: hanungaddi/model_router exists, skipping create repo\n",
            "\n",
            "Git LFS initialized.\n",
            "Cloning into '/content/model_router'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 9 (delta 0), reused 0 (delta 0), pack-reused 6\u001b[K\n",
            "Unpacking objects: 100% (9/9), 1.21 KiB | 621.00 KiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "#@title 2. Setup Repo\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "\n",
        "api = HfApi()\n",
        "user = api.whoami(write_token)\n",
        "\n",
        "#@markdown #### If your model repo didn't exist, it will automatically create your repo.\n",
        "repo_name = \"model_router\" #@param{type:\"string\"}\n",
        "make_this_repo_private_if_not_exist = False #@param{type:\"boolean\"}\n",
        "clone_with_git = True #@param{type:\"boolean\"}\n",
        "\n",
        "model_repo = user['name']+\"/\"+repo_name.strip()\n",
        "\n",
        "validate_repo_id(model_repo)\n",
        "\n",
        "if repo_name != \"\":\n",
        "  try:\n",
        "      api.create_repo(repo_id=model_repo,\n",
        "                      private=make_this_repo_private_if_not_exist)\n",
        "      print(\"Model Repo didn't exists, creating repo\")\n",
        "      print(\"Model Repo: \",model_repo,\"created!\\n\")\n",
        "\n",
        "  except HfHubHTTPError as e:\n",
        "      print(f\"Model Repo: {model_repo} exists, skipping create repo\\n\")\n",
        "\n",
        "if clone_with_git:\n",
        "  !git lfs install --skip-smudge\n",
        "  !export GIT_LFS_SKIP_SMUDGE=1\n",
        "  !git clone https://huggingface.co/{model_repo} /content/{repo_name}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSFPvY0MQ698",
        "outputId": "501ec2d3-b087-4efb-9eef-62e78802e2c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/model_router\n",
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Already up to date.\n",
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Local repo set up for largefiles\n",
            "Encountered 1 file(s) that may not have been copied correctly on Windows:\n",
            "\tmodel08082023.bin\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n",
            "[main 6824fc5] model 08 agustus 2023\n",
            " 1 file changed, 3 insertions(+)\n",
            " create mode 100644 model08082023.bin\n",
            "Uploading LFS objects: 100% (1/1), 6.2 GB | 22 MB/s, done.\n",
            "Enumerating objects: 4, done.\n",
            "Counting objects: 100% (4/4), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (3/3), done.\n",
            "Writing objects: 100% (3/3), 386 bytes | 386.00 KiB/s, done.\n",
            "Total 3 (delta 1), reused 0 (delta 0), pack-reused 0\n",
            "To https://huggingface.co/hanungaddi/model_router\n",
            "   4000ffd..6824fc5  main -> main\n"
          ]
        }
      ],
      "source": [
        "#@title 3.3 Upload via GIT (more stable)\n",
        "%cd /content/\n",
        "file_path = \"/content/model-step-1000/model08082023.bin\" #@param {type :\"string\"}\n",
        "!ln {file_path} /content/{repo_name}/\n",
        "#@markdown Set **git commit identity**\n",
        "email = \"hanungutomo45@gmail.com\" #@param {'type': 'string'}\n",
        "name = \"hanungaddi\" #@param {'type': 'string'}\n",
        "#@markdown Set **commit message**\n",
        "commit_m = \"model 08 agustus 2023\" #@param {'type': 'string'}\n",
        "\n",
        "%cd /content/{repo_name}\n",
        "\n",
        "!git lfs install --skip-smudge\n",
        "!export GIT_LFS_SKIP_SMUDGE=1\n",
        "!git pull -X theirs\n",
        "\n",
        "!git lfs install\n",
        "!huggingface-cli lfs-enable-largefiles .\n",
        "!git config --global user.email \"{email}\"\n",
        "!git config --global user.name \"{name}\"\n",
        "!git add .\n",
        "!git commit -m \"{commit_m}\"\n",
        "!git push"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "UzuATzvqRLwx",
        "EKti6sXNG4H5",
        "e3U67wWMIgK7",
        "TDdPrw8LM2M0"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "001143a4ecd846ad84d9e565dc056737": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0180be892d8f419e81855a1ecc7033e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01ac8010aabc4ed4843fa8241da464dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58bccdd26b5844edaca8d09215ea64b9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_406316501eba4b80b2bd53d1d4c484e3",
            "value": 1
          }
        },
        "0381111224ec422cae8d37bf3e06706f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03da351ee8d842c89a2041c41c95b46e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06374d2c569142998d2311ba5d952b99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "096f5e97c600483d82e74e08167dd29f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c1ad1b39a1645c0a7398d1d38e4be44": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c6a70cc6fd7465799283e6108650ec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0cfd21107c0f47d097892245cdce09c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_779b436f16074b6b9c08e82ff95397aa",
            "max": 357,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d38b5ac2f43944cda8be7d725c33bd04",
            "value": 357
          }
        },
        "0d52b83c460a4ef08cca248348e9aa66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5379d9d3752f45939e826617c2a77ec9",
            "max": 456356,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c38c8f010684c328a2ae84aea419ce5",
            "value": 456356
          }
        },
        "106c60f876b645b6b9803a57f69afbed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11b8fdedfd0044ce8417f189dd15b4a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13e567efe19c4ad79002695870a949d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16b29a5e93804d7487c35c9ffe6f9ec9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a4c52b1d95e4a538588195d8f8aef81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1afcb6956bf44c508e8557320f54ee95": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d328f4dbfb54e4797e99751952ac4ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d3c5612c59d4535bf3811266d39769d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e2cb681aade4267ad7ffb5c6b24ec7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2393ae60dce6441da0c9a5d698dfc46e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91467d50ce8d4c0ba1f0df1387d000dc",
            "placeholder": "​",
            "style": "IPY_MODEL_1a4c52b1d95e4a538588195d8f8aef81",
            "value": " 1/1 [00:00&lt;00:00, 44.95it/s]"
          }
        },
        "24ab1d48242d4c668eab496810753a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7e5906fe0d14325bf63aa5e84a5524d",
            "max": 930,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa8d2b3523a048c89d6e034fd01e198e",
            "value": 930
          }
        },
        "25e50b25772545d784852d47376d7a3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2763eb9aaead4b8d9603c2e221f35175": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2796ee751d2b495a960c1761fca55c63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cdee56d95114e728a1a3c5ef5836c38",
            "placeholder": "​",
            "style": "IPY_MODEL_c61cd2b63a7f4d0f95282dc449f21db9",
            "value": "100%"
          }
        },
        "2860d52f020e44eb8dac5c5048e37ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b279ecb918d444ab93f71458ca1c8309",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b602080fb65c4b8e86c16a28e43c5285",
            "value": 1
          }
        },
        "28ae3fd672eb4422b9ae57c520aeb706": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "292181dada26415a9850729403268400": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a153a8ccde7420ba66828a422ef74d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d8d32832aed423ab71c10831b68f227": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e575b165bff4d5eb43d04401a0813f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32420ecc341a421e98cf8c7797273be6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b122d4bbac594d26ad38940fa8ee2f26",
              "IPY_MODEL_7112919fc52644428c73b2478f64993b",
              "IPY_MODEL_92662d8e06324f589833a7c1bda4235b"
            ],
            "layout": "IPY_MODEL_0381111224ec422cae8d37bf3e06706f"
          }
        },
        "344e08676ea84f98a57fc283e0c49ae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "366091a81d4842ab89a9643f8cfd34b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d328f4dbfb54e4797e99751952ac4ff",
            "placeholder": "​",
            "style": "IPY_MODEL_8d8150bb405a40028d75f3bfe805ed9a",
            "value": " 930/930 [00:00&lt;00:00, 56.6kB/s]"
          }
        },
        "386621f03649421c91ffc028d87a45c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b66d27d5c594690a40e62a7f23b23c8",
            "max": 4039,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_67086759051a4e72abe753e2ba322f10",
            "value": 4039
          }
        },
        "3cd5de7a1aa54a2193b9e26a6943f73d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d30850ca66b04f60978dc8df00627918",
            "placeholder": "​",
            "style": "IPY_MODEL_fa1ef3a7df0d4307936ea8a8a71f0e80",
            "value": " 30%"
          }
        },
        "3d46182cae4d4e9c9a6129ec4c4d045f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c060c0c065bd4c3b9b09a6e8cdb7369d",
            "placeholder": "​",
            "style": "IPY_MODEL_1e2cb681aade4267ad7ffb5c6b24ec7b",
            "value": "Downloading: 100%"
          }
        },
        "406316501eba4b80b2bd53d1d4c484e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40dc912536714c699562e042ffe1b4a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41f07774e30c4472818585d4124be7fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41f5701217164a768206f0810f34bf3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42337807457141b8bd6d68ecb8954ead": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_651b02eb35304665861d2d553b8187a9",
            "placeholder": "​",
            "style": "IPY_MODEL_41f5701217164a768206f0810f34bf3c",
            "value": "100%"
          }
        },
        "427b1a89c1c54de6b34f320a8401d9e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43d9cbf70a684436b4e585a620bd5d49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "486754f3ebc3440abcbbaad87f8c7e30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a426f407ea745e49627017d8f79ea4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40dc912536714c699562e042ffe1b4a1",
            "placeholder": "​",
            "style": "IPY_MODEL_a6e45f80f1c34459a13e1867dacbf343",
            "value": " 619/619 [00:00&lt;00:00, 40.2kB/s]"
          }
        },
        "4b64e15e47ef4282b3ff7a5d171731b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b66d27d5c594690a40e62a7f23b23c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b9339d842fa40f2be1e3dcc8c7d2eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c6c53c7df6f43a2a9587911e5585e63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4eaa71e7a9954780b3cc0b464279ff89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5379d9d3752f45939e826617c2a77ec9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57ad044d24294fa58e9c7a0f98a51e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d2c9d0c717d420083af9a045b6015b7",
            "max": 6177012613,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f618c0fa81e4dafab88e9824edaf4e2",
            "value": 6177012613
          }
        },
        "58bccdd26b5844edaca8d09215ea64b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b0d3a297f5b467192ac5387b9efcb2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b790297f7204c3fb4bd828bba6e624e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c38c8f010684c328a2ae84aea419ce5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d2c9d0c717d420083af9a045b6015b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d5f1e3d45d24c5d97195e8e22fd4593": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e5048040ed2432dbe8f7c0526bed45c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6091e99e7d8848a385313a98d5c753e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f58dcba811744e8893b3c15f34cd81b0",
            "placeholder": "​",
            "style": "IPY_MODEL_28ae3fd672eb4422b9ae57c520aeb706",
            "value": "100%"
          }
        },
        "648dedb4d5b24b27bc6965bd2be3ee6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8898abb3e1ab40f6af2a0f5b98df1466",
            "placeholder": "​",
            "style": "IPY_MODEL_e069c7bb58084d5d9bf4e577831b3704",
            "value": " 357/357 [00:00&lt;00:00, 18.2kB/s]"
          }
        },
        "651b02eb35304665861d2d553b8187a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67086759051a4e72abe753e2ba322f10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a9888a9617f436e9c4bdfaade5c0a44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d46182cae4d4e9c9a6129ec4c4d045f",
              "IPY_MODEL_386621f03649421c91ffc028d87a45c8",
              "IPY_MODEL_e45d850417ce4643b8453c4bda55bcc1"
            ],
            "layout": "IPY_MODEL_106c60f876b645b6b9803a57f69afbed"
          }
        },
        "6d6a833dac7a4916b56bc3b6eda98d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1afcb6956bf44c508e8557320f54ee95",
            "placeholder": "​",
            "style": "IPY_MODEL_0c6a70cc6fd7465799283e6108650ec2",
            "value": " 1.00k/1.00k [00:00&lt;00:00, 65.2kB/s]"
          }
        },
        "7112919fc52644428c73b2478f64993b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f667c17b12aa4545b16c1586d283ad91",
            "max": 798156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e5048040ed2432dbe8f7c0526bed45c",
            "value": 798156
          }
        },
        "71e2993cb61c451da872695a35b2e612": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cbd418b0334c488e9e3b6a10d9328a7e",
              "IPY_MODEL_0cfd21107c0f47d097892245cdce09c2",
              "IPY_MODEL_648dedb4d5b24b27bc6965bd2be3ee6f"
            ],
            "layout": "IPY_MODEL_fe6e613d946a4ff0a3d6a5cd095e624e"
          }
        },
        "71fc6011610941b7a26e743ce21881da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "741f568c62174a728fa3d82eb3ef7655": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed6f10da7e70445ea675975a63879da3",
            "placeholder": "​",
            "style": "IPY_MODEL_4b9339d842fa40f2be1e3dcc8c7d2eb4",
            "value": " 1.31M/1.31M [00:00&lt;00:00, 15.5MB/s]"
          }
        },
        "757ed197f5b641008c6cacf82f1722ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "779b436f16074b6b9c08e82ff95397aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a3ef1d8b2c7455d977af860e2f109a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bb086ec6d6e47bb8c542e7c1349f344": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be590f3cd3684f5b8a406f84d3a2b422",
            "placeholder": "​",
            "style": "IPY_MODEL_5b0d3a297f5b467192ac5387b9efcb2b",
            "value": " 1/1 [00:00&lt;00:00, 48.46it/s]"
          }
        },
        "81bb3df4536b431382182fff2fce8d54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8313974a676645228503e24f63f9891b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03da351ee8d842c89a2041c41c95b46e",
            "max": 619,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_001143a4ecd846ad84d9e565dc056737",
            "value": 619
          }
        },
        "8460893d5e1a469ea29f1bf274142807": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87b1a5b20d1e4031bcc7d6eef518763e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3cd5de7a1aa54a2193b9e26a6943f73d",
              "IPY_MODEL_9038fa22d6da4cedba5d72f6f32a7e55",
              "IPY_MODEL_a05f242e3ffd40379666b80991e7071f"
            ],
            "layout": "IPY_MODEL_d9f28d0315b0488aa6759ab6670b3d19"
          }
        },
        "8898abb3e1ab40f6af2a0f5b98df1466": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88be0bb297154b0e854467279a5c27b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d58b3178f1643da9dca808fdf2c1d5c",
            "placeholder": "​",
            "style": "IPY_MODEL_8cf96837e9e34362b244ba727e45fd2c",
            "value": "100%"
          }
        },
        "893a015bcde7437796e30cdb63fdb178": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ac5adbdd66f4a1288af1cf68d42e979": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a14fb130a27742da9f155b737bdb0e65",
              "IPY_MODEL_f7d7094786524c219116083bc7d944de",
              "IPY_MODEL_6d6a833dac7a4916b56bc3b6eda98d02"
            ],
            "layout": "IPY_MODEL_d08bba4dfd5c42959e2169e15816cac7"
          }
        },
        "8cdee56d95114e728a1a3c5ef5836c38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cf96837e9e34362b244ba727e45fd2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d615e45ccf841e0b9c0ed09875fcced": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6091e99e7d8848a385313a98d5c753e9",
              "IPY_MODEL_01ac8010aabc4ed4843fa8241da464dc",
              "IPY_MODEL_97fec64e4261403b9dc9dd65c6db5dbd"
            ],
            "layout": "IPY_MODEL_292181dada26415a9850729403268400"
          }
        },
        "8d8150bb405a40028d75f3bfe805ed9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9038fa22d6da4cedba5d72f6f32a7e55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe174f78cca24f28aa0b8e2cea4e1917",
            "max": 3400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_096f5e97c600483d82e74e08167dd29f",
            "value": 1025
          }
        },
        "91467d50ce8d4c0ba1f0df1387d000dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92662d8e06324f589833a7c1bda4235b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11b8fdedfd0044ce8417f189dd15b4a3",
            "placeholder": "​",
            "style": "IPY_MODEL_b24be892edb54d2a8ddfcf26a47eab02",
            "value": " 779k/779k [00:00&lt;00:00, 5.41MB/s]"
          }
        },
        "950129df07df4a2ebc557c4f52e46374": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16b29a5e93804d7487c35c9ffe6f9ec9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8e3ab428ae4482d9c65d43782a42a09",
            "value": 1
          }
        },
        "97fec64e4261403b9dc9dd65c6db5dbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71fc6011610941b7a26e743ce21881da",
            "placeholder": "​",
            "style": "IPY_MODEL_5d5f1e3d45d24c5d97195e8e22fd4593",
            "value": " 1/1 [00:00&lt;00:00,  4.94ba/s]"
          }
        },
        "994e63412f8749c9a181bd06f8f5b9da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c12090aa8364f89aa34c37f3a073de7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eee1e490eb8a416abe42b16e566d9c89",
              "IPY_MODEL_da0b8feef55e45fba010a72db202b382",
              "IPY_MODEL_741f568c62174a728fa3d82eb3ef7655"
            ],
            "layout": "IPY_MODEL_4b64e15e47ef4282b3ff7a5d171731b4"
          }
        },
        "9d58b3178f1643da9dca808fdf2c1d5c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dce245b6c73463b91c50bb0398c3dc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f791373a068c41e2bcb09912b3b8ae47",
              "IPY_MODEL_57ad044d24294fa58e9c7a0f98a51e3e",
              "IPY_MODEL_b0bcc221d81644d79432c3cab1760a0d"
            ],
            "layout": "IPY_MODEL_25e50b25772545d784852d47376d7a3e"
          }
        },
        "9f618c0fa81e4dafab88e9824edaf4e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a05f242e3ffd40379666b80991e7071f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b46755c5795542868b85416edea370a7",
            "placeholder": "​",
            "style": "IPY_MODEL_da7ef222d7e9470d8102b55efe5b980c",
            "value": " 1025/3400 [2:01:37&lt;4:32:22,  6.88s/it]"
          }
        },
        "a14fb130a27742da9f155b737bdb0e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a153a8ccde7420ba66828a422ef74d2",
            "placeholder": "​",
            "style": "IPY_MODEL_5b790297f7204c3fb4bd828bba6e624e",
            "value": "Downloading: 100%"
          }
        },
        "a4568c0ed6d34f8cba428da42d6ff478": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4e3b34ee6b341e7a607b098ca6614f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2796ee751d2b495a960c1761fca55c63",
              "IPY_MODEL_2860d52f020e44eb8dac5c5048e37ca0",
              "IPY_MODEL_b701f68b627d48a69afdaa77d77fc1e3"
            ],
            "layout": "IPY_MODEL_0180be892d8f419e81855a1ecc7033e3"
          }
        },
        "a6e45f80f1c34459a13e1867dacbf343": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7122ceea3604476b531d5e7d9123906": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a796b2e81de246e189dd8e3d03acf254": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da0ae5acc64240498feb837a37247450",
              "IPY_MODEL_0d52b83c460a4ef08cca248348e9aa66",
              "IPY_MODEL_fc3319bfc16d49fbaefbb1d16a4dbd02"
            ],
            "layout": "IPY_MODEL_2763eb9aaead4b8d9603c2e221f35175"
          }
        },
        "a7e5906fe0d14325bf63aa5e84a5524d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa8d2b3523a048c89d6e034fd01e198e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0bcc221d81644d79432c3cab1760a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c823e95d20dc4d5ea8ef7c093e887982",
            "placeholder": "​",
            "style": "IPY_MODEL_bb59191bf62f49d5a4a9ab83fbf536ad",
            "value": " 5.75G/5.75G [01:39&lt;00:00, 48.9MB/s]"
          }
        },
        "b122d4bbac594d26ad38940fa8ee2f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81bb3df4536b431382182fff2fce8d54",
            "placeholder": "​",
            "style": "IPY_MODEL_41f07774e30c4472818585d4124be7fb",
            "value": "Downloading: 100%"
          }
        },
        "b1b31860c9e8494fbdaaa9cb55dc24a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b24be892edb54d2a8ddfcf26a47eab02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b279ecb918d444ab93f71458ca1c8309": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b46755c5795542868b85416edea370a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b602080fb65c4b8e86c16a28e43c5285": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b701f68b627d48a69afdaa77d77fc1e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8460893d5e1a469ea29f1bf274142807",
            "placeholder": "​",
            "style": "IPY_MODEL_dff750f83f9d436aa49c5270b6355c0f",
            "value": " 1/1 [00:00&lt;00:00, 33.17it/s]"
          }
        },
        "b8e3ab428ae4482d9c65d43782a42a09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb59191bf62f49d5a4a9ab83fbf536ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be590f3cd3684f5b8a406f84d3a2b422": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c060c0c065bd4c3b9b09a6e8cdb7369d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c61cd2b63a7f4d0f95282dc449f21db9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c823e95d20dc4d5ea8ef7c093e887982": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbd418b0334c488e9e3b6a10d9328a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e575b165bff4d5eb43d04401a0813f0",
            "placeholder": "​",
            "style": "IPY_MODEL_f44a86dd9f4b4d53a308b32f1f8a6ec8",
            "value": "Downloading: 100%"
          }
        },
        "d08bba4dfd5c42959e2169e15816cac7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d30850ca66b04f60978dc8df00627918": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d38b5ac2f43944cda8be7d725c33bd04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3c8079de8e04277926412f7e343bd0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4568c0ed6d34f8cba428da42d6ff478",
            "placeholder": "​",
            "style": "IPY_MODEL_ee661ca21d3c4155ac1c3a163a1fffc6",
            "value": "Downloading: 100%"
          }
        },
        "d4839891428b4809ad8b942d8b049ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88be0bb297154b0e854467279a5c27b4",
              "IPY_MODEL_f46bd9598f3e40338159172baa2cdd36",
              "IPY_MODEL_2393ae60dce6441da0c9a5d698dfc46e"
            ],
            "layout": "IPY_MODEL_d8d20ff6f5104b428f2f065c22e6172b"
          }
        },
        "d566535df8444e868222da339ea3b969": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d66ea430381f496d9125ccb85cc9617c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7d186e3da3b4831a8e7d8bdc773631a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3c8079de8e04277926412f7e343bd0b",
              "IPY_MODEL_24ab1d48242d4c668eab496810753a7a",
              "IPY_MODEL_366091a81d4842ab89a9643f8cfd34b5"
            ],
            "layout": "IPY_MODEL_486754f3ebc3440abcbbaad87f8c7e30"
          }
        },
        "d8d20ff6f5104b428f2f065c22e6172b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9f28d0315b0488aa6759ab6670b3d19": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da0ae5acc64240498feb837a37247450": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7122ceea3604476b531d5e7d9123906",
            "placeholder": "​",
            "style": "IPY_MODEL_2d8d32832aed423ab71c10831b68f227",
            "value": "Downloading: 100%"
          }
        },
        "da0b8feef55e45fba010a72db202b382": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13e567efe19c4ad79002695870a949d5",
            "max": 1373465,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_427b1a89c1c54de6b34f320a8401d9e0",
            "value": 1373465
          }
        },
        "da7ef222d7e9470d8102b55efe5b980c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dff750f83f9d436aa49c5270b6355c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e069c7bb58084d5d9bf4e577831b3704": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e31c90c6424f47e09924a9c2eb21d2ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f399eea3124e429fa744e10ee356d4d4",
              "IPY_MODEL_8313974a676645228503e24f63f9891b",
              "IPY_MODEL_4a426f407ea745e49627017d8f79ea4f"
            ],
            "layout": "IPY_MODEL_7a3ef1d8b2c7455d977af860e2f109a3"
          }
        },
        "e45d850417ce4643b8453c4bda55bcc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06374d2c569142998d2311ba5d952b99",
            "placeholder": "​",
            "style": "IPY_MODEL_994e63412f8749c9a181bd06f8f5b9da",
            "value": " 3.94k/3.94k [00:00&lt;00:00, 293kB/s]"
          }
        },
        "e8600d66cfc1431f8b2ce106d8c4c3ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecf8b83d036041cf8b6cd2346f618471": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed6f10da7e70445ea675975a63879da3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee661ca21d3c4155ac1c3a163a1fffc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eee1e490eb8a416abe42b16e566d9c89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_757ed197f5b641008c6cacf82f1722ad",
            "placeholder": "​",
            "style": "IPY_MODEL_4c6c53c7df6f43a2a9587911e5585e63",
            "value": "Downloading: 100%"
          }
        },
        "f399eea3124e429fa744e10ee356d4d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d3c5612c59d4535bf3811266d39769d",
            "placeholder": "​",
            "style": "IPY_MODEL_344e08676ea84f98a57fc283e0c49ae1",
            "value": "Downloading: 100%"
          }
        },
        "f44a86dd9f4b4d53a308b32f1f8a6ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f46bd9598f3e40338159172baa2cdd36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eaa71e7a9954780b3cc0b464279ff89",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d566535df8444e868222da339ea3b969",
            "value": 1
          }
        },
        "f58dcba811744e8893b3c15f34cd81b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f667c17b12aa4545b16c1586d283ad91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f791373a068c41e2bcb09912b3b8ae47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_893a015bcde7437796e30cdb63fdb178",
            "placeholder": "​",
            "style": "IPY_MODEL_ecf8b83d036041cf8b6cd2346f618471",
            "value": "Downloading: 100%"
          }
        },
        "f7d7094786524c219116083bc7d944de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8600d66cfc1431f8b2ce106d8c4c3ef",
            "max": 1021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43d9cbf70a684436b4e585a620bd5d49",
            "value": 1021
          }
        },
        "f8b6eeb07a3d4b85bfbfdf73a999d618": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42337807457141b8bd6d68ecb8954ead",
              "IPY_MODEL_950129df07df4a2ebc557c4f52e46374",
              "IPY_MODEL_7bb086ec6d6e47bb8c542e7c1349f344"
            ],
            "layout": "IPY_MODEL_0c1ad1b39a1645c0a7398d1d38e4be44"
          }
        },
        "fa1ef3a7df0d4307936ea8a8a71f0e80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc3319bfc16d49fbaefbb1d16a4dbd02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1b31860c9e8494fbdaaa9cb55dc24a6",
            "placeholder": "​",
            "style": "IPY_MODEL_d66ea430381f496d9125ccb85cc9617c",
            "value": " 446k/446k [00:00&lt;00:00, 4.40MB/s]"
          }
        },
        "fe174f78cca24f28aa0b8e2cea4e1917": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe6e613d946a4ff0a3d6a5cd095e624e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
